<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="https://tech.socarcorp.kr/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tech.socarcorp.kr/" rel="alternate" type="text/html" /><updated>2022-07-25T06:56:05+00:00</updated><id>https://tech.socarcorp.kr/feed.xml</id><title type="html">SOCAR Tech Blog</title><subtitle>쏘카 기술 블로그</subtitle><author><name>SOCAR</name></author><entry><title type="html">데이터에 신뢰성과 재사용성까지, Analytics Engineering with dbt</title><link href="https://tech.socarcorp.kr/data/2022/07/25/analytics-engineering-with-dbt.html" rel="alternate" type="text/html" title="데이터에 신뢰성과 재사용성까지, Analytics Engineering with dbt" /><published>2022-07-25T00:00:00+00:00</published><updated>2022-07-25T00:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/07/25/analytics-engineering-with-dbt</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/07/25/analytics-engineering-with-dbt.html">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;안녕하세요. 데이터 비즈니스 본부 - 데이터 엔지니어링 그룹의 험프리입니다.&lt;/p&gt;

&lt;p&gt;데이터 엔지니어링 그룹은 데이터 플랫폼 팀, 데이터 웨어하우스팀으로 나누어 쏘카의 구성원들 누구나 쏘카의 데이터를 쉽고 빠르게 조회할수록 최적의 방안을 마련하고 지원하는 일을 수행하고 있습니다. 그중 데이터 플랫폼 팀은 데이터 엔지니어링 그룹 내뿐만 아니라 데이터 비즈니스 본부 내의 엔지니어링 파트에서 서포트에서 필요한 인프라, 데이터 파이프라인 개발, 운영, 모니터링, 데이터 애플리케이션 개발, MLOps 등의 업무를 맡고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;1-들어가며&quot;&gt;1. 들어가며&lt;/h2&gt;

&lt;p&gt;쏘카에서는 데이터 분석가나 모델러, PM 등 여러 이해관계자가 데이터를 의사 결정에 활용하고 있습니다. 쏘카가 다루는 모빌리티 도메인의 데이터는 특히나 러닝 커브가 높습니다. 때문에 다양한 배경의 구성원들이 더 쉽게 쏘카의 데이터를 활용하기 위해 이미 오래전부터 데이터 마트를 운영하고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 여느 소프트웨어가 그렇듯 시간이 지남에 따라 비즈니스가 성장하고 새로운 사업들이 추가되면서, 그로부터 만들어지는 데이터를 반영하여 데이터 마트를 유지 보수하는 비용도 증가했습니다. 이미 복잡하게 얽혀있는 데이터에 새로운 데이터를 쌓아올려 비즈니스 이해관계자들이 쉽게 원하는 데이터를 조회할 수 있도록 유지하는 데는 많은 노력이 필요했습니다.&lt;/p&gt;

&lt;p&gt;이 글에서는 쏘카에서 복잡하게 얽혀있던 데이터 마트와 그를 지탱하는 데이터 파이프라인들을 견고하고 재사용 가능하게 만든 유즈케이스를 소개하고, 더 넓은 시각에서 규모를 키워가는 조직에서 데이터와 메타데이터를 관리하는 방법, 그리고 앞으로 나아가야 할 방향에 대해서 이야기하려고 합니다.&lt;/p&gt;

&lt;p&gt;예상 독자는 다음과 같습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모던 DW를 이용해 데이터를 제공하는 데이터 엔지니어&lt;/li&gt;
  &lt;li&gt;데이터 마트 구성과 모델링 등의 고민을 해본 적 있는 데이터 분석가&lt;/li&gt;
  &lt;li&gt;서비스 데이터를 효과적으로 분석하기 좋은 형태로 만드는 방법을 고민하는 백엔드 엔지니어&lt;/li&gt;
  &lt;li&gt;데이터를 이용해 업무를 하는 모든 직군&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;11-쏘카의-데이터-인프라&quot;&gt;1.1. 쏘카의 데이터 인프라&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt1.png&quot; alt=&quot;socar data infra&quot; /&gt;&lt;/p&gt;

&lt;p&gt;쏘카 데이터 파이프라인의 가장 큰 축은 구글 클라우드 플랫폼의 데이터 웨어하우스인 빅쿼리입니다.&lt;/p&gt;

&lt;p&gt;다양한 원천(Raw) 데이터를 빅쿼리로 모으고, 빅쿼리 내의 SQL로 테이블을 생성하는 기능을 활용해 가공한 테이블들을 만들고, 가공한 데이터들을 모아 집계된 테이블들을 만들어 활용합니다.&lt;/p&gt;

&lt;h3 id=&quot;12-양-날의-검&quot;&gt;1.2. 양 날의 검&lt;/h3&gt;

&lt;p&gt;빅쿼리를 중심으로 만들어진 데이터 인프라 구조는 장단점이 명확했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;장점
    &lt;ul&gt;
      &lt;li&gt;스토리지 비용이 비교적 저렴합니다.&lt;/li&gt;
      &lt;li&gt;SQL을 통해서 프로그래밍 지식이 없어도 쉽게 데이터를 조회하고, 변형할 수 있습니다.&lt;/li&gt;
      &lt;li&gt;다양한 소스의 데이터를 쉽게 저장하고, 조회할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;초기 빅쿼리 중심으로 데이터 인프라를 만들면서 쏘카는 기존 시장에서 우위를 점하고 있던 Hadoop과 Spark 기반의 인프라에 비해 빠르게 발전할 수 있었습니다. 많은 선수 지식이 필요하고 데이터 조직을 중심으로 중앙 집중적으로 만들어지고 관리되는 기존 데이터 인프라와 달리, 빅쿼리를 중심으로 한 데이터 인프라는 민주적으로(!) 데이터를 만들어내고 관리할 수 있어 관련 지식이 없더라도 쉽게 데이터에 접근하고 활용할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;단점
    &lt;ul&gt;
      &lt;li&gt;쉽게 넣을 수 있다 보니, 데이터 검증에 대한 고려를 하지 않는 경우가 많습니다.&lt;/li&gt;
      &lt;li&gt;데이터(데이터셋, 테이블) 히스토리와 오너쉽 등을 파악하기 어렵습니다.&lt;/li&gt;
      &lt;li&gt;데이터 간 의존성, 삭제 영향도 또한 파악하기 어렵습니다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;사내 빅쿼리 사용 유저만 해도 전체 300여 명 중 100명이 훨씬 넘는 분들이 빅쿼리를 통해 쏘카의 데이터에 접근하게 되었습니다. 시간이 지나면서 정말 많은 데이터가 빅쿼리에 쌓이게 되었고, 이를 모두 제어하려 하는 일은 여러 조직의 속도를 늦추는 일이 될 수도 있었습니다.&lt;/p&gt;

    &lt;p&gt;같은 데이터를 다루는 테이블도 만드는 사람에 따라 로직이 다를 수 있고 그에 따라 데이터 사용자가 “알아서” 검증을 해야 했고, 이를 개선하기 위한 데이터 마트를 만드는 데이터 엔지니어가 그 검증에 대한 책임을 도맡아야 했습니다. 또한 해당 데이터에 대한 히스토리 파악은 기존 테이블 생성자로부터 직접 전달받거나, 테이블을 생성하는 수 백 줄의 쿼리나 파이썬 코드를 보면서 직접 파악을 해야 했습니다. 때문에 데이터 마트를 추가하는 작업의 난이도가 시간이 갈수록 높아져 갔습니다.&lt;/p&gt;

    &lt;p&gt;테이블을 지우는 일도 신경을 써야 했습니다. 지우려는 테이블이 어떤 곳에 얽혀있는 지도 모르는 채, 테이블을 삭제하면 어떤 사이드 이펙트를 불러올지 몰랐습니다. 혹여나 매우 중요한 데이터 마트 테이블에 엮여있는 테이블들을 삭제한다면, 매출과 같은 중요한 데이터에 이슈가 생길 수도 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위와 같은 단점들은 시간이 지나면 지날수록 발목을 잡았고, 문제들을 해결하기 위한 비용도 급속도로 커져만 갔습니다. 마치 소프트웨어 엔지니어링에서 이야기하는 “&lt;strong&gt;a big ball of mud&lt;/strong&gt;”처럼요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt2.png&quot; alt=&quot;A big ball of mud in Data&quot; /&gt;&lt;br /&gt;
&lt;em&gt;출처: &lt;a href=&quot;https://deviq.com/antipatterns/big-ball-of-mud/&quot;&gt;Big Ball of Mud - DevIQ&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;13-analytics-engineer의-등장&quot;&gt;1.3. Analytics Engineer의 등장&lt;/h3&gt;

&lt;p&gt;위와 같은 문제점들은 저희만 겪고 있는 문제는 아니었습니다.&lt;/p&gt;

&lt;p&gt;Snowflake, Delta Lake 등 최근 부상하고 있는 모던 데이터 웨어하우스를 운영하고 있는 팀들은 비슷한 문제들을 겪고 있었고 각자에 상황에 맞는 방법으로 해결하고 있습니다.&lt;/p&gt;

&lt;p&gt;그러는 와중 저희의 눈에 띈 하나의 큰 흐름은, 데이터를 사용해서 비즈니스적인 의사결정을 도와주는 데이터 분석가와 데이터 파이프라인들을 만드는 데이터 엔지니어 사이에 중간자 적인 역할을 해주는 Analytics Engineer라는 직군이 등장하게 된 것입니다.&lt;/p&gt;

&lt;p&gt;Analytics Engineer의 대표적인 role은 아래와 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data Modeling: 데이터 모델링을 통해 유즈케이스에 맞게 데이터를 관리할 수 있도록 모델링 합니다.&lt;/li&gt;
  &lt;li&gt;Data Warehouse Management: 데이터 웨어하우스를 관리해서 사용자들이 쉽게 데이터를 조회하고 히스토리 등을 확인할 수 있게 합니다.&lt;/li&gt;
  &lt;li&gt;Data Orchestration: 스케줄링 도구 등을 사용해서 데이터가 정상적으로 업데이트 되도록 관리합니다.&lt;/li&gt;
  &lt;li&gt;Setting Best Practices: 모델링과 데이터를 관리하는 사내의 Best Practice를 정의하고 구현하도록 돕습니다.&lt;/li&gt;
  &lt;li&gt;Cross-collaboration: 분석가나 엔지니어와 소통하며 비즈니스 요구 사항을 수집하고, 성공적인 분석 결과를 만들어 내고, 모델링에 다른 조직의 요구 사항을 반영합니다. 또한 소프트웨어 엔지니어링의  실천 방법을 분석에서도 유용하게 사용할 수 있도록 돕습니다 (Version Control, Testing, … )&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;해외의 다양한 회사에서 Analytics Engineer를 채용하고 있습니다.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://about.gitlab.com/job-families/finance/analytics-engineer/&quot;&gt;https://about.gitlab.com/job-families/finance/analytics-engineer/&lt;/a&gt;
&lt;a href=&quot;https://www.glassdoor.com/Job/analytics-engineer-jobs-SRCH_KO0,18.htm&quot;&gt;https://www.glassdoor.com/Job/analytics-engineer-jobs-SRCH_KO0,18.htm&lt;/a&gt;
&lt;a href=&quot;https://www.glassdoor.com/Job/analytics-engineer-jobs-SRCH_KO0,18.htm&quot;&gt;https://www.glassdoor.com/Job/analytics-engineer-jobs-SRCH_KO0,18.htm&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Analytics Engineering의 등장에서 얻을 수 있는 교훈은 1) &lt;strong&gt;“중요한”&lt;/strong&gt; 데이터들을 오너십을 갖고 관리하고 발전시켜 나가는 시스템의 필요성, 2) 일반적인 조회뿐만 아니라 &lt;strong&gt;재사용&lt;/strong&gt; 가능하게 만드는 노력, 그리고 3) 데이터 간의 의존성 및 메타데이터 관리의 중요성이었습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Data Engineer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Analytics Engineer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Data Analyst&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;커스텀 데이터 통합 기능을 만듭니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;클린하고, 정제되고, 분석 가능한 데이터를 제공합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 인사이트 관련 (e.g., 왜 유저 이탈이 지난 달에 높았을까?)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;전반적인 파이프라인 오케스트레이션을 관리합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;소프트웨어 엔지니어링의 베스트 프랙티스를 분석 코드에 적용합니다. (e.g., version control, testing, CI 등)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;비즈니스 이해 관계자와 협업하여 요구 사항을 파악합니다&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 플랫폼을 만들고 유지보수 합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 문서와 정의 등을 유지보수 합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;중요한 대시보드를 만들고 유지보수 합니다.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 웨어하우스의 성능 문제를 최적화합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;비즈니스 데이터 유저가 시각화 툴을 사용할 수 있도록 교육합니다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터를 통해 예측 합니다.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;직군별 비교 (Data Engineer vs Analytics Engineer vs Data Analyst)&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;하지만 어떤 기술이던 그러하듯, 다른 회사에서 작동했던 개념과 기술들이 우리 조직에서도 잘 도입되고 활용되려면 많은 노력과 고려가 필요하다고 생각했습니다. 큰 회사에서처럼 당장 Analytics Engineering만 담당하는 인원을 채용해서 발전시키는 게 현실적으로 어렵기도 하고, 또한 이러한 분야의 발전이 조직에서 필요하다는 컨센서스를 만드는 과정도 시간이 많이 들 수 있는 일이었습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-data-build-tooldbt&quot;&gt;2. data build tool(dbt)&lt;/h2&gt;

&lt;p&gt;dbt는 데이터 엔지니어링의 큰 요소 중 하나인 ETL or ELT (Extract, Transform, Load) 중 &lt;strong&gt;변형(Transform)에 집중합니다&lt;/strong&gt;. 어디서 데이터를 추출해 내고 적재하는지에 대한 관심보다는 “어떻게” 존재하는 데이터를 변형해서, 재사용할지에 대해 고민합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt4.png&quot; alt=&quot;출처: [https://www.getdbt.com/](https://www.getdbt.com/)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;출처: &lt;a href=&quot;https://www.getdbt.com/&quot;&gt;https://www.getdbt.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;dbt를 통해 데이터를 검증, 변형한 후 자동화된 문서와 데이터 리니지(계보)를 제공해서 데이터 사용자가 쉽게 원하는 데이터를 찾아서 쓸 수 있게 만들어 줍니다. 쏘카에서 사용하는 빅쿼리뿐만 아니라, snowflake, Postgres, Redshift, Delta Lake와 같은 다른 데이터 웨어하우스 기술들과도 쉽게 연결해서 사용할 수 있는 integration을 제공합니다.&lt;/p&gt;

&lt;p&gt;가장 매력적인 부분은 대부분의 기능들을 SQL만 알아도 이용할 수 있고, yaml만 조작해도 수백 GB의 큰 테이블에 대한 테스트, 문서 화 등의 일련의 작업들을 쉽게 할 수 있다는 점이었습니다. 쏘카는 위에서
언급한 것과 같이 조직 내 많은 인원들이 SQL을 사용하고 있어서 더욱 적합하다고 판단했습니다.&lt;/p&gt;

&lt;p&gt;또한 데이터 관련 작업들에서 부가적으로 얻을 수 있는 이점들이 꽤 있다고 생각했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;쿼리의 버전 관리&lt;/strong&gt;: SQL을 사용하는 많은 분들이 별도의 버전 관리를 하지 않고 있습니다. dbt를 사용하면 SQL 기반 데이터 파이프라인의 배포를 git의 라이프사이클 안에서 만들어나갈 수 있고, 쿼리에 대한 리뷰 그리고 나아가 데이터에 대한 정보가 흐를 수 있는 문화를 만들어 갈 수 있는 발판이라고 생각했습니다.&lt;/li&gt;
  &lt;li&gt;오너십: ‘주인 없는 데이터’는 분석을 하기에 생각보다 큰 허들입니다. 가져다가 사용하기도, 그렇다고 없애기도 애매하죠. 어떤 로직으로 동작하는지 확인을 하기 위해 SQL을 확인하는 것도 한계가 있을 수 있습니다. dbt를 사용하며 쿼리에 대한 문서화와 히스토리를 작성하는 문화를 만들어가면서 ‘SQL 드리븐 데이터’에서 좀 더 ‘사람 드리븐 데이터’로 옮겨갈 수 있을 거라고 생각했습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사실 dbt 말고도 비슷한 문제를 해결하는 기술들이 아시다시피 꽤 많습니다.&lt;/p&gt;

&lt;p&gt;dbt의 대안으로 생각되는 기술들로는 다음과 같은 기술들이 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ 아래 다른 기술과의 비교는 지극히 개인적인 저의 판단입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;이름&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;장점&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;단점&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;비고&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/dataform-co/dataform&quot;&gt;Dataform&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;BigQuery에 집중. SQL 기반. SaaS 기반 웹 IDE 무료 제공, Looker 연동&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Self Hosting 불가 (as opposed to dbt), 비싼 가격 (당시 월간 유저 당 $95, 지금은 수정된 것 같습니다), Typescript 기반&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/sodadata/soda-sql&quot;&gt;Soda SQL&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;오픈 소스, 직관적인 yaml 활용법 (sql in yaml), Python 기반&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;유저풀이 적음. yaml 관련 오퍼레이션이 많아서 더 복잡함.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;최근에는 yaml + SQL 기반에서 자체적인 DSL(SodaCL)을 만들어 사용&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;a href=&quot;https://github.com/great-expectations/great_expectations&quot;&gt;Great Expectation&lt;/a&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 테스팅 및 분포 UI, 테스팅 preset&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;무겁다. 어렵다.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt5.png&quot; alt=&quot;dbt-to-be&quot; /&gt;&lt;/p&gt;

&lt;p&gt;dbt는 다른 툴들이 각자 가지고 있는 장점들을 (그만큼 뾰족하지는 않더라도) 전반적으로 시도해 볼 수 있는 툴이고 Analytics Engineering을 빠르게 도입하고 구현하는 데 저희 조직에 가장 알맞은 툴이라고 생각했습니다.&lt;/p&gt;

&lt;p&gt;dbt는 두 가지 방법으로 서비스를 제공하고 있습니다. dbt Cloud와 dbt CLI입니다. 전자는 Team Plan 기준 월간 인당 50불(…)의 비용을 내면서 쓸 수 있고, Airflow와 같은 workflow와 Git Integration, Alerting 등의 다양한 기능들을 제공해 줍니다. 후자는 무료로 dbt의 많은 기능들을 전부 이용할 수 있었지만, 자유도가 높은 만큼 러닝 커브와 세팅을 위한 인프라 및 애플리케이션 배포 작업이 필요합니다.&lt;/p&gt;

&lt;p&gt;저희 조직은 분석가 대비 엔지니어가 꽤 많은 (당시 8명) 상황이었고, 세팅을 위한 역량도 충분히 갖춰져 있는 상황이었고 오히려 분석가들 및 SQL 사용자가 모두가 dbt cloud를 사용한다면 지불해야 하는 cost가 더 크다고 생각했습니다. 그래서 dbt CLI를 기반으로 여러 가지 시도를 해보기 시작했습니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;⚠️ dbt에 대한 기본적인 사용법은 이 글에서 다루지 않습니다.&lt;/p&gt;

  &lt;p&gt;더 자세한 dbt에 대한 정보는 dbt 공식 문서와 제 개인 블로그를 참고해 주세요.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://www.getdbt.com/&quot;&gt;dbt - Transform data in your warehouse&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.humphreyahn.dev/blog/efficient-elt-pipelines-with-dbt&quot;&gt;dbt로 ELT 파이프라인 효율적으로 관리하기&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3-유즈-케이스---soda-store-v2&quot;&gt;3. 유즈 케이스 - SODA Store v2&lt;/h2&gt;

&lt;p&gt;SODA Store는 2020년 부터 유지되어왔던 쏘카의 비즈니스와 맞닿아 있는 지표들을 모아놓은 데이터셋을 부르는 별칭입니다. (SOCAR + Data Store = SODA)&lt;/p&gt;

&lt;p&gt;소다스토어에는 사고 집계 데이터, 예약 당시 차량 상태, 분석을 위한 일간 차량 데이터, 사용자 기본 정보 및 예약 누적 데이터 등 쏘카 내의 여러 개의 MSA에 걸쳐서 나눠져 있는 데이터를 집계해 관리하고 있습니다.&lt;/p&gt;

&lt;p&gt;이 파트에는 조금 더 저 수준에서 어떻게 프로젝트를 만들어 나갔고, 어떤 의사 결정과 문제를 해결했는지에 대해서 자세히 이야기해보려고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;31-프로젝트-시작-전&quot;&gt;3.1. 프로젝트 시작 전&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;분석&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;기존 소다스토어는 에어플로우를 통해 빅쿼리로부터 오직 SQL만을 통해서 데이터 전처리를 진행하고, 전처리 및 변형 과정이 끝난 데이터를 모두 빅쿼리에 적재하는 방식으로 테이블을 적재했습니다.&lt;/p&gt;

&lt;p&gt;이 방법은 매우 편하고 관리 포인트도 적었지만 다음과 같은 한계점이 존재했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;로직을 담은 SQL이 매우 비대했습니다. (≥ 500 lines)&lt;/li&gt;
  &lt;li&gt;테스트의 부재 및 실패 시 이유를 찾기 어려움&lt;/li&gt;
  &lt;li&gt;흐르지 않는 도메인 지식 및 이슈 트래킹&lt;/li&gt;
  &lt;li&gt;위의 이유들 때문에 새로운 데이터를 추가하거나 기존 데이터 관련 이슈를 트러블 슈팅하기 어려움&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위와 같은 한계점이 존재함에도 불구하고, 소다 스토어는 총 66만 회 이상의 쿼리에서 참조 되고, 데이터 관련 직군 외의 사업, 운영, 마케팅, 전략 등 다양한 부서에서 고르게 사용되며 비즈니스 의사 결정에 기여하고 있습니다.&lt;/p&gt;

&lt;p&gt;해당 이슈를 해결해야 빠르게 성장하는 쏘카의 비즈니스의 데이터와 관련한 의사 결정에 기여할 수 있도록 할 수 있다고 생각했습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;목표 설정&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 문제를 해결하기 위해서 &lt;code class=&quot;highlighter-rouge&quot;&gt;Analytics Engineering&lt;/code&gt;이 필요하다는 생각을 하게 되었습니다.&lt;/p&gt;

&lt;p&gt;SODA Store에 &lt;code class=&quot;highlighter-rouge&quot;&gt;Analytics Engineering&lt;/code&gt;을 적용하며 가진 목표는 다음과 같았습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SODA Store v2 주요 목표&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존 테이블의 사용성 검토 및 정비&lt;/li&gt;
  &lt;li&gt;신규 서비스 및 요구사항에 따른 마트 테이블 추가 작성&lt;/li&gt;
  &lt;li&gt;외부에 분산된 마트 테이블들의 통합 검토&lt;/li&gt;
  &lt;li&gt;지표 데이터 정리&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Analytics Engineering 주요 목표&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;테이블 테스트 추가 및 정합성 검증&lt;/li&gt;
  &lt;li&gt;테이블 모델링 (정규화, 컬럼 필터링 등)&lt;/li&gt;
  &lt;li&gt;데이터 의존 관계 시각화 (데이터 리니지)&lt;/li&gt;
  &lt;li&gt;데이터 문서화&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;계획&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위 목표를 달성하기 위해 넘어야 했던 가장 큰 허들은 1) dbt라는 툴에 대한 이해, 2) 복잡한 도메인 지식에 대한 심리적 안정감 높이기였습니다.&lt;/p&gt;

&lt;p&gt;때문에 다음과 같이 단계를 나누어서 진행을 하도록 계획했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;프로젝트 구성&lt;/li&gt;
  &lt;li&gt;테이블 모델링 &amp;amp; 테이블 테스트 추가&lt;/li&gt;
  &lt;li&gt;검증&lt;/li&gt;
  &lt;li&gt;배포&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;32-프로젝트-구성&quot;&gt;3.2. 프로젝트 구성&lt;/h3&gt;

&lt;p&gt;소다 스토어의 쿼리는 이미 Airflow를 통해 적재 중인 DAG에 정의가 되어 있습니다.&lt;/p&gt;

&lt;p&gt;프로젝트를 구성할 때는 해당 SQL 파일을 그대로 옮겨와 빠르게 작업을 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;dbt 레포 셋업&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;소다 스토어 v2를 위한 레포 구성은 일반적인 &lt;code class=&quot;highlighter-rouge&quot;&gt;dbt init&lt;/code&gt; 을 통한 구조와는 살짝 달랐습니다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.../socar-data-soda-store
├── .github &lt;span class=&quot;c&quot;&gt;# Github action 관련 &lt;/span&gt;
├── cli &lt;span class=&quot;c&quot;&gt;# 커스텀 CLI 툴&lt;/span&gt;
├── scripts &lt;span class=&quot;c&quot;&gt;# 배포를 위한 스크립트&lt;/span&gt;
└── soda_store &lt;span class=&quot;c&quot;&gt;# dbt 프로젝트 구성에 필요한 모듈&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 커스터마이징을 했던 이유는 소다 스토어의 특성이 있습니다.&lt;/p&gt;

&lt;p&gt;소다 스토어는 데이터 마트로서의 역할도 하지만, SQL 및 로직 자체가 다른 쿼리 사용자가 참고를 많이 하고 있습니다.&lt;/p&gt;

&lt;p&gt;때문에 SQL 문 자체를 다양한 환경에서 사용할 가능성이 있습니다. 예를 들어 BigQuery에서 직접 해당 SQL을 사용하는 것도, 또는 애플리케이션 또는 Airflow 등의 전반적인 python 환경에서
sql 문을 import 해서 사용하거나, 아니면 cli로 사용하는 경우도 생길 수 있을 거라고 생각했습니다.&lt;/p&gt;

&lt;p&gt;때문에 dbt 파일 구조를 공식 문서에서 가이드 하는 대로 하지 않고 일반적인 python 라이브러리 구조와 같이 가져간다면 패키징과 빌드에 적합한 형태로 만들어갈 수 있고, 쏘카 사내 Pypi 레포 등을 통해서
소다 스토어에서 빅쿼리에 있는 데이터와 로직을 그대로 사용해서 데이터를 추가적으로 정제할 필요 없이 그대로 사용할 수 있다는 이점이 있습니다.&lt;/p&gt;

&lt;p&gt;또한 Profile에 대한 설정(&lt;code class=&quot;highlighter-rouge&quot;&gt;profiles.yml&lt;/code&gt;)을 변경했습니다. dbt에서 profile은 데이터웨어하우스 (e.g., BigQuery, Snowflake, Delta Lake 등)에 대한 설정부터,
개발 및 운영 환경에 대한 정보를 다루는 가장 중요한 설정에 대한 정보가 담겨있는 파일이라고 할 수 있습니다. 때문에 dbt init을 통해 프로젝트를 생성하면 기본적으로 유저의 &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.dbt&lt;/code&gt; 디렉토리에
profiles를 생성하고 dbt cli가 해당 디렉터리를 바라보도록 설정이 되어있습니다.&lt;/p&gt;

&lt;p&gt;소다 스토어를 구성하면서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;profiles.yml&lt;/code&gt; 을 &lt;code class=&quot;highlighter-rouge&quot;&gt;soda_store&lt;/code&gt; 즉, dbt 디렉토리 안으로 가져왔습니다. 그 이유는 재사용성을 위함이었습니다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jaffle_shop:
  target: dev
  outputs:
    dev:
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: postgres
      host: localhost
      user: alice
      password: &amp;lt;password&amp;gt;
      port: 5432
      dbname: jaffle_shop
      schema: dbt_alice
      threads: 4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위는 profiles.yml 파일의 예제입니다. 파일을 보시면 직접 host, user, pw 등 데이터웨어하우스에 대한 credential 정보를 직접 입력할 수 있게 됩니다. 작업자가 혼자라면, 위와 같이 관리해도 되지만 공동으로 작업을 하고 빠르게 CI/CD 파이프라인을 통해서 배포하게 되려면 인프라 레벨에서 환경 변수를 통해서 오버라이딩하기 쉽게 만들어주는 과정이 필요하다고 생각했습니다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;soda_store:
  target: dev
  outputs:
    dev:
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: bigquery
      method: service-account
      project: ...
      dataset: ...
      threads: 1
      timeout_seconds: 300
      priority: interactive
      maximum_bytes_billed: 1000000000000 &lt;span class=&quot;c&quot;&gt;# 1TB&lt;/span&gt;
      keyfile: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;
    ci:
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: bigquery
      method: service-account
      project: ...
      dataset: ...
      threads: 10
      timeout_seconds: 3000
      priority: interactive
      maximum_bytes_billed: 1000000000000 &lt;span class=&quot;c&quot;&gt;# 1TB&lt;/span&gt;
      keyfile: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;
    live:
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: bigquery
      method: service-account
      project: ...
      dataset: ...
      threads: 5
      timeout_seconds: 300
      priority: interactive
      keyfile: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dbt에서는  `` 와 같이 yaml 상에 환경 변수를 런타임에서 치환할 수 있게 하는 편의 기능을 제공할 수 있어, 쿠버네티스 환경에서 configmap과 secret을 활용해서 volume mount 및 env를 애플리케이션에 적용해 각기 다른 환경에도 쉽게 배포를 할 수 있도록 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;환경별 셋업 (dbt Profile + Infra)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CI(PR Check)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;CI 환경에서는 PR 단계에서 만든 유저의 변경 사항들을 적용해서 dbt test 및 run 커맨드를 실행합니다. 해당 변경 사항이 기존의 데이터 validation 테스트를 깨뜨리지는 않는지 확인하고, 테스트가 통과하면 머지 할 수 있도록 branch protection rule을 적용했습니다.&lt;/p&gt;

&lt;p&gt;이 부분에서 사실 퍼포먼스를 극적으로 올려줄 수 있는 부분이 있는데 이는 뒷부분  배포)에서 다루겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;개발 환경 (dev)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;개발 환경은 개발 데이터 웨어하우스에 연결해서 운영에 적용하기 전 전체 테이블들을 테스트하고 재구성 해보기 위해 만들었습니다.&lt;/p&gt;

&lt;p&gt;작업 코드 베이스에서 main에 머지 되면 CI/CD (Github Action) 파이프라인을 통해서 자동 실행되고, 실패 시 운영 배포를 할 수 없습니다.&lt;/p&gt;

&lt;p&gt;모든 과정이 에러 없이 통과된다면, 개발 Docker Image를 도커 레지스트리에 푸쉬합니다. 해당 이미지는 Airflow 등의 환경에서 테스트를 하기 위해 사용됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;운영 환경 (live)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;운영 환경에서는 여러 가지 시행착오가 있었습니다.&lt;/p&gt;

&lt;p&gt;운영을 위한 dbt 이미지를 만드는 과정은 개발 환경과 다르지 않지만, 해당 이미지를 사용하기 위한 런타임 환경을 만드는 과정이 다양했습니다.&lt;/p&gt;

&lt;p&gt;고려 및 테스트했던 환경은 1) 일반 도커 이미지 실행 2) dbt RPC, 3) Python 웹서버 4) 도커 이미지 실행 + Object Storage 등이 있습니다. 이에 대한 시행착오와 문제 상황은 아래 **Step #4tep #2 - 모델링 &amp;amp; 테스팅&lt;/p&gt;

&lt;p&gt;다음은 기존의 쿼리를 재사용하기 좋은 방법으로 모델링하고, 테스트 가능하게 만드는 과정이 필요했습니다.&lt;/p&gt;

&lt;p&gt;기존의 쿼리가 100개 이상의 테이블들을 조인해서 데이터 마트를 만들어내야 했기에 “적절하게” 쿼리를 리팩토링하고 나누는 과정은 매우 어려웠는데요.&lt;/p&gt;

&lt;p&gt;dbt 공식 문서에서는 fishtown analytics(dbt labs의 전신)의 모델링 패턴을 소개합니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355&quot;&gt;How we structure our dbt projects&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview&quot;&gt;How we structure our dbt projects - dbt Docs&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;최근에 공식 문서 안으로 들어갔네요!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt;: 원천 테이블 또는 서드파티 데이터. 소스 테이블 스키마를 따릅니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Staging Models&lt;/strong&gt;: 데이터 모델링의 가장 작은 단위. 각 모델은 소스 테이블과 1:1 관계를 갖습니다.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Staging Model 규칙&lt;/p&gt;

        &lt;p&gt;Staging Models는 아래와 같은 규칙을 가집니다.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stg_&lt;/code&gt; 접두사로 구분한다. (e.g., stg_member__chat)&lt;/li&gt;
          &lt;li&gt;컬럼 네이밍, 타입을 일관적인 방법으로 정리되어 있어야 한다.&lt;/li&gt;
          &lt;li&gt;데이터 클렌징이 완료되어 있어야 한다.&lt;/li&gt;
          &lt;li&gt;PK가 유니크하고 Non-null 이어야 한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Staging Model 예제&lt;/p&gt;

        &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ├── dbt_project.yml
  └── models
      ├── marts
      └── staging
          ├── braintree
          └── stripe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stg_&amp;lt;source&amp;gt;__&amp;lt;object&amp;gt;&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;보통 view로 materialize 된다 (↔ table)&lt;/p&gt;

            &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ├── dbt_project.yml
  └── models
      ├── marts
      └── staging
          └── braintree
              ├── src_braintree.yml
              ├── stg_braintree.yml
              ├── stg_braintree__customers.sql
              └── stg_braintree__payments.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;1 Staging Model, 1 Source Definition&lt;/p&gt;

            &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ├── dbt_project.yml
  └── models
      ├── marts
      └── staging
          └── braintree
              ├── base
              |   ├── base.yml
              |   ├── base_braintree__failed_payments.sql
              |   └── base_braintree__successful_payments.sql
              ├── src_braintree.yml
              ├── stg_braintree.yml
              ├── stg_braintree__customers.sql
              └── stg_braintree__payments.sql
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;staging base 모델로 공통 로직 모듈화 시키기도 합니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mart Models&lt;/strong&gt;: 비즈니스와 맞닿아 있는 데이터들을 다루는 모델
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Mart Model 예제&lt;/p&gt;

        &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ├── dbt_project.yml
  └── models
      ├── marts
      |   ├── core
      |   ├── finance
      |   ├── marketing
      |   └── product
      └── staging
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;p&gt;기본적인 Mart 모델은 위와 같이 비즈니스 또는 조직의 구조와 닮아있습니다.&lt;/p&gt;

        &lt;p&gt;Mart 모델은 fact와 dimension 모델로 구성됩니다.&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;fct_&lt;verb&gt;: 길고 좁은 테이블 (컬럼이 적고, 로우가 많은), 현실의 프로세스들을 나타내 줘야함. e.g., sessions, transactions, orders, stories,
votes&lt;/verb&gt;&lt;/li&gt;
          &lt;li&gt;dim_&lt;noun&gt;: 짧고 넓은 테이블 (컬럼이 많고, 로우가 적은), 각 row가 사람, 장소, 사물 등. 변경가능하지만, 변경 주기가 긴 것들이 온다. e.g., 고객, 상품, 빌딩, 직원 등&lt;/noun&gt;&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;Mart 모델에서 유용한 패턴&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;쿼리 퍼포먼스를 위해 fact와 dimension 테이블들은 table로 materialize 한다&lt;/li&gt;
          &lt;li&gt;중간 변환 과정의 테이블들을 Mart 모델 안에서 남겨둔다.&lt;/li&gt;
          &lt;li&gt;각 모델의 테스트 yaml을 추가한다&lt;/li&gt;
          &lt;li&gt;md 파일로 문서화&lt;/li&gt;
        &lt;/ul&gt;

        &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ├── dbt_project.yml
  └── models
      ├── marts
      │   ├── core
      │   │   ├── core.md
      │   │   ├── core.yml
      │   │   ├── dim_customers.sql
      │   │   ├── fct_orders.sql
      │   │   └── intermediate
      │   │       ├── customer_orders__grouped.sql
      │   │       ├── customer_payments__grouped.sql
      │   │       ├── intermediate.yml
      │   │       └── order_payments__joined.sql
      │   ├── finance
      │   ├── marketing
      │   └── product
      └── staging
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;

        &lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gitlab에서는 위와 같은 Fishtown Analytics의 모델링 패턴을 참고해서 팀 고유의 모델링 패턴들을 만들어 나가고 있습니다. (놀랍게도, SQL을 포함한 모든 소스 코드가 public입니다)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://about.gitlab.com/handbook/business-technology/data-team/platform/dbt-guide/#model-structure&quot;&gt;dbt Guide&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;쏘카에서도 위 패턴을 좀 더 활용해 독자적인 모델링 패턴을 사용했습니다.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;models/
	|- mart
	|- ods &lt;span class=&quot;c&quot;&gt;# NEW!&lt;/span&gt;
	|- staging
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ODS는 엔터프라이즈 데이터웨어하우스(EDW)에서 사용되는 개념으로, 임시로 운영계 데이터를 보관하는 장소입니다. Mart에 레이어에 본격적으로 마트 테이블들을 생성하기 전에, ODS에 메인 로직들을 저장해 두고 Mart에서는 해당 ods 레이어의 모델을 레퍼런스 해서 사용합니다.&lt;/p&gt;

&lt;p&gt;ODS 레이어를 추가하게 된 가장 큰 이유 중 하나는, 소다 스토어의 경우에는 기존에 존재하는 데이터(v1)와 값을 비교를 하기 위해서입니다. ODS에서는 메인 로직들을 담아두고, (대부분의 케이스에서) View로 모델을 생성합니다. 그 덕분에 Mart에서는 ODS 모델을 레퍼런스 해서 Table로 Materialize를 시키면, 해당 작업이 시작한 시점 기준으로 테이블들을 만들 수 있어 기존에 존재하는 v1 데이터와 적재
시점을 동일하게 만들어 데이터를 비교할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 가독성을 위해서도 필요했습니다. Staging에서는 원천 테이블에 대한 가공, Mart에서는 Staging에 있는 모델들을 조합해서 비즈니스 지표에 가까운 테이블을 만들어 낸다면, Mart에 로직이 대부분 집약되어서 부수 테이블이 많이 만들어져서 결국 mart 테이블들의 데이터들을 이해하기 어려워졌습니다. ODS에 메인 로직들을 분산해서 저장해 두고, Mart는 최대한 &lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT ... FROM ... WHERE&lt;/code&gt;
로 표현해서 비즈니스 지표들을 찍어낼 수 있다고 판단했습니다.&lt;/p&gt;

&lt;p&gt;해당 부분은 모든 dbt 프로젝트에 적용하기보다는 기존 쿼리 및 데이터 비교를 위한 작업이라면 참고해 볼 수 있을 패턴이라고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;34-검증&quot;&gt;3.4. 검증&lt;/h3&gt;

&lt;p&gt;소다 스토어에서 검증해야 하는 것은 명확했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;기존 데이터와 “정확히” 일치해야 한다.&lt;/li&gt;
  &lt;li&gt;기존 데이터에서 논리적 및 통계적인 오류가 없어야 한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위 요구 사항은 모두 dbt의 테스트 기능을 활용해서 해결했습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;기존 데이터와 “정확히” 일치해야 한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;기존 데이터를 dbt 코드 베이스에 Source 테이블로 만들어서 검증하는 방법도 있었지만, 매번 기존 테이블을 가져와서 새로운 테이블로 만드는 건 비효율적이라고 생각했습니다. 때문에 dbt 코드에 포함하지 않고 Macro를 통해서 쿼리를 이용해서 아래와 같이 검증하도록 만들었습니다.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;

&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compare_v1_and_v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1_soda_store_dataset_ref&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'v1_table'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1_soda_store_dataset_ref&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;distinct&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ods_&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;union&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;all&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;project&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ods&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ods_&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;distinct&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1_soda_store_dataset_ref&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;{{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_name&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endtest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mart_model&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;compare_v1_and_v2&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;model_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;table_from_v1&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;column1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;컬럼1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;기존 데이터에서 논리적 및 통계적인 오류가 없어야 한다&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 부분은 dbt에서 기본으로 제공하는 Generic Tests (unique, not_null, accepted_values, relationships) 뿐만 아니라 여러 dbt 패키지들을 사용했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dbt-labs/dbt-utils&quot;&gt;dbt-labs/dbt-utils&lt;/a&gt;: dbt 공식으로 제공하는 여러 가지 매크로들이 내장되어 있습니다. 매크로를 위한 쿼리를 추가적으로 작성할 필요 없이 테이블에 대한 검증 (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;unique_combination_of_columns&lt;/code&gt;)과 select 문에서 사용할 column renaming 매크로 또한 제공합니다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/calogica/dbt-expectations&quot;&gt;calogica/dbt_expectations&lt;/a&gt;: 위에서 언급했던 Great Expectation에서 제공하는 다양한 검증 preset들을 매크로로 구현해둔 패키지입니다. 테이블 row 또는 column 별 통계 값에 대한 검증이나 String Matching, Distribution function 등 GE 대비 dbt의 단점들을 보완해 줄 수 있을 만큼 많은 매크로들이 존재합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예를 들어 다음과 같은 테스트를 추가해 보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;your&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;your&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;tables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;your&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'your table'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dbt_expectations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expect_compound_columns_to_be_unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;column_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;col1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;col2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;&quot;col3&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dbt run을 통해서 아래와 같은 SQL로 변환됩니다.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_errors&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;col2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;col3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;`your-database`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`your-schema`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;`your-table`&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;having&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;select&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_errors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HERE&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dbt로 만들어진 테스트들은 마지막 SELECT 문에서 0개 이상의 로우가 리턴되면 Fail, 아닌 경우에는 Pass 하도록 기본 설정되어 있습니다. 자세한 설명은 dbt test 공식 문서를 참고해 주세요.&lt;/p&gt;

&lt;p&gt;위와 같은 패키지를 통해 쉽게 테스트를 yaml만 수정해서 추가할 수 있었고, 덕분에 기존 파이프라인에 안정성을 더할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;35-배포&quot;&gt;3.5. 배포&lt;/h3&gt;

&lt;p&gt;데이터 엔지니어링 그룹의 대부분의 애플리케이션들을 GitOps Principle에 따라서 배포가 됩니다. 따라서 dbt 기반 레포도 마찬가지의 절차를 따라서 구성했습니다. 다만 일반 파이썬 애플리케이션과 달리 배포 전 고려해야 하는 사항들이 꽤 존재합니다.&lt;/p&gt;

&lt;p&gt;배포는 다음과 같은 절차로 이루어집니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;PR 생성&lt;/li&gt;
  &lt;li&gt;CI Test&lt;/li&gt;
  &lt;li&gt;메인 브랜치 머지&lt;/li&gt;
  &lt;li&gt;dev 환경 Update&lt;/li&gt;
  &lt;li&gt;Release / 운영(live) 환경 Update&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;소다 스토어 dbt 레포에 테이블을 추가하기 위해 PR을 생성하면 Github Action을 통해 PR의 변경 사항을 감지하고, 정상적으로 SQL 파싱 되고 테스트를 통과하는지 테스트합니다. 아래와 같은 Github Action Workflow를 구성했습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Check PR&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;pull_request&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;branches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;main&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;check_pr&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Incremental CI&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;bash scripts/ci.sh incremental&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;GCLOUD_SERVICE_KEY&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;SERVICE_ACCOUNT_JSON_PATH&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;특별한 부분은 없고, Incremental CI라는 파트가 궁금하실 텐데요. 이 부분은 Slim CI를 다루는 아래 파트에서 이야기하겠습니다. Incremental CI를 통해서 변경이 된 부분만 감지해서, 필요한 부분만 생성할 수 있습니다.&lt;/p&gt;

&lt;p&gt;CI 테스트가 통과된 PR만 main 브랜치로 머지 할 수 있도록 branch protection 룰을 설정해서 main을 언제나 배포가 가능한 형태로 유지하고 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 PR 테스트를 하고 main 브랜치로 머지되고 나면, 개발 환경에 배포를 하기 위해 dbt의 개발 target에 모든 파이프라인을 run 합니다. (Full CI)&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Dev Publish&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;branches&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;main&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;dev_publish&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;!startsWith(github.ref,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'refs/tags/v')&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;!startsWith(github.event.head_commit.message,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bump:')&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Full CI&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;bash scripts/ci.sh full&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;GitOps Update Dev&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gitops-update-dev&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;continue-on-error&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;git clone https://github.com/baloise/gitopscli.git&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;pip3 install gitopscli/&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;gitopscli deploy --git-provider-url &quot;https://github.com&quot; \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--username $GITHUB_USERNAME \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--password $GITHUB_PASSWORD \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--organisation $GITHUB_ORGANIZATION \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--repository-name $GITHUB_REPOSITORY_NAME \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--file $VALUES_FILE \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--values &quot;{app.container.image.tag: v${GITHUB_SHA::7}&quot; \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--git-user &quot;&amp;lt;your-git-user&amp;gt;&quot; \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--git-email &quot;&amp;lt;your-git-user-email&amp;gt;&quot; \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--commit-message &quot;&amp;lt;your-commit-message&amp;gt;&quot; \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--create-pr \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--auto-merge \&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;--merge-method=&quot;squash&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;GITHUB_USERNAME&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;GITHUB_PASSWORD&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;GITHUB_ORGANIZATION&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;GITHUB_REPOSITORY_NAME&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;VALUES_FILE&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;lt;your-values.yaml&amp;gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이 단계가 통과되면, 개발 환경 데이터웨어하우스에 dbt로 메인 로직을 전부 테스트한 상황이 되고, 운영 배포로 Airflow를 통해서 주기적으로 배포할 수 있는 환경이 됩니다.&lt;/p&gt;

&lt;p&gt;또한, dbt에서 제공하는 lineage 웹페이지를 helm chart로 구성해 운영하고 있기 때문에 변경 사항을 helm chart 레포에도 전달해 줘야 합니다. 때문에 GitOps CLI라는 라이브러리를 사용해서 해당 레포에 values.yaml에 정의되어 있는 개발 container image tag를 변경해 줍니다.&lt;/p&gt;

&lt;p&gt;마지막으로 운영 환경을 위해 release를 하는 단계만 남았습니다. 운영은 개발 환경 업데이트 보다 쉽습니다. Full CI를 통해서 전체 파이프라인을 재실행할 필요가 없기 때문입니다. 운영에서는 전체 재실행을 하지 않는 이유는 Airflow를 통해서 실행되는 운영 dbt 도커 이미지의 결과와 충돌할 수 있기 때문입니다. 같은 로직을 개발 환경에서는 실행하고 통과했다는 전제하에 운영 이미지를 release하는 파이프라인에서는 Docker Build, Push, Config Repo Update, Github Release 생성 정도만 하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slim CI&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*OnGT6eelEiLpu2q7bE0OEA.jpeg&quot; alt=&quot;Setup a Slim CI for dbt with BigQuery and Docker&quot; /&gt;
&lt;em&gt;출처: &lt;a href=&quot;Setup a Slim CI for dbt with BigQuery and Docker&quot;&gt;Setup a Slim CI for dbt with BigQuery and Docker&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Slim CI는 쉽게 말해서 dbt의 실행 결과를 캐싱해서 필요한 부분만 재실행할 수 있도록 하는 테크닉입니다. dbt cloud에서는 쉽게 사용할 수 있도록 설정이 되어있지만, dbt cli 유저라면 직접 구현을 해줘야하는 부분입니다.&lt;/p&gt;

&lt;p&gt;간단한 원리를 설명하자면, dbt run을 했을 때 dbt는 local state를 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt;에 저장을 해둡니다. 이 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt;이 정상적으로 저장된 정보를 불러올 수 있으면 Graph selector(&lt;code class=&quot;highlighter-rouge&quot;&gt;state:modified+&lt;/code&gt;)로 변경된 사항들만 run을 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 어려운 부분은 저희 조직의 데이터 파이프라인의 대부분은 Docker를 통해서 로직을 실행하고, 따로 볼륨을 관리하고 있지 않아서 dbt 런타임에서 실행하고 컨테이너 내부에 저장된 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt;을 다시 불러오기 힘든 환경이라는 점이었습니다.&lt;/p&gt;

&lt;p&gt;처음에는 Kubernetes Persistent Volume을 사용해서 영속하는 걸 생각했지만, 그렇게 됐을 때는 dbt 런타임을 계속해서 띄워두고 여러 소스에서 해당 dbt pod에 요청을 하는 형태가 되어야 했습니다. 해당 pod 관리 코스트 측면에서 안 좋을 거라고 생각을 했습니다.&lt;/p&gt;

&lt;p&gt;다른 방법은 dbt rpc를 이용하는 방법이었습니다. &lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_procedure_call&quot;&gt;Remote Procedure Call (RPC)&lt;/a&gt; 방식으로 dbt 서버에 요청을 보내서 작업을 수행하게 하는 인터페이스를 dbt cli에서는 제공을 하고 있습니다 (&lt;a href=&quot;https://github.com/dbt-labs/dbt-rpc&quot;&gt;Github Repo&lt;/a&gt;). 하지만 이 방식은 현재 deprecated된 상태로, dbt Server라는 기능으로 대체되어 2022년 이후에 완전히 지원을 중단할 예정입니다. 때문에 지속 가능하지 않은 방법이라고 생각해서 이 방법도 (시도는 했으나) 선택하지 않았습니다.&lt;br /&gt;
최종적으로 선택한 방법은 CI/CD 파이프라인을 최대한 이용해서 도커 이미지에 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt;을 포함시키는 방법이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt%20ci.png&quot; alt=&quot;dbt ci&quot; /&gt;&lt;/p&gt;

&lt;p&gt;동작시키기 위해서는 두 개의 Dockerfile이 필요합니다. 1) &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile.full.ci&lt;/code&gt;, 2) &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile.incremental.ci&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;과정은 다음과 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(on PR create) → Incremental CI에서 Full CI 이미지 기반으로 도커 이미지를 빌드하고 레지스트리에 저장합니다.&lt;br /&gt;
 1-1. Full CI 이미지의 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt; 파일이 있으면, 아래와 같은 코드로 현재 코드의 변경 사항만 dbt run을 적용할 수 있습니다.&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ```bash
 #!/bin/bash
 ...
    
 dbt run --target=ci -s &quot;state:modified+1 1+exposure:*,+state:modified+&quot; --defer --state &amp;lt;your-state-directory&amp;gt; -x --full-refresh
 dbt test --target=ci -s &quot;state:modified+1 1+exposure:*,+state:modified+&quot;  --defer --state &amp;lt;your-state-directory&amp;gt; -x
 ```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;1-2. 이 시점에 빌드를 하고 있는 컨테이너 안에는 이전 Full CI가 실행될 때의 코드 + 현재 PR에서의 변경 사항이 합쳐진 &lt;code class=&quot;highlighter-rouge&quot;&gt;manifest.json&lt;/code&gt; 이 저장됩니다.&lt;br /&gt;
 1-3. 해당 이미지를 빌드 완료 후 push 합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;(on main branch merge) → Full CI에서 개발(dev) 환경으로 전체 dbt 모델들을 실행하고, 도커 이미지를 만든 뒤 결과물을 레지스트리에 저장합니다. (실패 시, 운영 배포할 수
없습니다.)&lt;/li&gt;
  &lt;li&gt;(on release) → 운영(live) 환경으로 dbt 모델들을 실행하지 않고, 이미지만 빌드하고 푸시 합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위와 같은 방식으로 최대 12분 정도 걸리던 PR Check 파이프라인을 1분 내외로 단축시켰습니다. 저희 유즈케이스처럼 테이블이 많은 경우 (100+) 퍼포먼스를 크게 향상시킬 수 있는 예제 같습니다.&lt;/p&gt;

&lt;p&gt;굳이 도커 기반으로 푸는 것 말고도 다양한 방법으로도 이런 문제를 해결해볼 수 있을 거라고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;36-결과&quot;&gt;3.6. 결과&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Seamless Change&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;v1 데이터와 v2 데이터가 정확하게 일치하는 것을 확인하고, v1 로직을 v2로 100% 교체해 데이터 사용자분들이 어떤 쿼리 수정이 없이도, 더 신뢰할 수 있고 더 많은 마트 테이블들을 사용하실 수 있게 매끄럽게 교체 작업이 이루어졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Lineage Graph&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt9.png&quot; alt=&quot;소다 스토어 v2 Data Lineage ~~(흐린 눈)~~&quot; /&gt;&lt;/p&gt;

&lt;p&gt;소다 스토어 v2 Data Lineage &lt;del&gt;(흐린 눈)&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;dbt로 이관 후, 소다 스토어를 구성하기 위해 필요한 수많은 테이블들, 그리고 의존관계가 어떻게 흐르는지를 확인할 수 있는 방법이 생겼습니다. 이로 인해 유사시에 어떤 테이블의 오류가 다른 마트 테이블들에 영향을 주는지 시각적으로 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New SODA Store DAG&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt_10.png&quot; alt=&quot;Airflow DAG (부제: 일했다 dbt)&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/img/analytics-engineering-with-dbt/dbt_11.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;em&gt;Airflow DAG (부제: 일했다 dbt)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;소다 스토어 v2가 2022년 4월 마무리되고 3개월 뒤인 7월까지 에러를 한 번도 낸 적이 없어 아쉬웠습니다. 다행히(?) 이 글을 쓰고 있는 와중에 해당 테이블에 새로운 서비스 관련 데이터가 추가 되어 한 컬럼에 기존에 정의되지 않았던 값이 들어오자 dbt가 제 역할을 해주었습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-그-밖에-시도했던-것들&quot;&gt;4. 그 밖에 시도했던 것들&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sqlfluff linting&lt;/strong&gt;: 초반에 SQLFluff를 이용해서 사용자들의 쿼리 스타일을 모두 통일하려고 했습니다. 하지만 아직 dbt Integration이 생긴지 얼마 지나지 않아서인지 CI 단계에서 설정해둔 파이프라인이 깨지는 일이 많았습니다. 현재에는 Github Action에서 빼놓은 상태입니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;dbt custom cli tool&lt;/strong&gt;: dbt cli도 많은 기능을 제공하지만, 저희의 유즈케이스에서는 기존에 있는 테이블 dbt 코드 베이스로 이관을 해야 하고, 써줘야 하는 코드가 매우 많았습니다 (e.g. schema.yaml, source.yaml의 테이블과 column, description 등). 때문에 dbt cli를 래핑 해서 빅쿼리에 저장되어 있는 메타데이터 정보를 dbt에서 사용하는 yaml로 만들어주는 툴이나, 기존 쓰인 sql을 dbt에서 사용하는 jinja sql로 변환하는 기능 등 저희의 입맛에 맞게 커스텀 툴을 만들어서 사용했습니다. (혹시나 궁금해하시는 분들이 있다면 추후에 오픈소스화하겠습니다)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Airflow Task Dependency Tree&lt;/strong&gt;: Astronomer의 &lt;a href=&quot;https://www.astronomer.io/blog/airflow-dbt-1/&quot;&gt;이 글&lt;/a&gt;처럼 dbt model을 run → test 하는 과정 하나하나를 전부 Airflow Task로 만들려는 시도를 했습니다. dbt lineage 그래프처럼 airflow를 통해서도 시각적으로 볼 수 있을 거라고 기대했지만, 아마 다들 예상하시다시피, task가 너무 많아서 다양한 문제들이 발생했습니다. 때문에 현재와 같이 하나의 mart를 생성하는 task들로 구성하게 되었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-알면-좋았던-것들&quot;&gt;5. 알면 좋았던 것들&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;모델링보다 테스트를 먼저&lt;/strong&gt;&lt;br /&gt;
모델링 보다 source - staging - ods - mart에 이르는 레이어에 대한 테스트가 선행되었다면 뒤에 있었을 안 맞는 데이터들을 맞추는 시간이 덜 들지 않았을까… 코드처럼 TDD를 했으면 더 시간이 덜 들었을 거 같습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;공동 작업을 위한 환경 및 배포 파이프라인 구성&lt;/strong&gt;&lt;br /&gt;
일반적인 애플리케이션들이 그러하듯, dbt 기반 data pipeline 또한 배포 파이프라인을 먼저 구성해두고 즉각적으로 변경 사항들이 피드백 루프를 만들도록 구성했으면 커뮤니케이션 코스트가 덜 들 수 있지 않았을까 싶습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-개선해야-하는-점들&quot;&gt;6. 개선해야 하는 점들&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;현재 특정 유즈케이스에서만 동작하도록 설정을 해뒀습니다. 더 범용적으로 dbt를 사용할 수 있도록 확장성 있게 레포를 구성하는 게 필요합니다.&lt;/li&gt;
  &lt;li&gt;지속적으로 Analytics Engineering과 관련 기술을 습득하기 위해서는 많은 노력이 필요합니다. 앞으로 어떻게 이 구조를 유지할지 고민이 더 필요할 듯합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;7-나아가서&quot;&gt;7. 나아가서&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Metric Store&lt;/strong&gt;&lt;br /&gt;
dbt를 활용한 Analytics Engineering이 시작되자, 최신 트렌드인 &lt;a href=&quot;https://medium.com/@thekensta/making-sense-of-metric-stores-c85faba231a&quot;&gt;Metric Store&lt;/a&gt;도 도입을 하려고 하는 움직임들이 생겨났습니다.&lt;br /&gt;
dbt도 메트릭 스토어로서의 기능도 제공을 하고 있지만, 아직 초기 단계이다 보니 적극적으로 도입하고 있지 않은 상황입니다. 반복적인 업무를 해야 하는 데이터 분석가 또는 사이언티스트 분들의 작업을 간소화할 수 있고 나아가 자동화할 수 있는 부분이라고 생각합니다.&lt;br /&gt;
관련해서 dbt-core의 이슈에서 창업 멤버인 Drew Banin이 해당 기능을 제안한 &lt;a href=&quot;https://github.com/dbt-labs/dbt-core/issues/4071&quot;&gt;이슈 문서&lt;/a&gt;가 매우 인상적이었습니다. 한번 살펴보시는 걸 추천드립니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Data Service (In House)&lt;/strong&gt;&lt;br /&gt;
이 모든 걸 아우르는 반복적 업무들을 개선하기 위한 자체 인하우스 데이터 서비스를 개발하고 불필요한 일련의 과정들을 내재화 시키는 것이 필요하다고 생각합니다.&lt;br /&gt;
dbt 기반 SQL 작성과 테이블 의존관계 확인, 간단한 구성으로 metric으로 만들어서 재사용할 수 있는 기능 등을 생각하고 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;8-마치며&quot;&gt;8. 마치며&lt;/h2&gt;

&lt;p&gt;dbt와 Analytics Engineering에 대한 제 의견을 종합하자면,&lt;/p&gt;

&lt;p&gt;&lt;em&gt;이런 장점이 있어요&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;분석가들의 업무를 개발자 처럼&lt;/strong&gt;: 일회성 또는 요청 기반의 분석가들의 업무가 Git 중심, CI 파이프라인을 활용한 지속적 테스팅, 지속적 배포 등의 도구를 사용해 더욱 생산성을 높힐 수 있습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;높아지는 데이터의 재사용성&lt;/strong&gt;: dbt라는 기술을 통해 데이터 웨어하우스의 기능을 극대화 시킬 수 있습니다. 단순 테이블만 적재하고 쿼리하던 구조에서 벗어나 팩트/디멘션 테이블을 고민하고, View, Materialized View 등의 데이터 웨어하우스 기능들을 활용하거나, 데이터 리니지 그래프를 보며 기존 데이터를 어떻게 더 활용할 지, 또한 데이터 웨어하우스의 모델링에 대한 고민을 시작하게 되었습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;견고해지는 데이터 파이프라인&lt;/strong&gt;: 데이터 테스팅, 문서화, 오너십 등의 기능들 그리고 위에 언급한 이점들 덕분에 기존 SQL 기반 데이터 파이프라인을 더욱 견고하게 만들 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;이런 어려움들이 있어요&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;배포 및 버저닝&lt;/strong&gt;: 분석가로만 구성된 팀에서는 시작하기 어려울 거 같습니다. 지속적인 배포와 버저닝 전략 등 전통적인 소프트웨어 엔지니어 팀에서 할 고민들과 팀의 해법이 정리되어야 제대로 사용하실 수 있다는 생각입니다. 개발 조직의 도움을 받거나 시간을 좀 더 들여서 팀의 입맛에 맞게 사용하는 기간을 더 두시길 추천합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;러닝 커브&lt;/strong&gt;: 초기 러닝 커브가 꽤 높은 툴이라고 생각합니다. 기본 기능만 사용하게 되신다면 문제가 없을 수 있으나, 운영 환경에서 중요한 데이터를 다뤄야할 때 난관에 봉착할 수 있습니다. 국내 사용자가 (아직은) 많지 않으므로 공식 문서와 dbt slack 커뮤니티 등을 이용해서 답을 찾아 나가야 합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;개발 문화&lt;/strong&gt;: 결국 dbt에서 가장 중요한 건 Analytics Engineering이 조직의 문화와 현재 상황에 잘 맞는 가? 라는 질문이라고 생각합니다. 분석가와 데이터를 다루는 누구나 반복되는 업무와 밀려들어오는 요청으로 인해 재사용성과 테스트 등의 우선순위가 떨어질 수 있습니다. dbt를 도입하는 것은 많은 문제를 해결할 수 있으나, 은총알은 아니기에 많은 고민이 필요한 영역이라고 생각합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dbt라는 기술이 최근 데이터 엔지니어링 트렌드가 되어가고 있는 거 같습니다. 하지만 (어떤 기술이 그러하듯,) 이 기술을 실제로 적용하고 규모가 있는 조직에서 사용하기 위해서는 생각보다 정말 많은 고려를 해야 하고 노력이 필요하다는 점을 다시 한번 느끼게 되었습니다. 물론 비용을 지불해서 dbt cloud를 통해서 이런 과정들을 간소화할 수 있지만, 기술들을 도입하기 전에 마일스톤들을 잘 만들어 두는 것이 필요하다고 생각합니다.&lt;/p&gt;

&lt;p&gt;특정 기술을 도입한다고 해서 Analytics Engineering이 해결하려고 하는 모던 데이터 인프라의 문제점들을 해소시켜주지는 않는다고 생각합니다. 하지만 이러한 움직임들이 모여서 큰 흐름을 만들어 내리라고 믿습니다.&lt;/p&gt;

&lt;p&gt;이 글을 있게 해준 소다 스토어 파티 플래시, 녹스, 디니, 토마스, 그리고 모든 데이터 비즈니스 본부 분들 감사드립니다.&lt;/p&gt;

&lt;p&gt;긴 글 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>험프리</name></author><category term="data" /><category term="data-engineering" /><category term="analytics-engineering" /><summary type="html"></summary></entry><entry><title type="html">쏘카 예약을 효율적으로 - 수학적 모델링을 활용한 쏘카 예약 테트리스</title><link href="https://tech.socarcorp.kr/data/2022/06/10/reservation-tetris.html" rel="alternate" type="text/html" title="쏘카 예약을 효율적으로 - 수학적 모델링을 활용한 쏘카 예약 테트리스" /><published>2022-06-10T00:00:00+00:00</published><updated>2022-06-10T00:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/06/10/reservation-tetris</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/06/10/reservation-tetris.html">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;안녕하세요, 쏘카 데이터 비즈니스 본부의 캐롯, 카일입니다.&lt;/p&gt;

&lt;p&gt;쏘카에서 차량을 예약하기 위해 쏘카 앱을 탐색하다 보면 하나의 쏘카존에서 아반떼, 레이, 코나 등 다양한 차량들이 눈에 들어옵니다. 하나의 존에 차량이 1대만 있을 수도 있고, 같은 차종의 차량이 여러 대가 있을 수 있습니다. 이때, 하나의 존에 같은 종류의 차량이 여러 개가 있다면, 어떤 차량이 어떤 과정을 거쳐 나에게 배정되는지 궁금하신 적 없으신가요?&lt;/p&gt;

&lt;p&gt;이번 글에서는 그 과정에 있는 &lt;strong&gt;예약 테트리스 프로젝트&lt;/strong&gt;를 다룹니다.&lt;/p&gt;

&lt;p&gt;프로젝트를 시작하게 된 계기부터, 선형 최적화 모델을 실제 서비스에 적용하기 위해 구축한 인프라 구조까지 프로젝트의 전반적인 내용을 자세하게 다루었으니 이 글을 읽고 최적화가 모빌리티 산업에서 어떻게 적용되는지 이해할 수 있는 시간이 되길 바랍니다.&lt;/p&gt;

&lt;p&gt;예상 독자는 다음과 같습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카의 예약 과정의 뒷부분이 궁금하신 분&lt;/li&gt;
  &lt;li&gt;모빌리티 산업에서 최적화(Optimization) 프로젝트가 어떻게 동작하는지 궁금하신 분&lt;/li&gt;
  &lt;li&gt;데이터 기반의 오퍼레이션을 만드는 과정이 궁금하신 분&lt;/li&gt;
  &lt;li&gt;프로젝트 과정에서 어떻게 아키텍처를 설계했는지 궁금하신 분&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1-예약-테트리스-프로젝트-소개&quot;&gt;예약 테트리스 프로젝트 소개&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-예약-테트리스-최적화-모델링&quot;&gt;예약 테트리스 최적화 모델링&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-예약-테트리스-모델-배포&quot;&gt;예약 테트리스 모델 배포&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-예약-테트리스-적용-성과&quot;&gt;예약 테트리스 적용 성과&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-마무리&quot;&gt;마무리&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-예약-테트리스-프로젝트-소개-&quot;&gt;1. 예약 테트리스 프로젝트 소개 &lt;a name=&quot;1-예약-테트리스-프로젝트-소개&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;11-프로젝트-이름의-유래&quot;&gt;1.1 프로젝트 이름의 유래&lt;/h3&gt;

&lt;p&gt;프로젝트를 설명하기에 앞서, ‘예약 테트리스’ 이름의 유래를 공유드리겠습니다. 테트리스는 위에서 내려오는 블록을 빈 공간에 맞춰 차곡차곡 쌓아 꽉 찬 한 줄 한 줄을 만들어 포인트를 얻는 퍼즐 게임입니다.&lt;/p&gt;

&lt;p&gt;이번 프로젝트는 하나의 쏘카존에 있는 여러 차량(같은 차종의 차량)이 있는 상황에서 예약과 차량을 배정(매칭)하는 프로젝트입니다. &lt;strong&gt;들어오는 예약을 차량의 빈 공간에 차곡차곡 정리하는 과정&lt;/strong&gt;이 마치 테트리스 게임과 비슷하다고 생각했기에 예약 테트리스라는 프로젝트 이름을 붙였습니다.&lt;/p&gt;

&lt;p&gt;쏘카에선 예약에 차량을 어떻게 배정하는지가 매우 중요합니다. &lt;strong&gt;예약에 차량을 어떻게 배정하냐에 따라 얼마나 많은 고객이 쏘카를 이용할 수 있는지가 결정되기 때문&lt;/strong&gt;입니다. 차량이 사용되지 않는 시간이 적을수록(=고객의 차량의 점유 시간이 길수록) 운영 효율이 좋아지기 때문에 예약에 차량을 어떻게 배정하는지에 따라 비즈니스에 영향을 줍니다. 즉, 비즈니스 임팩트에 영향을 줄 수 있는 문제입니다.&lt;/p&gt;

&lt;p&gt;한 쏘카존에 같은 종류의 차량이 두 대 있는 경우를 예시로 추가 설명드리겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/example-reserve-fail.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A와 B라는 서로 다른 두 고객이 각각 08시 ~ 12시와 20시 ~ 24시 총 두 건의 예약을 한 상태에서, 0시부터 24시까지 24시간 동안 이용하고 싶어 하는 C라는 고객이 같은 존에서 같은 차량을 대여하고 싶어 하는 상황을 생각해 봅시다.&lt;/p&gt;

&lt;p&gt;이때 만약 A와 B에게 서로 다른 차량이 배정된 상태라면 C는 이용할 수 있는 차량이 없을 것입니다.&lt;/p&gt;

&lt;p&gt;두 차량 모두 C의 예약을 받을 수 있을 만큼의 충분한 여유 공간이 없기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/example-reserve-success.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 만약 A와 B의 예약에 같은 차량을 배정했다면 두 사람의 예약에 배정되지 않은 하나의 차량이 남아 C는 수월하게 쏘카를 이용할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;이처럼 예약의 상황마다 더 적절한 차량을 배정해 주면 더 많은 차량을 준비하지 않아도 더 많은 고객들이 쏘카의 서비스를 이용할 수 있도록 할 수 있고 운영 효율이 좋아지게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-예약-테트리스의-목적&quot;&gt;1.2 예약 테트리스의 목적&lt;/h3&gt;

&lt;p&gt;예약 테트리스 프로젝트가 도입되기 이전에도 예약을 차량에 배정하는 효율화 작업은 이루어지고 있었습니다. 단, 지금과 다른 점이 있다면 &lt;strong&gt;이 모든 작업이 쏘카 직원분들의 손으로 진행되고 있었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;앞서 설명한 문제 수준의 난이도라면 사람 손으로도 쉽게 처리할 수 있겠지만, 쏘카는 15,000대 이상의 차량과 4,000개의 쏘카존을 보유하고 있습니다. 이런 상황에서 발생하는 예약이 매우 많기 때문에 사람이 직접 진행하기 어렵고, 진행한다고 해도 매우 많은 시간이 소요됩니다.&lt;/p&gt;

&lt;p&gt;만약 같은 종류의 차량이 두 대뿐이라고 해도 쏘카는 최대 90일 전부터 예약을 할 수 있다는 점을 생각하면 사람이 직접 최적의 차량 배정을 하기가 쉽지 않습니다. 즉, &lt;strong&gt;어제 수정한 배정 결과가 내일이 되면 새로운 예약이 추가되어 더 이상 최적이 아닐 수 있습니다.&lt;/strong&gt; 쏘카에서 차량이 가장 많은 제주공항 존의 경우는 예약 테트리스 서비스가 배포되기 전까진 하루에 최대 4시간 정도 시간을 사용하며 고생하고 있었습니다.&lt;/p&gt;

&lt;p&gt;차량 배정 프로세스를 자동화하면 쏘카의 직원분들이 업무 시간을 더 효율적으로 사용할 수 있고, 사람의 머리로 도출하기 어려운 조합을 자동으로 산출해 최적화된 차량 배정으로 서비스를 운영할 수 있게 됩니다. &lt;strong&gt;즉, 예약 테트리스 프로젝트는 업무 효율과 차량 운영 효율 두 마리 토끼를 모두 잡기 위한 프로젝트입니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;해결하려는 목적을 정리하면 다음과 같습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카의 운영 효율 개선&lt;/li&gt;
  &lt;li&gt;차량 배정 최적화 과정을 자동화해 내부 운영 리소스 효율화(업무 효율 개선)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-예약-테트리스-최적화-모델링-&quot;&gt;2. 예약 테트리스 최적화 모델링 &lt;a name=&quot;2-예약-테트리스-최적화-모델링&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-최적화-문제optimization-problem의-접근법&quot;&gt;2.1 최적화 문제(Optimization Problem)의 접근법&lt;/h3&gt;

&lt;p&gt;최적화 문제(Optimization Problem)는 어떤 목적 함수(Objective Function)의 함숫값을 최대화 또는 최소화하는 변수의 해 값을 찾는 유형의 문제를 뜻합니다. 최적화 문제를 푸는 방법은 다양하게 있는데, 다음과 같이 정리할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/optimization-methods.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;최적화 문제를 푸는 방법 중 첫 번째는 알고리즘을 사용하는 것이고, 두 번째는 해 찾기 도구인 솔버(Solver)를 활용하는 것입니다.&lt;/p&gt;

&lt;p&gt;알고리즘으로 최적화 문제를 접근하는 방법도 두 가지로 나눌 수 있는데 먼저 &lt;strong&gt;해당 문제에 특정된 알고리즘을 사용 또는 개발&lt;/strong&gt;하는 방법이 있습니다. 특정 문제를 풀기 위한 알고리즘의 대표적 예시로는 최단거리 경로를 구하는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%81%AC%EC%8A%A4%ED%8A%B8%EB%9D%BC_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98&quot;&gt;다익스트라 알고리즘 (Dijkstra’s Algorithm)&lt;/a&gt; 을 들 수 있습니다. 문제에 딱 맞는 알고리즘을 개발하면 알고리즘으로 도출된 해가 최적임을 보장할 수 있다는 큰 장점이 존재합니다. 하지만 그런 알고리즘을 개발하는 데까지 시간과 노력이 매우 많이 필요하기 때문에 단점 또한 명확합니다.&lt;/p&gt;

&lt;p&gt;최적화 문제를 알고리즘으로 접근하는 방식의 두 번째 갈래로는 &lt;strong&gt;휴리스틱 알고리즘&lt;/strong&gt;이 있습니다.&lt;/p&gt;

&lt;p&gt;휴리스틱 알고리즘은 정확한 최적해를 효율적으로 찾을 수 있는 방법이 없을 때 해의 최적성을 희생해 더 빠르고 효율적으로 해를 찾는 알고리즘을 의미하고, 유전 알고리즘(Genetic Algorithm), 개미 집단 알고리즘(Ant Colony Optimization Algorithm) 등이 잘 알려진 예시입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;문제 유형과 관계없이 쉽고 빠르게 근사 최적해를 구할 수 있다는 이점이 있지만 휴리스틱적 접근은 계산보다는 탐색에 더 가깝기 때문에 찾은 해가 최적임을 보장할 수 없다는 단점&lt;/strong&gt;이 있습니다.&lt;/p&gt;

&lt;p&gt;알고리즘을 직접 구현하지 않고 &lt;strong&gt;Solver를 활용하여 모델링을 통해 최적화 문제를 푸는 방법&lt;/strong&gt;도 존재합니다.&lt;/p&gt;

&lt;p&gt;Solver는 수학적으로 표현된 최적화 문제의 해를 구하는 최적해 탐색 도구로 &lt;a href=&quot;https://www.gurobi.com/&quot;&gt;Gurobi&lt;/a&gt;, &lt;a href=&quot;https://www.ibm.com/kr-ko/analytics/cplex-optimizer&quot;&gt;CPLEX&lt;/a&gt; 등이 상업적 Solver와 &lt;a href=&quot;http://www.pyomo.org/&quot;&gt;Pyomo&lt;/a&gt;, &lt;a href=&quot;https://developers.google.com/optimization&quot;&gt;OR-Tools&lt;/a&gt; 등의 오픈 소스 Solver 등이 있습니다.&lt;/p&gt;

&lt;p&gt;Solver로 최적화 문제를 풀기 위해서는 문제를 수식으로 모델링 해야 하는데 이 과정을 &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_model&quot;&gt;수학적 모델링(Mathematical Programming)&lt;/a&gt;이라고 부릅니다.&lt;/p&gt;

&lt;p&gt;문제의 성격이 선형 또는 비선형인지에 따라 &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_programming&quot;&gt;선형 계획법(Linear Programming)&lt;/a&gt;과 &lt;a href=&quot;https://en.wikipedia.org/wiki/Nonlinear_programming&quot;&gt;비선형 계획법(Nonlinear Programming)&lt;/a&gt;으로 나뉘고, 구하고자 하는 해가 정수로 한정되는 경우는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_programming&quot;&gt;정수 계획법(Integer Programming)&lt;/a&gt;으로 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;수학적 모델링을 통해 수식화된 문제를 풀기 위해 Solver를 사용하면 매우 빠른 속도로 최적해 또는 최적해에 매우 근접한 해를 도출할 수 있다는 매우 큰 장점이 존재합니다. 
하지만 문제가 복잡할수록 시간이나 컴퓨팅 파워 같은 연산을 위한 리소스가 많이 요구되어 비용적인 측면을 고려해야 한다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;예약 테트리스 프로젝트에서는 Solver를 사용하여 최적화 문제를 해결하고 있습니다.&lt;/strong&gt; 직접 서비스에 사용하는 서비스인만큼 최대한 최적에 가까운 해를 도출하는 것이 필요했고, 알고리즘을 직접 개발하는 것 대비 훨씬 짧은 시간 안에 좋은 결과를 도출할 수 있었기 때문입니다. 연산에 필요한 컴퓨팅 파워는 후술할 GCP(Google Cloud Platform)의 힘을 빌려 단점으로 꼽히는 리소스 측면도 충분히 해결할 수 있었기 때문에 최적화 문제의 접근법 중 가장 적합한 방법론이라고 판단했습니다.&lt;/p&gt;

&lt;h3 id=&quot;22-정수-계획법integer-programming&quot;&gt;2.2 정수 계획법(Integer Programming)&lt;/h3&gt;

&lt;p&gt;예약 테트리스 프로젝트는 차량 배정 최적화 모델링을 위해 &lt;strong&gt;정수 계획법&lt;/strong&gt;을 사용합니다.&lt;/p&gt;

&lt;p&gt;구체적인 모델을 다루기 전 먼저 정수 계획법이 무엇인지 짚고 넘어가겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Integer_programming&quot;&gt;정수 계획법(Integer Programming)&lt;/a&gt;은 주로 선형 제약조건으로 표현된 해 공간에서 조합 최적화(Combinatorial Optimization) 문제를 푸는 최적화 기법입니다.&lt;/p&gt;

&lt;p&gt;생소한 단어들이 많아 처음 접하는 개념처럼 느낄 수도 있지만 다음과 유사한 일차 부등식 문제를 풀어본 경험이 있다면 낯설지 않다고 느낄 것입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“0보다 큰 정수 x와 y가 아래와 같은 조건을 만족할 때, k 값을 최대화하는 x, y를 찾으면?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/cp-example.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런 유형의 문제가 바로 정수 계획법을 사용할 수 있는 간단한 예라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;우리에게 익숙한 이 문제를 최적화 모델의 3요소라 할 수 있는 &lt;strong&gt;제약조건, 결정 변수, 목적 함수&lt;/strong&gt;로 설명하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;제약조건(Constraints)
    &lt;ul&gt;
      &lt;li&gt;특정 제약을 거는 조건입니다. 예를 들어 x는 0 보다 큰 값이다, -x와 y를 더하면 2보다 작다입니다.&lt;/li&gt;
      &lt;li&gt;x, y 축과 부등식으로 표현된 두 직선이 x와 y가 가질 수 있는 값을 한정하는 &lt;strong&gt;제약조건&lt;/strong&gt;이 됩니다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결정 변수(Decision Variable)
    &lt;ul&gt;
      &lt;li&gt;우리가 알고자 하는 변수인 x, y는 k 값을 결정하는 &lt;strong&gt;결정 변수&lt;/strong&gt;로 정의할 수 있습니다&lt;/li&gt;
      &lt;li&gt;모든 제약조건을 만족하는 결정 변수 값의 집합을 &lt;strong&gt;해 공간(Solution Space)&lt;/strong&gt;라고 합니다&lt;/li&gt;
      &lt;li&gt;해 공간이 정수로만 이루어져 있기 때문에 정수 계획법을 사용하게 됩니다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;목적 함수(Objective Function)
    &lt;ul&gt;
      &lt;li&gt;최대화하고자 하는 값 k는 이 문제의 목적이 되는 함수로 &lt;strong&gt;목적 함수&lt;/strong&gt;라고 부릅니다&lt;/li&gt;
      &lt;li&gt;목적 함수가 결정 변수에 대해 선형 관계를 가지면 선형 계획법, 제곱 등의 비선형 관계를 가지면 비선형 계획법이 됩니다&lt;/li&gt;
      &lt;li&gt;이 문제는 k가 x와 y에 대해 일차식의 형식을 띄고 있기 때문에 &lt;strong&gt;정수 선형 계획법(Integer Linear Programming)&lt;/strong&gt;의 예시로 볼 수 있습니다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 문제에서 각 요소들을 표시하자면 다음처럼 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/cp-example-explained.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-최적화-지표-정의하기--어떤-상태가-더-최적일까&quot;&gt;2.3 최적화 지표 정의하기 : 어떤 상태가 더 ‘최적’일까?&lt;/h3&gt;

&lt;p&gt;예약 테트리스 모델링에서 가장 어려운 부분은 &lt;strong&gt;어떤 차량 배정 상태가 더 “최적”인지를 수치적으로 판단할 수 있는 지표를 정의&lt;/strong&gt;하는 것이었습니다.&lt;/p&gt;

&lt;p&gt;모델 성능과 최적화 정도 사이에서 고민한 끝에, &lt;strong&gt;가상 예약&lt;/strong&gt;이라는 개념을 도입해 지표를 만들었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;실제 예약: 실제로 고객이 이용 중이거나 이용 예정인 예약&lt;/li&gt;
  &lt;li&gt;가상 예약: 실제 예약이 들어올 수 있는 여유 공간을 나타낸 가상의 예약&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;완벽한 최적해는 아니더라도 가상의 예약 건이 가장 많이 들어올 수 있도록 예약에 차량을 배정하는 모델을 작성하기로 했습니다.&lt;/p&gt;

&lt;p&gt;즉, &lt;strong&gt;가상 예약 건의 예약 가능성이 클수록 더 최적이라고 판단&lt;/strong&gt;한 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/example-possibility-0.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/example-possibility-1.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 들었던 예시로 설명하자면, A와 B의 예약을 서로 다른 차량에 배정한다면 예약 가능한 하루 단위의 가상 예약건수는 0이 되지만, 두 예약을 같은 차량에 배정한다면 1이 되기 때문에 후자의 경우를 더 최적이라고 판단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;예약이 어떤 차량에 배정되었는지가 중요하고, 다른 예약과의 선후 관계는 변수 단위에서 고려하지 않아도 되기 때문에 모델의 최적해 도출 속도에서 강점을 가지는 접근법이었습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;24-최적화-모델-정의하기&quot;&gt;2.4 최적화 모델 정의하기&lt;/h3&gt;

&lt;p&gt;예약 테트리스의 최적화 모델은 다음과 같이 정의됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;결정 변수: 실제 또는 가상의 예약에 차량의 배정 여부(0 또는 1)&lt;/li&gt;
  &lt;li&gt;목적 함수: 예약 가능한 가상 예약건수의 최대화&lt;/li&gt;
  &lt;li&gt;제약조건
    &lt;ul&gt;
      &lt;li&gt;모든 예약은 단 하나의 차량에 배정되어야 한다&lt;/li&gt;
      &lt;li&gt;차량이 고정되어야 하는 예약은 배정된 차량이 변경되면 안 된다&lt;/li&gt;
      &lt;li&gt;임의의 서로 다른 두 개의 실제 예약은 같은 차량에 배정된 경우 서로 겹칠 수 없다&lt;/li&gt;
      &lt;li&gt;임의의 실제 예약과 가상 예약은 같은 차량에 배정된 경우 서로 겹칠 수 없다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 수식으로 표현하면 다음과 같이 작성할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/formulation.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;25-최적화-모델-구현하기--google-or-tools&quot;&gt;2.5 최적화 모델 구현하기 : Google OR-Tools&lt;/h3&gt;

&lt;p&gt;수식화한 모델을 코드로 구현하고 최적해를 찾기 위해서는 Solver가 필요합니다.&lt;/p&gt;

&lt;p&gt;예약 테트리스의 최적화 모델은 구글의 오픈소스 Solver인 &lt;a href=&quot;https://developers.google.com/optimization&quot;&gt;Google OR-Tools&lt;/a&gt;의 파이썬 패키지로 구현하였습니다.&lt;/p&gt;

&lt;p&gt;Google OR-Tools가 지원하는 Solver 중 조합 최적화 문제를 위한 &lt;a href=&quot;https://developers.google.com/optimization/cp/cp_solver&quot;&gt;CP-SAT&lt;/a&gt; Solver는 무료로 사용할 수 있는 오픈소스임에도 &lt;a href=&quot;https://www.minizinc.org/challenge2021/results2021.html&quot;&gt;뛰어난 성능&lt;/a&gt;을 가지고 있고, 사용자의 입맛에 맞는 커스텀 설정들을 지원하기 때문에 이번 프로젝트에 적합하다고 판단했습니다.&lt;/p&gt;

&lt;p&gt;제약조건을 수식화하는 것도 직관적이기 때문에 개발 난이도도 높지 않은 편입니다. 최적화 문제를 푸는 분이 계시다면 Google Or-Tools Solver를 추천드립니다.&lt;/p&gt;

&lt;p&gt;앞에서 예시로 가져온 문제를 CP-SAT Solver로 푸는 파이썬 코드를 보면 실제 수식과의 괴리감이 크게 없는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/python-code-example.png&quot; alt=&quot;&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-예약-테트리스-모델-배포-&quot;&gt;3. 예약 테트리스 모델 배포 &lt;a name=&quot;3-예약-테트리스-모델-배포&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;최적화 모델링을 통해 생성된 모델을 실제 서비스에 배포하기 위해선 여러 작업이 필요합니다. 우선 쏘카 서비스 서버에서 최적화 요청을 해야 하고, 최적화 결과를 서비스 서버에 전달해 데이터베이스에 반영해야 합니다.&lt;/p&gt;

&lt;p&gt;이를 위해 쏘카의 서비스 엔지니어링 본부와 협업을 진행했습니다&lt;/p&gt;

&lt;p&gt;이 프로젝트의 요구 조건은 다음과 같습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) Solver를 실행할 서버 환경이 필요하며, 빠르게 결과를 반환해야 합니다&lt;/li&gt;
  &lt;li&gt;2) 쏘카 서비스 서버와 최적화 서버의 데이터 요청 프로토콜이 필요합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;31-최적화-서버--병렬-처리&quot;&gt;3.1 최적화 서버 &amp;amp; 병렬 처리&lt;/h3&gt;
&lt;p&gt;Solver는 실행할 서버 환경을 “최적화 서버”로 지칭하고, 쏘카 서비스의 서버는 “쏘카 서비스 서버”로 지칭하겠습니다.&lt;/p&gt;

&lt;p&gt;Solver는 CPU 연산으로 진행되며, 저희 프로젝트에선 최대 90일 치의 데이터를 가지고 최적화를 수행해야 합니다. 이 과정에서 많은 데이터를 빠르게 처리해야 합니다. 배정 가능한 경우의 수를 따지면 &lt;code class=&quot;highlighter-rouge&quot;&gt;존 x 차종 별 예약건수(최대 90일) x 차량 대수&lt;/code&gt;가 됩니다. 쏘카는 약 4,000개의 존을 가지고 있고 15,000대가 넘는 차량을 보유하고 있습니다. 위 최적화를 실행하기 위해 많은 컴퓨팅 파워가 필요합니다.&lt;/p&gt;

&lt;p&gt;쏘카 서비스 서버에서 최적화 서버에 최적화 요청을 하고, 최적화 연산이 실행되는 동안 새로운 예약이 들어온다면 그 최적화는 다시 실행해야 합니다. 따라서 저희는 더 빠르게 최적화할 수 있는 방법을 고민했습니다.&lt;/p&gt;

&lt;p&gt;Solver 자체의 성능 개선을 하는 방법과 최적화를 병렬로 실행하는 작업을 모두 할 수 있다고 판단했고, Solver 자체의 성능은 최적화에서 연산을 줄이기 위한 트릭을 사용해 줄였습니다. 그 후엔 &lt;a href=&quot;https://github.com/ray-project/ray&quot;&gt;Ray&lt;/a&gt;를 사용해 최적화 작업을 병렬로 실행했습니다.&lt;/p&gt;

&lt;p&gt;Ray는 다음과 같이 데코레이터를 사용해 간단하게 병렬 처리를 실행할 수 있는 라이브러리입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ray&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;futures&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위에서 말씀드린 것처럼 연산을 실행하기 위해 많은 컴퓨팅 파워가 필요하기 때문에, CPU 성능이 좋은 인스턴스를 선택해 사용하고 있습니다. 더 자세히 말씀드리면 Google Cloud Platform의 vCPU n2d-highcpu-224 인스턴스를 사용하고 있습니다. CPU 224개를 사용한다고? 생각하실 수 있지만 선점형(Preemtible) 인스턴스를 사용할 경우 US 기준 1시간에 1.69 달러로 저렴합니다. GCP의 선점형 인스턴스는 AWS의 스팟 인스턴스와 유사한 서비스입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/preemtible-price.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단, 선점형 인스턴스는 사용하던 도중 구글 클라우드 측에서 회수할 수 있는 단점을 가지고 있습니다. 이런 경우를 대비해 만약 선점형 인스턴스가 종료된다면 새로운 최적화 서버를 띄우는 방식으로 구현했습니다. 이 과정에서 &lt;a href=&quot;https://cloud.google.com/compute/docs/shutdownscript?hl=ko&quot;&gt;Shutdown Script&lt;/a&gt;를 사용했습니다.&lt;/p&gt;

&lt;p&gt;최적화 요청을 모두 처리하면, 최적화 서버를 자동으로 종료하도록 설정해 낭비될 수 있는 비용을 미리 절감했습니다.&lt;/p&gt;

&lt;h3 id=&quot;32-쏘카-서비스-서버와-최적화-서버의-데이터-프로토콜&quot;&gt;3.2 쏘카 서비스 서버와 최적화 서버의 데이터 프로토콜&lt;/h3&gt;
&lt;p&gt;데이터 프로토콜을 정의하기 전에 &lt;strong&gt;“예약이 하나 생성될 때마다 실시간으로 실행돼야 하는가?”&lt;/strong&gt;에 대해 고민했습니다. 예약이 새로 생성될 때마다 최적화를 실행해서 결과를 반영하는 방식도 존재했고, 많은 예약을 배치로 정리하는 방식도 존재했습니다. 예약이 들어올 때마다 기존 예약을 바꾸는 것을 실험해 본 결과 일희일비하는 모습을 보이기도 했고, 쏘카 매니저분들이 운영하실 때 혼란을 가중할 수 있다 판단했습니다.&lt;/p&gt;

&lt;p&gt;실제로 현업에 계신 분에게 여쭤보니, 예약이 들어올 때마다 할 필요는 없고 성수기 기준 1시간에 1번씩 실행하면 괜찮다고 이야기 들었습니다. &lt;strong&gt;한번 최적화를 잘 해두면, 1시간 후엔 바꿀 부분이 많이 없기 때문에 매번 할 필요가 없다는 현업의 이야기를 들었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위 내용을 토대로 예약을 일정 간격으로 정리하는 방식(Batch)으로 결정했습니다. 단, 쏘카 서비스 서버에서 최적화 요청 빈도는 추후에 변경될 수 있기에 이런 변경 가능성도 고려했습니다.&lt;/p&gt;

&lt;p&gt;그와 동시에 데이터를 어떻게 주고받을지에 대한 논의도 진행했습니다. 최적화를 하기 위해 데이터가 존재해야 하는데, 데이터가 저장되는 데이터 웨어하우스인 BigQuery는 실시간으로 데이터가 저장되지 않고, ETL 파이프라인을 통해 1시간에 1번씩 저장됩니다. 추후에 최적화 요청 빈도가 변할 수 있기에 데이터 웨어하우스의 데이터를 사용하지 않고 별도의 메시지(Message)로 데이터를 주고받기로 했습니다.&lt;/p&gt;

&lt;p&gt;메시지 프로토콜은 서버와 데이터 조직이 나뉘는 쏘카에서 적합했으며, 서버에선 메시지를 Push 하고, 최적화 서버에선 메시지를 받아와(Pull) 연산을 진행한 후 다시 서버 쪽에게 최적화 결과를 반환하는 구조로 진행됩니다.&lt;/p&gt;

&lt;p&gt;API 형태로 진행하는 방법도 고민했으나, 최적화 작업의 특성상 연산 시간이 오래 소요될 수 있기 때문에 메시지 형태로 작업하기로 협의했습니다.&lt;/p&gt;

&lt;p&gt;메시지 시스템은 대표적으로 Apache Kafka, AWS SQS, GCP Pub/Sub 등을 활용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 프로젝트를 위해 Kafka를 띄우기엔 관리의 리소스가 더 크다고 판단했고 클라우드의 매니지드 시스템인 AWS SQS와 GCP Pub/Sub을 고려했습니다.&lt;/p&gt;

&lt;p&gt;AWS SQS와 GCP Pub/Sub의 큰 차이는 메시지를 저장하는 기간(리텐션)의 차이와 메시지의 Payload 사이즈 등이 있습니다. AWS SQS는 최대 Payload Size가 256KB, GCP Pub/Sub은 최대 10MB입니다. 저희는 90일간의 예약 내역과 차종의 데이터를 담아 메시지를 보내기 때문에 256KB보단 10MB가 안정적이라 판단했고, 따라서 GCP Pub/Sub을 사용하기로 결정했습니다&lt;/p&gt;

&lt;p&gt;혹시 메시지 시스템에서 생소하신 분들을 위해 간단하게 아시면 좋은 내용을 정리하면 다음과 같습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pub/Sub 구조
    &lt;ul&gt;
      &lt;li&gt;서비스 간의 비동기 통신을 위해 사용되는 통신 모델입니다. Pub/Sub 구조에서 사용되는 몇 가지 중요한 개념을 짚고 넘어가면 다음과 같습니다&lt;/li&gt;
      &lt;li&gt;Topic
        &lt;ul&gt;
          &lt;li&gt;Publisher가 발행하는 메시지를 Subscriber에게 전송하는 창구 같은 개념입니다&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Publisher/Producer
        &lt;ul&gt;
          &lt;li&gt;메시지를 생성해 Topic으로 발행하는 서비스입니다&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Subscriber/Consumer
        &lt;ul&gt;
          &lt;li&gt;Topic을 구독해 메시지를 받는 주체입니다&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Push와 Pull
        &lt;ul&gt;
          &lt;li&gt;Subscriber가 메시지를 전달받는 방식의 종류에는 Push와 Pull 두 가지가 있습니다.&lt;/li&gt;
          &lt;li&gt;Push: Pub/Sub 시스템이 Subscriber에서 메시지를 밀어 넣는(Push) 방법&lt;/li&gt;
          &lt;li&gt;Pull: Subscriber가 직접 서비스로부터 메시지를 당겨오는(Pull) 방법&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;메시지를 받은 후, Google Cloud Platform의 Dataflow로 데이터를 처리했습니다. 이 과정에서 Apache Spark를 사용할 수 있었지만, Pub/Sub과 Dataflow의 결합이 편리하므로 Dataflow를 사용했습니다.&lt;/p&gt;

&lt;p&gt;최종 인프라는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/architecture.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카 서비스 서버에서 최적화 요청을 하기 전에, 최적화 서버를 띄우는 메시지를 보냅니다&lt;/li&gt;
  &lt;li&gt;그 과정에서 최적화 서버가 띄워지고, 쏘카 서비스 서버는 Pub/Sub에 최적화가 필요한 예약과 차종 데이터를 보냅니다(Push)&lt;/li&gt;
  &lt;li&gt;최적화 서버에선 메시지를 받고(Pull) Ray로 최적화를 병렬로 실행합니다&lt;/li&gt;
  &lt;li&gt;최적화 결과를 쏘카 서비스 서버에게 보냅니다(Push)&lt;/li&gt;
  &lt;li&gt;쏘카 서비스 서버에선 최적화 결과를 데이터베이스에 반영하고 성공 여부를 다시 최적화 결과 Pub/Sub에 보냅니다(Push)&lt;/li&gt;
  &lt;li&gt;실행된 최적화 서버는 서버의 CPU, Memory 사용량이 특정 조건이 될 경우(예 : 일정 시간 이상 CPU Usage가 5% 이하인 경우) 최적화 서버를 종료합니다&lt;/li&gt;
  &lt;li&gt;위 과정에서 선점형 인스턴스가 구글에게 회수된다면, Shutdown Script가 다시 최적화 서버를 실행시킵니다&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-예약-테트리스-적용-성과-&quot;&gt;4. 예약 테트리스 적용 성과 &lt;a name=&quot;4-예약-테트리스-적용-성과&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;프로젝트 소개 단에서도 언급했다시피, 예약마다 어떤 차량을 배정해 주는지에 따라 같은 대수의 차량에서도 얼마나 많은 고객이 쏘카를 이용할 수 있는지가 달라집니다.&lt;/p&gt;

&lt;p&gt;예약 테트리스 프로젝트로 실제 운영 효율이 얼마나 더 좋아졌는지는 &lt;strong&gt;사용한 차량 대비 차량이 얼마나 점유되었는지&lt;/strong&gt;를 확인해 보면 됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;사용한 차량&lt;/strong&gt; = 전체 운영 중인 차량 중 예약에 사용된 차량의 비율 = &lt;strong&gt;차량 사용 비율&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;차량의 점유 정도&lt;/strong&gt; = 판매 가능한 모든 차량의 시간 중 예약으로 점유된 시간의 비율 = &lt;strong&gt;가동률&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 개념은 카셰어링 비즈니스를 처음 접한다면 다소 생소한 개념으로 느껴질 수 있기 때문에 위에서 들었던 예시로 설명해 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/metric-example-1.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;총 두 대의 차량이 있을 때, 앞선 두 개의 예약에 각각 다른 차량을 배정하여 세 번째 예약이 예약 실패되었다면 차량 사용 비율은 1.0, 가동률은 0.25가 됩니다.&lt;/p&gt;

&lt;p&gt;반면 두 개의 예약에 같은 차량을 배정하여 세 번째 예약이 무사히 예약되었다면 차량 사용 비율은 동일하게 1.0이지만 가동률은 0.75로 더 많은 예약을 수용할 수 있었음을 뜻합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/metric-example-2.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;동일한 예시에 차량을 한 대 추가하여 앞선 두 개의 예약에 각자 다른 차량을 배정해도 세 번째 예약을 받을 수 있었던 경우도 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;세 개의 예약에 각각 다른 차량을 배정한 경우에는 운영 중인 모든 차량을 사용했기 때문에 차량 사용 비율은 1.0이 되고, 가동률은 0.5가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;하지만 같은 차량을 배정할 수 있는 예약을 잘 정리한다면 3대의 차량 중 2대만을 사용해도 충분하기 때문에 가동률은 0.5로 동일하지만 차량 사용 비율은 0.67로 더 낮게 운영할 수 있습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이렇게 차량 사용 비율과 가동률을 비교하면 예약 테트리스 프로젝트를 통해 같은 대수의 차량으로 더 많은 예약을 수용하게 되었는지, 또는 같은 예약량이 더 적은 차량 대수로도 수용할 수 있게 되었는지 판단할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 실제 일별 차량 사용 비율 대비 가동률을 적용 전후로 확인해 보면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/result-graph-2.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래프에서 볼 수 있듯이 적용 후의 추세선이 전과 비교해 더 높은 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이는 동일한 차량 사용 비율 관점에서 보면 같은 대수의 차량을 공급하여 더 많은 예약을 수용할 수 있다는 것을, 동일한 가동률 관점에서는 더 적은 비용으로 동일한 매출을 만들어낼 수 있음을 의미합니다.&lt;/p&gt;

&lt;p&gt;결론적으로 예약 테트리스 프로젝트를 통해서 &lt;strong&gt;평균적인 차량 사용 비율 대비 약 4%가량의 가동률 증분&lt;/strong&gt;을 만들어낼 수 있었습니다.&lt;/p&gt;

&lt;p&gt;또한 차량 사용 비율이 더 낮을수록 두 추세 선의 차이가 더 두드러지는 것을 볼 수 있는데 이는 최적화를 할 수 있는 여지와 관련이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;최적화를 하기 위해서는 최적화를 할 수 있는 여지가 충분히 있어야 합니다. 이미 모든 차량에 예약이 빽빽하게 차있는 경우라면 최적화를 할 수 있는 여유가 부족해 예약 테트리스의 효과가 두드러지기 어렵습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;하지만 &lt;strong&gt;반대로 예약이 많지 않은 경우라면 배정할 수 있는 차량의 경우의 수가 많은 만큼 최적화할 여유 공간이 남아있어 예약 테트리스를 적용했을 때 그 결과가 더 두드러지게 나타나게 됩니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/result-graph-explained.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예약 테트리스 프로젝트는 단순히 매출과 가동 관점에서만 성과가 있던 것은 아니었습니다.&lt;/p&gt;

&lt;p&gt;매일 직접 예약을 정리하던 업무가 모두 자동화되었기 때문에 각 지역사업팀에서는 업무시간을 훨씬 더 효율적으로 쓸 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;특히 팬데믹 이후 여행 수요가 몰려 성수기 수준으로 예약이 밀려들어오는 상황인 제주사업팀에서 예약 테트리스가 큰 도움이 되고 있다는 소식을 들었을 때는 일을 하며 뿌듯함을 느꼈던 순간 중 하나입니다 :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-tetris/reaction.png&quot; alt=&quot;img&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-마무리-&quot;&gt;5. 마무리 &lt;a name=&quot;5-마무리&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;실시간으로 서비스에 적용되는 최적화 프로젝트는 실제 케이스가 많지 않아,  시행착오도 있었지만 함께 작업하며 서포트해 주신 많은 분들 덕분에 성공적으로 프로젝트를 마무리할 수 있었다고 생각합니다. 쏘카 예약 테트리스는 2020년에 완성한 프로젝트로 현재까지 이상 없이 동작하고 있습니다.&lt;/p&gt;

&lt;p&gt;예약 테트리스 프로젝트에서 서버 개발 쪽을 맡아주신 맷과 브루스, 프로젝트가 진행되는 동안 여러 부서를 오가며 디테일하게 챙겨주신 주디, 그리고 적극적으로 피드백 주신 제주사업팀과 기타 지역사업팀에게 감사드리고 싶습니다.&lt;/p&gt;</content><author><name>carrot, kyle</name></author><category term="data" /><category term="data" /><category term="optimization" /><summary type="html"></summary></entry><entry><title type="html">쏘카 PM의 차량 예약 퍼널 단계 개선기(feat. AB TEST)</title><link href="https://tech.socarcorp.kr/product/2022/06/02/reservation-funnel-improvement-with-abtest.html" rel="alternate" type="text/html" title="쏘카 PM의 차량 예약 퍼널 단계 개선기(feat. AB TEST)" /><published>2022-06-02T00:00:00+00:00</published><updated>2022-06-02T00:00:00+00:00</updated><id>https://tech.socarcorp.kr/product/2022/06/02/reservation-funnel-improvement-with-abtest</id><content type="html" xml:base="https://tech.socarcorp.kr/product/2022/06/02/reservation-funnel-improvement-with-abtest.html">&lt;p&gt;안녕하세요, 쏘카에서 PM(Product Manager)으로 일하는 루시아입니다. 이 글은 쏘카 PM이 쏘카에서 어떤 생각을 가지며 일하는지를 알 수 있는 내용을 담은 글입니다. 제품과 관련된 AB TEST 사례를 공유해 PM분들이 데이터 기반 사고 과정을 얻길 희망해 글을 작성했습니다.&lt;/p&gt;

&lt;p&gt;구체적으로 더 말씀드리면 쏘카 부름 서비스의 성장 방법을 찾기 위해 신규 부름 UX의 AB TEST를 진행했던 경험을 작성했습니다. 제가 PM으로서 사용자의 마음을 들여다보고, 문제를 해결 방법을 도출한 과정을 공유합니다.&lt;/p&gt;

&lt;p&gt;글의 목차는 다음과 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#user-recognition&quot;&gt;우리 사용자들은 부름 서비스를 알고 있을까요?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#user-recognition-enhancement&quot;&gt;부름을 어떻게 전달하면 사용자들이 알아볼까요?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#brand-new-d2d&quot;&gt;부름을 새로운 모습으로 소개합니다.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wrap-up&quot;&gt;느낀 점&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다음에 해당하신다면 글을 끝까지 읽어보시는 것을 추천드립니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PM, PO가 일하는 방식이 궁금한 분&lt;/li&gt;
  &lt;li&gt;PM, PO가 문제를 어떻게 해결하는지 궁금하신 분&lt;/li&gt;
  &lt;li&gt;사용자의 생각을 통해 서비스 성장점을 찾는 방법을 고민 중인 분&lt;/li&gt;
  &lt;li&gt;기획자에서 더 나아가 제품의 성장 어젠다를 발굴하는 역할에 흥미가 있는 분&lt;/li&gt;
  &lt;li&gt;AB TEST를 사용해 제품을 개선한 AB TEST 예시가 궁금하신 분&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;쏘카 PM은 앱/웹/인터널 프로덕트 등 회사 내의 여러 제품을 관리하는 역할을 합니다. 저희들은 문제 분석과 어젠다 발굴의 주체일 때도, 제품 기획자일 때도 있습니다. 실제로 경험해 보면 굉장히 동적이고 다면적인 일입니다. 이전에 마리가 작성해 주신 글인 &lt;a href=&quot;https://tech.socarcorp.kr/product/2022/02/23/growing-up-together-with-the-pm-team.html&quot;&gt;쏘카 PM(Product Manager)은 어떻게 성장하나요?&lt;/a&gt;을 읽어보시면, 저희 그룹의 분위기를 더 느끼실 수 있습니다.&lt;/p&gt;

&lt;p&gt;저는 평소에 프로젝트 안에서 매우 다양한 문제와 만나고 있습니다. 그리고 문제의 원인을 찾기 위해 노력합니다. 물론, 문제의 원인을 파악하는 과정이 쉽지는 않습니다. 위로 보고 아래로 보고 다양한 관점을 토대로 파악하려고 합니다. 쏘카 PM들은 사업, 운영, 프로덕트의 동료들과 셜록 홈스 뺨치는 주도면밀한 관찰을 해야 합니다. 이렇게 머리를 쥐어짠 🤯 끝에는, 문제 해결 방법을 도출합니다. 즉 가설을 설정하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_17.jpg&quot; alt=&quot;Frame 17&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가설을 설정하는 과정에서 다음과 같은 고민을 할 수 있습니다&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;선택한 가설과 가설에 기반한 문제 해결 방법이 맞는지를 어떻게 사전에 판단할 수 있을까요?&lt;/li&gt;
  &lt;li&gt;새로운 기능(또는 새로운 UX)을 사용자들은 과연 좋아할까요?&lt;/li&gt;
  &lt;li&gt;새로운 기능을 배포한 후, 지표는 잘 나올까요?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;가설을 도출한 후에는 그 가설이 올바른 문제 해결 방법인지 확신을 얻는 과정이 필요합니다. 이렇게 확신을 얻기 위해 쏘카 PM은 AB TEST를 사용합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AB TEST란?&lt;/p&gt;

  &lt;p&gt;가설이 실제로 들어맞는지를 확인하기 위해 실험군/대조군(A, B 혹은 그 이상)으로 사용자 그룹을 구분해 서로 다른 버전의 기능이나 페이지를 사용자에게 보여주고, 결과를 평가하는 실험입니다. 새로운 기능에 대해 미리 성공 여부를 예측하기 어려운 경우, 사용자 전체에 배포하기 전에 적은 규모의 그룹에게서 미리 반응을 확인할 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1--우리-사용자들은-부름-서비스를-알고-있을까요&quot;&gt;1.  우리 사용자들은 부름 서비스를 알고 있을까요?&lt;a name=&quot;user-recognition&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;11-큰-문제-이해하기&quot;&gt;1.1. 큰 문제 이해하기&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_12.jpg&quot; alt=&quot;Frame 12&quot; /&gt;&lt;em&gt;부름 서비스는 내가 지정한 위치로 차를 부르는 서비스입니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;먼저 쏘카의 부름 서비스란 내가 지정한 위치로 차를 “부르는” 서비스입니다. 사용자는 차를 대여할 위치와, 반납할 위치를 자유롭게 설정할 수 있습니다. (쏘카존에 직접 방문해서 차를 픽업하고 그 자리에 차를 반납하는 ‘쏘카 왕복’과 다릅니다)&lt;/p&gt;

&lt;p&gt;어느 날, 부름 서비스의 BO(Business Owner - 사업기획을 담당하는 분들) 분들이 미팅을 요청합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“올해 부름은 예약량을 늘려야 해요. 지난 1년여간 예약량 지표가 정체되어 있었습니다. 전사적으로 볼 때 올해는 예약량 부스팅 반드시 필요하고요. 그나저나 부름 참 좋은데… 말로 표현을 못 하겠네…”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;“예약량 지표 정체”라는 문제 상황은 명확합니다. 하지만 아직까지 사용자가 서비스를 잘 쓰지 않는 이유를 명확히 잡아내기 어려운 상태입니다. 문제를 조금 더 뾰족하게 만들어보기로 합니다. PM, BO가 머리를 맞대고 고민을 시작했습니다.&lt;/p&gt;

&lt;h3 id=&quot;12-현재-상황-파악하기&quot;&gt;1.2. 현재 상황 파악하기&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;사용자들은 우리 앱을 보며 어떤 생각을 하고 있을까요?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;과연 우리가 생각하는 것처럼 부름 서비스가 좋아서 고개를 끄덕여 주었을까요? 현재 사용자들이 보고 있는 우리는 어떤 모습일까요? 서비스 현황을 확인합니다. 먼저 왕복, 부름 서비스가 각각 무엇인지 확인해 봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/round_trip.png&quot; alt=&quot;왕복&quot; /&gt;&lt;em&gt;왕복 서비스&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;왕복 서비스&lt;/strong&gt;는 정해진 “쏘카존”에 차량들이 주차되어 있고, 사용자가 직접 &lt;strong&gt;쏘카존까지 이동하여&lt;/strong&gt; 예약한 차를 픽업합니다. 쏘카에서 제공 중인 가장 일반적인 차량 대여 서비스입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/d2d.png&quot; alt=&quot;부름&quot; /&gt;&lt;em&gt;부름 서비스&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;부름&lt;/strong&gt; 서비스는 반대로 사용자가 집 앞으로 차를 부르고, 차량이 지정한 &lt;strong&gt;장소로 호출됩니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;앱의 UX&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;그런데 현재 부름은 왕복과 Flow, UX를 거의 똑같이 공유하고 있습니다.&lt;/strong&gt; 왕복이 아닌 부름 상품을 원하는 사용자에게 이것이 최고의 경험일까요? 여기서부터 뒤집어서 생각해 보기로 했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_11.jpg&quot; alt=&quot;Frame 11&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;13-해결할-문제-뾰족하게-짚기&quot;&gt;1.3. 해결할 문제 뾰족하게 짚기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PM : ”이 문제를 해결하는 우리들은, 쏘카 전체가 아닌, 1인칭 부름 사용자 중심으로 생각하도록 시선을 바꾸어야 해요! 부름 사용자들이 특히 어려워하는 것을 찾아 해결해 주어야 해요.”&lt;/li&gt;
  &lt;li&gt;BO : ”왜요? 무슨 차이가 있어서요?”&lt;/li&gt;
  &lt;li&gt;PM : 현재 “여기로 쏘카 부르기”라고 적힌 &lt;code class=&quot;highlighter-rouge&quot;&gt;부름 PIN&lt;/code&gt; 을 통해서 뿐 아니라, &lt;code class=&quot;highlighter-rouge&quot;&gt;쏘카존&lt;/code&gt; 버튼을 통해서도 동일한 ‘부름’ 차량을 예약할 수 있는데요, 결제 CTA 버튼이 있는 ‘결제정보 확인’ 페이지까지의 도달 전환율이 다음과 같이 차이를 보였습니다. (비율로 표현하면 쏘카존 20% : 부름 PIN 70% 입니다)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_13.jpg&quot; alt=&quot;Frame 13&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BO : 똑같은 상품을 팔고 있는데도, 출발점에 따라서 결과가 다르다고요?&lt;/li&gt;
  &lt;li&gt;PM : 네. 출발점이 부름 PIN인 경우, “여기로 쏘카 부르기”라고 말해주었기 때문에 사용자도 &lt;strong&gt;[부름 차량을 빌려야지]라고 명확히 결심한 상황&lt;/strong&gt;이라고 생각할 수 있을 것 같아요. 부름에 대한 이해, 결제 결심이 반영된 것 아닐까요?&lt;/li&gt;
  &lt;li&gt;BO : 흥미롭네요. 그러면 부름 사용자만의 특징을 같이 좀 더 살펴봐요.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;14-파고들기&quot;&gt;1.4. 파고들기&lt;/h3&gt;

&lt;h4 id=&quot;141-사용자-segment-확인&quot;&gt;1.4.1. 사용자 Segment 확인&lt;/h4&gt;

&lt;p&gt;차량 예약 데이터를 확인했을 때, 부름 사용자는 왕복과 &lt;strong&gt;나이, 대여 시간에 차이&lt;/strong&gt;를 보인다는 점을 발견했습니다. (아래 데이터는 일정 기간 동안의 평균치입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/segments.jpg&quot; alt=&quot;segments&quot; /&gt;
그리고 실제 부름을 애용하는 사용자들에게 전화 인터뷰(In-depth interview)를 진행했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;차에 실어야 할 짐이 있거나, 승차할 동행이 많은 경우, 야간에 대여를 시작하는 경우 등 특정한 TPO에 차를 부름으로 불러서 이용한다&lt;/strong&gt;는 패턴이 있었습니다.&lt;/li&gt;
  &lt;li&gt;사용자들은 위의 상황에서는 &lt;strong&gt;명확하게 부름을 쓰러 앱에 들어오며, 쏘카존까지 걸어가지 않아도 되는 부름 서비스의 장점을 느꼈다&lt;/strong&gt;는 이야기를 자세히 들을 수 있었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;역시 부름은 ‘사용자’가 달랐다는 점을 확인했습니다.&lt;/strong&gt; 사용자의 목소리를 들어본 뒤, 타깃에 맞춘 제품이 필요하다는 생각을 굳히게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/user-stock.png&quot; alt=&quot;user&quot; /&gt;&lt;em&gt;차에 살어야 할 짐이 많은 경우 등 특정 TPO에 부름 서비스를 이용했습니다. by &lt;a href=&quot;https://unsplash.com/@rocinante_11&quot;&gt;Mick Haupt&lt;/a&gt; on Unsplash&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;142-사용성-테스트&quot;&gt;1.4.2. 사용성 테스트&lt;/h4&gt;

&lt;p&gt;사용자들이 보여주고 있던 예약 데이터의 결과와 더불어 제품에서도 사용자들이 허들로 느끼고 있는 명확한 UX를 짚어내기 위해,  &lt;strong&gt;앱 사용성 테스트를 진행&lt;/strong&gt;했습니다. 디자이너, PM, BO 모두 뛰어들어, 사용자들이 어떻게 기존 앱을 사용하는지, 무슨 생각을 하는지 꼼꼼히 수집했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_15.jpg&quot; alt=&quot;Frame 15&quot; /&gt;&lt;em&gt;앱 사용성 테스트 과정&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;부름 서비스를 한 번도 안 써본 사람들과 어느 정도 익숙한 사람들로 &lt;strong&gt;그룹을 분리해 진행&lt;/strong&gt;했습니다.&lt;/li&gt;
  &lt;li&gt;부름을 한 번도 이용해 보지 않은 사용자들에게서는 &lt;strong&gt;처음에 둘러보기는 했으나 이해하기 어려워 사용하지 않았다&lt;/strong&gt;는 피드백을 다수 들었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;더욱이 사용자들이 제품 기획 의도와 다르게 받아들이고 있는 UI 화면들을 발견했습니다&lt;/strong&gt;.  그동안 부름을 써보지 않은 사용자들이 무엇을 불안해하고 있었는지 구체적으로 근거를 찾아낼 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_14.jpg&quot; alt=&quot;Frame 14&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;사용자의 목소리는 생각의 전환점이었습니다.&lt;/strong&gt; 이제 팀원들은 부름 사용자들은 왕복과는 다른 니즈가 있다는 점에 동의했습니다. 그리고 부름 사용자들만이 느꼈던 아쉬운 점, 불편했던 경험을 귀로 생생하게 들으며 깜짝 놀랐습니다. 사용자의 시각은 예상과 다르며, 책상 앞에 가만히 앉아서는 느낄 수 없다는 것을 새삼 깨달을 수 있었습니다.&lt;/p&gt;

&lt;p&gt;이런 불편에 공감하고 해결해야, 사용자들이 쏘카 서비스에 매력을 느끼고 계속 이용하게 될 것입니다. 후행적인 사업 지표에 골몰하는 것이 아닌,  먼저 사용자들의 생각 속에 깊이 빠져들어 본다면, &lt;strong&gt;자연스럽게 예약량은 늘어나게 될 거라 기대합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-부름을-어떻게-전달하면-사용자들이-알아볼까요&quot;&gt;2. 부름을 어떻게 전달하면 사용자들이 알아볼까요?&lt;a name=&quot;user-recognition-enhancement&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-가설-도출하기&quot;&gt;2.1. 가설 도출하기&lt;/h3&gt;

&lt;h4 id=&quot;211-문제-정리&quot;&gt;2.1.1. 문제 정리&lt;/h4&gt;

&lt;p&gt;수집한 정보를 바탕으로 팀원들은 문제와 해결 방법을 다음과 같이 정리했습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;문제&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;사용자는 앱에서 부름이 정확히 무슨 서비스이고, 무슨 장점이 있어서 예약해야 하는지 알기 힘들다.&lt;/li&gt;
  &lt;li&gt;부름을 한 번도 써보지 않은 상태에서는 차를 내가 지정한 장소로 부를 때 드는 걱정들이 어떻게 해결되는지, 쏘카가 어떤 방지책을 제공하는지 알기 힘들다.&lt;/li&gt;
  &lt;li&gt;한 번도 부름을 써보지 않은 사용자의 고통이 부름 이용 경험자 보다 크게 나타난다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;해결 방법&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;예약량 확대를 위해서는 사용자에게 부름만의 매력을 직관적으로 전달하면서(=PMF 찾기)&lt;/li&gt;
  &lt;li&gt;불안감을 낮출 수 있는 예약 경험을 주어야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;PMF(Product/Market Fit)이란, 타깃을 만족시키는 제품을 가능성 있는 시장에 제공하는 상태입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;212-가설-설정&quot;&gt;2.1.2. 가설 설정&lt;/h4&gt;

&lt;p&gt;다음과 같은 &lt;strong&gt;가설을 설정했습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;가설 : 지금처럼 왕복과 똑같은 Flow가 아닌 &lt;strong&gt;[부름 사용자] 관점에서 설계한 부름 전용 예약 Flow를 제공한다면,&lt;/strong&gt; 결제 전환율의 차이와 기분 좋은 예약 경험 두 가지를 확인할 수 있을 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;차량을 예약할 때 내 바로 앞으로 차를 부르고 싶은 사용자 마음의 흐름에 맞추어 독립적인 부름 서비스 전용 &lt;strong&gt;‘신규 예약 페이지’를 새로 구현&lt;/strong&gt;해보자는 아이디어입니다.&lt;/p&gt;

&lt;h3 id=&quot;22-프로덕트-제작-실험-출시하기&quot;&gt;2.2. 프로덕트 제작, 실험 출시하기&lt;/h3&gt;

&lt;h4 id=&quot;221-부름-신규-예약-페이지-프로토타입-제작&quot;&gt;2.2.1. 부름 ‘신규 예약 페이지’ 프로토타입 제작&lt;/h4&gt;

&lt;p&gt;가설에서 제안한 &lt;strong&gt;[부름 사용자] 관점에서 설계한 부름 전용 ‘신규 예약 페이지’&lt;/strong&gt;를 고민하기 시작합니다. 디자이너와 PM이 사용성 테스트에서 수집한 인사이트들을 바탕으로  와이어 프레임을 새롭게 스케치했습니다. 그리고 가벼운 프로토타입으로 만들어 주변 동료들에게 직접 써보게 하고, 이를 통해 우리가 생각한 UX가 기존 사용자의 문제를 해결하는지 확인하며 스케치를 진행했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/prototype-1.png&quot; alt=&quot;prototype-1&quot; /&gt;&lt;em&gt;와이어프레임 스케치&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/prototype-before.jpg&quot; alt=&quot;prototype-before&quot; /&gt; &lt;em&gt;UX 변경 전, 기존 사용자 관점의 문제&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/prototype-after.jpg&quot; alt=&quot;prototype-after&quot; /&gt;&lt;em&gt;프로토타입 구상&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;222-실행-방법-고르기&quot;&gt;2.2.2. 실행 방법 고르기&lt;/h4&gt;

&lt;p&gt;그런데 여기에서 위 도입부에 이야기했던 &lt;strong&gt;[올바른 문제해결 방법인지 확신이 필요한]&lt;/strong&gt; 문제가 대두됩니다. 새로운 예약 Flow를 적용해 보려는 생각을 부름의 사업, 운영팀에 공유했을 때 이런 우려점이 있었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;갑자기 예약 과정을 다 바꿔버려도 될까요?&lt;/li&gt;
  &lt;li&gt;기존 사용자들이 적응을 못하고 이탈하면 어떡하죠? 이 UX를 사용자들이 좋아할 거라 어떻게 확신할 수 있어요?&lt;/li&gt;
  &lt;li&gt;사용자 테스트를 대규모로 하면 시간이 너무 많이 들지 않을까요?&lt;/li&gt;
  &lt;li&gt;혹시나 잘못되면 원래 유지되던 예약률은 방어할 수 있을까요?&lt;/li&gt;
  &lt;li&gt;배포했다가 지표가 잘못되면 롤백 할 수 있을까요?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;223-ab-test-적용&quot;&gt;2.2.3. AB TEST 적용&lt;/h4&gt;

&lt;p&gt;위와 같은 우려를 보완하기 위해, &lt;strong&gt;AB TEST&lt;/strong&gt;를 통해 제품 및 사업 지표가 달성되는지를 확인한 뒤, 전체 배포 여부를 결정하기로 합니다.
PM과 데이터 사이언스팀은 아래와 같이 AB TEST 실험을 설정하고, 사업 팀과 목표 및 관찰 지표를 정했습니다.&lt;/p&gt;

&lt;p&gt;쏘카에는 데이터 기반으로 실험하기 위한 인프라가 마련되어 있어서 수월하게 실험 계획을 짤 수 있었습니다. 데이터 분석 직군이 아니더라도, PM들이 AB TEST 실험을 프로젝트에 사용할 수 있도록 여러 교육과 가이드가 준비돼 있습니다. ❤️&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;실험 기간&lt;/th&gt;
      &lt;th&gt;2022년 1월 24일 ~ 2월 4일 (11일간)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;실험군 정의&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 지도에 들어온 뒤, 부름 PIN을 클릭 시, 신규 부름 예약 페이지를 보여준다. &lt;br /&gt;- &lt;strong&gt;즉, 부름용 ‘신규 예약 페이지’를 통해 예약하는 그룹&lt;/strong&gt; &lt;br /&gt;- 전체 앱 접속자의 5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;대조군 정의&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 지도에 들어온 뒤, 부름 PIN을 클릭 시, 기존 차량 리스트를 보여준다.&lt;br /&gt;- &lt;strong&gt;즉, 기존과 동일한 화면 통해 예약하는 그룹&lt;/strong&gt; &lt;br /&gt;- 전체 앱 접속자의 5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;목표 설정&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 부름용 ‘신규 예약 페이지’를 통해 예약하는 그룹은 전환율이 더 높을 것이다. &lt;br /&gt;- 실험군은 대조군 대비 22% 이상을 목표로 한다. (통계적 유의미성을 띄는 규모 + BO의 매출 기대치 반영)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;관찰 지표 설정&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 예약 페이지 입장 후, [결제 확인] 페이지에 도달한 방문 전환율 &lt;br /&gt;- [결제 확인] 페이지에서 [차량 예약]한 예약 전환율 &lt;br /&gt;- 예약 페이지 입장 후, [차량 예약]한 예약 전환율&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;배포 비율&lt;/strong&gt;은 먼저 1월 24일 전체 사용자의 5%로 시작한 뒤 2월 4일 실험 결과를 통해 1차 결론을 냅니다.&lt;br /&gt;더 많은 모수에서 같은 결과가 나오는지 확신을 갖기 위해 4월 24일까지 10%, 50%로 단계별로 늘려가며 사업지표와 CS 센터에 악영향이 없는지를 모니터링했습니다.&lt;br /&gt;전체 예약 과정을 바꾸었기 때문에 단편적인 UI 성과 측정만이 목표는 아니었고, 사업과 운영에도 영향이 없는지도 중요한 품질 판단 요소였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_21.jpg&quot; alt=&quot;Frame 21&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;224-개발-착수하기&quot;&gt;2.2.4. 개발 착수하기&lt;/h4&gt;

&lt;p&gt;💡 BO가 고민하던 아젠다인 &lt;code class=&quot;highlighter-rouge&quot;&gt;부름의 예약 성장시킬 아이디어 찾기&lt;/code&gt;부터, 드디어 &lt;strong&gt;“부름 전용 신규 예약 페이지를 개발하고, AB TEST로 출시하여 실험군과 대조군 차이 발견하기”&lt;/strong&gt; 라는 &lt;code class=&quot;highlighter-rouge&quot;&gt;프로덕트 제작 목표&lt;/code&gt;까지 도착했습니다. 이에 맞추어 다음 과정이 진행됩니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;디자이너와 UX Writer의 아이디어가 UX 설계에 뿜어져 나옵니다. 🔥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;특히 사용자들이 부름에 대해서 이해가 되지 않았던 부분을 해결하기 위해서, 문장 하나하나에 특히 신경을 많이 써주셨어요.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;iOS, Android, Server 팀, QA 팀은 새로운 프로덕트를 설계하고 개발합니다. 🔥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;부름 예약만을 위한 독립 페이지를 만드는 만큼, 기존 예약 페이지를 활용하면서도 새로운 플로우를 많이 설계해야 해서 꼼꼼하게 봐야 하는 작업이었어요.&lt;/p&gt;

&lt;h3 id=&quot;23-결과를-확인하자&quot;&gt;2.3. 결과를 확인하자&lt;/h3&gt;

&lt;p&gt;치열했던 개발 기간이 흐르고..  드디어 실험이 담긴 버전을 출시!!! 🚀🚀🚀 몇 주간 두근두근 하며 사용자들의 반응을 모니터링 했습니다.&lt;/p&gt;

&lt;h4 id=&quot;231-실험-결과&quot;&gt;2.3.1. 실험 결과&lt;/h4&gt;

&lt;p&gt;(총 기간 : 2022-01-24~ 2022-04-24)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;지표&lt;/td&gt;
      &lt;td&gt;기존 그룹과 신규 그룹의 차이&lt;/td&gt;
      &lt;td&gt;설명&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;예약 페이지 입장 후,&lt;strong&gt;[결제 확인] 페이지에 도달한 방문 전환율&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;-6% p&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 기존과는 달리 첫 페이지에서 부름에 서비스에 대해서 명확하게 설명했기 때문 &lt;br /&gt; - 결제 직전 페이지에는 결국, 진짜 부름에 관심 있는 사용자만 남음.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[결제 확인] 페이지에서 &lt;strong&gt;[차량 예약]한 예약 전환율&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;+21.8% p&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;- 앞에서와 같이, &lt;strong&gt;부름에 대해서 관심 있는 사용자만 남아 결제 가능성은 높아짐&lt;/strong&gt; &lt;br /&gt;- 부름을 원하는 사용자에게 지금보다 더 많이 노출된다면 예약량은 많아질 수 있음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;예약 페이지 입장 후, &lt;strong&gt;[차량 예약]한 예약 전환율&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;없음&lt;/td&gt;
      &lt;td&gt;- 싱품이나 가격 조건이 똑같을 때 둘 사이에 큰 차이는 발견하지 못함 &lt;br /&gt;- 다만 신규 UX를 전체 배포했을 때 사업지표에 리스크는 없다고 판단&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;232-후속-사용성-테스트-결과&quot;&gt;2.3.2. 후속 사용성 테스트 결과&lt;/h4&gt;

&lt;p&gt;더불어, 처음에 문제 확인을 위해 진행했던 기존 UX에 대한 사용성 테스트를 새로운 UX를 가지고 다시 진행해 보았습니다. 새로 개발한 &lt;code class=&quot;highlighter-rouge&quot;&gt;부름홈&lt;/code&gt;을 보여주면서, 기존과 동일한 질문을, 새로운 사람들에게 던져보았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_18.jpg&quot; alt=&quot;Frame 18&quot; /&gt;&lt;/p&gt;

&lt;p&gt;부름 사용자의 가장 큰 페인 포인트였던 “서비스를 이해하기 어렵다”라는 문제가 크게 해결되었다는 걸 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_23.jpg&quot; alt=&quot;Frame 23&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;3-부름을-새로운-모습으로-소개합니다&quot;&gt;3. 부름을 새로운 모습으로 소개합니다.&lt;a name=&quot;brand-new-d2d&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;31-액션-플랜-실행하기&quot;&gt;3.1. 액션 플랜 실행하기&lt;/h3&gt;

&lt;h4 id=&quot;311-배포&quot;&gt;3.1.1. 배포&lt;/h4&gt;

&lt;p&gt;결제 가능성과 UX 이해도를 높인 파워풀한 새로운 [부름 홈]은 더 많은 사용자에게 배포해도 되는 제품으로 결론을 내렸습니다. 50% 배포 후 사업지표 및 CS 지표에 이슈 없는걸 확인한 뒤 4월 25일을 기준으로 100% 사용자에게 모두 배포할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/Frame_16.jpg&quot; alt=&quot;Frame 16&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;312-tpo-재정의&quot;&gt;3.1.2. TPO 재정의&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/reservation-funnel-improvement-with-abtest/tpo.png&quot; alt=&quot;tpo.png&quot; /&gt;&lt;em&gt;부름의 TPO를 반영하는 새로운 메인화면&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;부름 서비스는 사용자들에게 &lt;code class=&quot;highlighter-rouge&quot;&gt;왕복과 다른 TPO로 설명될 수 있다&lt;/code&gt;는 것을 확인한 프로젝트인 만큼 새로 출시될 메인화면에서 “여기로 부르기”라는 버튼으로 부름을 나타내는 입구를 신설하기로 했습니다. 이제 사용자들이 기존 왕복 예약과 부름 서비스를 분리해서 이해할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2022년 5월 업데이트 한 새로운 쏘카 앱 메인 페이지에서, 
새로운 부름 서비스를 더 편리하게 이용해 주세요!&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-느낀-점&quot;&gt;4. 느낀 점&lt;a name=&quot;wrap-up&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;맨 처음 ‘부름 서비스를 성장시키자’는 목표를 들었을 때 들었던 일차원적인 생각은, 여기서 더 어떻게?였습니다. 😂&lt;/p&gt;

&lt;p&gt;약 3년 전 쏘카의 신규 사업에 대한 치열한 고민을 통해 탄생했던 부름 서비스는 론칭 후 운영되며 나름대로 좋은 성적을 유지하고 있었습니다.&lt;/p&gt;

&lt;p&gt;PM으로서 저 자신이 어떤 태도로 임해야 하는지 고민이 됐습니다. 그리고 생각의 함정에 빠질 뻔한 순간도 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“부름처럼 큰 서비스를? 그동안 담당자도 아니었던 내가 어떤 시사점을 더할 수 있을까? 부름의 A to Z를 다 파악하고 나서야 개선점이 눈에 보인다면 공부하는 데 한참 걸릴 텐데.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;만약 제가 주어진 목표를 쉽게 해결하는 방법에 집중했다면, 단순히 ‘안 보이던 안내 문구를 더 잘 보이게’라던가, ‘상품 가격을 내려볼까’ 같은 무책임하고 지속 불가능한 안을 선택했을 것 같습니다. 하지만 이번 프로젝트에서는 &lt;strong&gt;“무엇을 해결할 것인가”&lt;/strong&gt;를 찾는 부분에서 사용자의 도움을 많이 받았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사용자는 무엇이 마음에 안 들어서 부름을 쓰러 들어왔다가 이탈할까?&lt;/li&gt;
  &lt;li&gt;반면 부름을 계속 써오던 사용자들은, 추가 비용을 지불하며 부름을 왜 쓸까?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;두 가지 질문에 대해 &lt;strong&gt;사용자들의 입에서 직접 대답을 들었고&lt;/strong&gt; 또한 사용자의 모습을 관찰하는 과정에서 &lt;strong&gt;뜻밖의 단서&lt;/strong&gt;들을 찾을 수 있었습니다. 운영자 관점에서 단순히 예약이 더 많이 들어왔으면 좋겠다는 생각에 집중했다면 사용자가 무슨 생각을 하는지 파헤쳐 보자는 선택을 하지 않았을 것 같습니다.&lt;/p&gt;

&lt;p&gt;저도 프로젝트의 처음부터 끝까지 ‘사용자’를 중심에 놓고 온전히 집중해 본 경험은 흔치 않았습니다. 돌이켜보니 정말 배운 게 많은, 잘 한 선택이었다 생각합니다. 문제는 현재 진행형으로 해결하고 있지만 성장점을 찾아야 하는 PM에게는 이 방향성을 알게 된 것만으로도 큰 수확이었습니다.&lt;/p&gt;

&lt;p&gt;더욱이 프로덕트 Funnel 데이터 집계 결과와 AB TEST를 통해 얻은 비교 근거들을 가설 검증 과정에서 사용했던 부분 또한 큰 도움이 되었습니다. 저는 데이터 분석 전문가나 지표를 바탕으로 사업전략을 짜는 직무는 아닙니다.  그러나 쏘카에서 그런 데이터에 접근할 수 있고, 쿼리 하여 읽을 수 있고, 관찰한 바를 토대로 문제를 추정할 수 있게 열린 환경이었기에 가능했습니다. (결론적으로 저는 쏘카라는 회사를 좀 더 좋아하게 됐어요. ㅎㅎ)&lt;/p&gt;

&lt;p&gt;제가 준비한 글은 여기까지입니다. 긴 글 읽어주셔서 감사합니다.&lt;/p&gt;</content><author><name>루시아</name></author><category term="product" /><category term="product" /><category term="pm" /><category term="data" /><summary type="html">안녕하세요, 쏘카에서 PM(Product Manager)으로 일하는 루시아입니다. 이 글은 쏘카 PM이 쏘카에서 어떤 생각을 가지며 일하는지를 알 수 있는 내용을 담은 글입니다. 제품과 관련된 AB TEST 사례를 공유해 PM분들이 데이터 기반 사고 과정을 얻길 희망해 글을 작성했습니다.</summary></entry><entry><title type="html">딥러닝 기반 세차 인증 자동화 모델 개발 이야기</title><link href="https://tech.socarcorp.kr/data/2022/04/18/develop-model-classifying-washed-car.html" rel="alternate" type="text/html" title="딥러닝 기반 세차 인증 자동화 모델 개발 이야기" /><published>2022-04-18T00:00:00+00:00</published><updated>2022-04-18T00:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/04/18/develop-model-classifying-washed-car</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/04/18/develop-model-classifying-washed-car.html">&lt;p&gt;안녕하세요 쏘카 데이터비즈니스 본부의 AI팀 에스더라고 합니다! 😎&lt;/p&gt;

&lt;p&gt;저는 AI팀에서 Computer Vision 관련 다양한 Task와 연구를 하고 있습니다. 이번 글에서는 &lt;strong&gt;세차 인증 자동화 모델을 개발한 과정&lt;/strong&gt;을 정리해보려고 합니다. 이 글을 끝까지 다 읽고 나면 현실에서 AI를 적용하여 비즈니스적인 임팩트를 내는 과정을 이해하실 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;p&gt;목차는 다음과 같습니다&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;세차 인증 자동화는 왜 해야 할까&lt;/li&gt;
  &lt;li&gt;세차를 위한 데이터 스킴을 어떻게 정할 수 있을까&lt;/li&gt;
  &lt;li&gt;모델 적용기
    &lt;ul&gt;
      &lt;li&gt;3.1. 분류 모델 실험 과정&lt;/li&gt;
      &lt;li&gt;3.2. Rejection 모델 개발&lt;/li&gt;
      &lt;li&gt;3.3. 실무 적용 위한 External Validation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이 글을 마치며&lt;/li&gt;
  &lt;li&gt;참고&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-세차-인증-자동화는-왜-해야할까&quot;&gt;1. 세차 인증 자동화는 왜 해야할까&lt;/h2&gt;

&lt;p&gt;쏘카는 여러 유저가 쏘카의 차량을 공유하는 서비스입니다. 따라서 공유되는 차량을 주기적으로 세차해 주는 것이 중요한데, 이를 위해 반납 전 세차하는 고객에게 쏘카 크레딧을 제공하고 있습니다.&lt;/p&gt;

&lt;p&gt;쏘카 크레딧이 제공되는 과정은 다음과 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;고객은 세차장을 방문하여 세차를 진행합니다.&lt;/li&gt;
  &lt;li&gt;세차가 완료되면, 세차를 수행했음을 인증할 수 있는 사진을 쏘카 앱에 업로드합니다.&lt;/li&gt;
  &lt;li&gt;쏘카 직원이 업로드된 사진을 검토한 뒤, 크레딧 지급 여부를 결정합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;하지만 서비스가 성장함에 따라 어느덧 쏘카 매니저분들이 한 달에 6,000건 이상 세차 사진을 보아야 하는 상황에 이르렀습니다. &lt;strong&gt;매니저분들의 단순 작업이 가중되었고, 고객에게도 크레딧 지급 여부 결과를 빠르게 받기 어려운 문제&lt;/strong&gt;가 생기게 되었습니다.&lt;/p&gt;

&lt;p&gt;이에 &lt;strong&gt;쏘카 매니저분들을 대신하여 세차 인증을 해줄 딥러닝 모델&lt;/strong&gt;을 만들게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/0.png&quot; alt=&quot;0&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-세차를-위한-데이터-스킴을-어떻게-정할-수-있을까&quot;&gt;2. 세차를 위한 데이터 스킴을 어떻게 정할 수 있을까&lt;/h2&gt;

&lt;p&gt;딥러닝 관련 문제들을 풀다 보면, 중요한 것은 어떤 모델이나 프레임워크를 사용하는 것이 아니라 좋은 데이터를 사용하는 것임을 알 수 있습니다. 하지만 현실에서 수집되는 데이터는 우리가 생각하는 이상향과는 많이 다른 경우가 많습니다.&lt;/p&gt;

&lt;p&gt;같은 뷰로 찍은 사진이라도 접사가 있을 수 있고, 너무 멀리서 찍어서 어떤 것을 말하고자 하는 것인지 모를 때도 있습니다. 혹은 지금 풀고자 하는 문제와 완전히 Distribution이 다른 데이터가 들어올 수도 있습니다. 이러다 보니, 비즈니스 문제 상황을 잘 표현하며 모델이 잘 학습할 수 있는 데이터 스킴을 만드는 작업이 매우 중요합니다.&lt;/p&gt;

&lt;p&gt;데이터 스킴에는 &lt;strong&gt;각 Class마다 같은 Characteristic을 가진 데이터&lt;/strong&gt;만이 속해있어야 합니다. 레이블러의 주관에 따라 달라질 수 있는 애매한 데이터들은 오히려 성능을 더 낮출 수도 있습니다. 따라서 전체 데이터 중에서 상대적으로 명확한 기준을 잡는 것은 매우 중요하지만 동시에 어려운 일이기도 합니다.&lt;/p&gt;

&lt;p&gt;저희 데이터 모델링 팀에서는 2020-2021, 약 2년간 적재된 고객 수행 세차 건 이미지 데이터를 분석하여 세차 여부를 검토하는 4개의 패턴을 발견할 수 있었습니다. 이에 따라 &lt;strong&gt;세차 여부를 분류해 주는 모델을 만들고, 분류가 애매한 이미지들을 매니저의 검토가 필요하도록 별도로 Isolate(격리)해주는 모델을 설계했습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;분류 레이블은 크게 “인증 성공”과 “인증 보류” 2개로 나뉘고 구체적으로는 아래처럼 4개로 나뉩니다. 인증 성공과 보류로 거품이 묻어있는 외관이나 기계 세차장 내부에서 찍은 사진은 세차 인증으로 보고, 차량 외관만 찍은 사진이나 일반 차량 내부 사진들은 세차 인증 보류 클래스로 분류했습니다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(1) 인증 성공 : 외부에서 촬영한 거품 묻은 차량 이미지
(2) 인증 성공 : 내부에서 촬영한 기계 세차중인 이미지
(3) 인증 보류 : 세차 완료된 깨끗한 차량 이미지
(4) 인증 보류 : 일반 차량 내부 이미지
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래처럼 명확한 분류 기준이 없는, 즉 사람이 검토해 줘야 할 애매한 경우에는 Rejection하도록 모델을 설계했습니다. 여기서 Rejection이란 4개의 분류 레이블 어느 것에도 속하지 않는 애매한 이미지가 들어올 경우, 완전히 다른 클래스인 Rejection 클래스로 분류하는 것을 의미합니다. 저희는 아래와 같이, 우리가 분류하고자 하는 스킴에서 벗어나는 데이터들을 Out-of-distribution 데이터라고 부르겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/2.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;세차를 위해 잡은 전체적인 구조는 아래와 같습니다. 첫 번째로 Input으로 들어오는 데이터에 대해서 4-class Classification을 진행하고, 그 Output 값을 받아 Open-set Recognition을 적용하여 Rejection Class를 생성하는 형태로 보실 수 있습니다. 각 구조에 대한 자세한 설명은 &lt;code class=&quot;highlighter-rouge&quot;&gt;3. 모델 적용기&lt;/code&gt;에서 자세한 다루겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/3.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;3-모델-적용기&quot;&gt;3. 모델 적용기&lt;/h2&gt;

&lt;h3 id=&quot;31-모델-실험-과정&quot;&gt;3.1. 모델 실험 과정&lt;/h3&gt;

&lt;p&gt;세차 인증 모델을 만들기 위해 저희는 2가지 방법으로 접근했습니다. &lt;strong&gt;첫 번째 접근은 기본적인 Supervised Classifier 모델이었고, 두 번째 접근은 Image Retrieval 활용하는 방식이었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Image Retrieval은 간단하게 생각하면, Training set의 이미지 Feature 들을 데이터베이스에 저장해두고, 입력으로 들어오는 이미지와 가장 비슷한 이미지 k개를 뽑아주는 방식의 접근입니다. 이 방식은 Place Recognition이나 Map Matching에서 많이 활용하는 접근 방법입니다. 세차 인증 시에도 결국 비슷한 차량의 부분들을 다른 View로 사진을 찍어 올라오게 되는 형태이므로 비슷한 접근으로 풀 수 있을 것이라고 생각했습니다.&lt;/p&gt;

&lt;p&gt;또 Inference 시에 Data-shift에 좀 더 유리할 것이라고도 생각했습니다. 예를 들어 “세차 기계 안에서 차 내부를 찍은 사진”과 “비가 많이 오는 날 차 내부를 찍은 사진”을 생각해 보면, 후자의 경우 Training Set에 없었다면 기본 분류 모델은 이 사진을 세차 완료로 잘못 분류할 수 있습니다. Image Retrieval로 이 문제를 풀게 되면 Feature Vector가 다르게 생성되기 때문에 다른 클래스로 분류하게 될 수 있을 거라고 생각했습니다.&lt;/p&gt;

&lt;p&gt;저희는 Image Retrieval 모델 중에서 Place Recognition 문제에서 높은 성능을 보였던 NetVLAD 모델을 저희 데이터에 맞게 변형하여 사용했고 Classification 모델로는 Resnet50을 사용했습니다.&lt;/p&gt;

&lt;p&gt;그리고 실험 결과는 다음과 같았습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;NetVLAD&lt;/th&gt;
      &lt;th&gt;NetVLAD (w. self-supervision)&lt;/th&gt;
      &lt;th&gt;Supervised Classifier&lt;/th&gt;
      &lt;th&gt;Supervised Classifier (w. self-supervision)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test set acc&lt;/td&gt;
      &lt;td&gt;0.8694&lt;/td&gt;
      &lt;td&gt;0.8444&lt;/td&gt;
      &lt;td&gt;0.9583&lt;/td&gt;
      &lt;td&gt;0.9555&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;test set precision&lt;/td&gt;
      &lt;td&gt;0.8773&lt;/td&gt;
      &lt;td&gt;0.8604&lt;/td&gt;
      &lt;td&gt;0.9608&lt;/td&gt;
      &lt;td&gt;0.9565&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;test set recall&lt;/td&gt;
      &lt;td&gt;0.8759&lt;/td&gt;
      &lt;td&gt;0.8548&lt;/td&gt;
      &lt;td&gt;0.9576&lt;/td&gt;
      &lt;td&gt;0.9550&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;test set f1-score&lt;/td&gt;
      &lt;td&gt;0.8740&lt;/td&gt;
      &lt;td&gt;0.8556&lt;/td&gt;
      &lt;td&gt;0.9591&lt;/td&gt;
      &lt;td&gt;0.9557&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;이 실험을 통해 저희는 Imagenet Pretrained Supervised Classifier를 Baseline으로 사용하기로 결정했고, 2가지 Takeaway도 같이 정리해 볼 수 있었습니다.&lt;/p&gt;

&lt;h4 id=&quot;1-retrieval-보다-supervised-classifier-사용했을-때가-성능이-더-좋다&quot;&gt;(1) Retrieval 보다 Supervised Classifier 사용했을 때가 성능이 더 좋다&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;대부분 Retrieval로 푸는 문제들을 보면 공통된 픽셀 값을 가지는 곳, 즉 이미지 내에서 같은 객체가 있는 곳을 Landmark로 보고 같은 곳이라고 판단해 이 부분들을 매칭하는 방식을 사용합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/4.png&quot; alt=&quot;4&quot; /&gt;
&lt;em&gt;Patch Net-VLAD에서 비슷한 Object가 있는 부분을 보고, 같은 Place라고 예측하는 이미지&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;하지만 저희가 사용하는 사진의 경우는 같은 Class 내에서 같은 Object를 공유하는 형태가 아닙니다. 아래 사진과 같이 같은 “세차 보류 : 차량 내부” 사진이라도 핸들이 등장하는 경우도 있고 차량 시트 쪽이 등장하는 경우도 있기 때문에 Feature Space 상에서 유사하다고 판별하기 어려웠습니다. 따라서 Motivation은 괜찮았지만 데이터 특성상 좋은 성능을 내기 힘들었습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/5.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2-self-supervised-learning-보다-imagenet-pretrained-weight을-사용했을때-성능이-더-좋다&quot;&gt;(2) Self-Supervised Learning 보다 Imagenet-pretrained Weight을 사용했을때 성능이 더 좋다&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;저희는 Upstream Weight을 설계할 때 Self-Supervised Weight을 사용하는 경우와 Imagenet-pretrained weight을 사용하는 경우의 두 가지 성능도 같이 비교해 보았습니다&lt;/li&gt;
  &lt;li&gt;일반적으로 Domain Specific한 형태의 이미지에서는 Self-Supervision을 사용한 성능이 더 좋아야 했지만, 저희는 그렇지 않았습니다. 그 이유는 Self-Supervised Learning을 학습한 데이터에 있었습니다&lt;/li&gt;
  &lt;li&gt;Self-Supervision Weight의 Training 데이터로 차종 분류 문제를 풀 때 사용했던 이미지를 사용했고, 해당 문제를 풀 때 좋은 성능을 냈던 Weight을 가져와서 사용했습니다. 하지만 세차 인증을 위해서는 “깨끗한 외부 사진” 뿐만 아니라 “거품이 있는 외관”과 같은 Fine-grained 한 특징까지 알아야 하므로 해당 Weight으로는 좋은 성능을 내지 못했습니다.&lt;/li&gt;
  &lt;li&gt;더 좋은 성능을 내고자 했다면, 우리가 지금 풀고자 하는 Fine-grained 한 문제를 푸는 도메인을 잘 이해하고 있는 모델로 Self-Supervised Learning 모델을 학습한 Weight을 사용했었어야 합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;32-rejection-모델-개발&quot;&gt;3.2. Rejection 모델 개발&lt;/h3&gt;

&lt;p&gt;이제 Baseline Classifier를 다 만들었다면, Inference 시에 들어오는 Out-of-distribution 데이터, 즉 저희가 Classification 스킴 이외의 데이터가 들어온다면 이를 &lt;strong&gt;어떤 클래스로도 포함하지 않고 reject 하여 담당 매니저에게 검수를 요청하기 위한 모델을 개발&lt;/strong&gt;해야 합니다.&lt;/p&gt;

&lt;p&gt;예를 들어, 세차 인증을 위해 고객분들이 영수증을 찍어 업로드하시는 경우가 생각보다 많습니다. 이를 모델로 판단하기 위해서는 OCR로 영수증으로부터 텍스트를 읽어내고 다시 판단하는 과정을 거쳐야 하지만, 일단 지금 단계에서는 Out-of-distribution으로 판단하고, Reject 한 이후에 담당 매니저에게 검토를 요청하는 방식을 취하기로 결정했습니다.&lt;/p&gt;

&lt;p&gt;Rejection으로 들어가는 이미지는 다음과 같습니다. 영수증 사진이라 사람이 봐도 (세차 여부를 판단하기) 애매한 사진들 혹은 세차장 내부에서 찍은 사진인 것 같으나 역시나 애매한 사진들에 해당됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/6.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번에도 저희는 두 가지 접근 방법을 사용했는데요, &lt;strong&gt;첫 번째 방법은 모델 Output의 Certainty를 활용하여 Threahold로 Out-of-distribution 데이터를 골라내는 방식이고, 두 번째 방법은 Open-set Recognition 접근 방법을 사용했습니다.&lt;/strong&gt; 우선 첫 번째 방법으로 inference된 영수증 데이터들의 distribution을 확인해 본 그림은 아래와 같습니다. 아주 높은 Certainty로 다른 In-of-distribution으로 예측하고 있으므로 이 데이터를 Threaholding으로 Reject 하기는 매우 어려워 보입니다. 이뿐만 아니라, 다른 애매한 사진들, 즉 사람의 판단이 필요한 데이터들이 들어왔을 때도 Calibration Problem 때문에 높은 Certainty로 다른 클래스로 분류해버리는 문제가 생겼습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/7.png&quot; alt=&quot;7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 &lt;strong&gt;두 번째 방법인 Open-set Recognition으로 넘어가게 되었는데요,&lt;/strong&gt; 이 방법의 접근 방법을 다음처럼 크게 두 가지로 떠올려볼 수 있는데요,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;모델의 Output Logit 값을 이용해서 판단하는 방식&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Generative Model을 활용하여 Reject 될 만한 Out-of-distribution을 일부러 만들어 이를 분류하는 모델을 추가적으로 개발하는 방식&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;저희는 좀 더 직관적이고 컴퓨팅 리소스가 덜 소모되는 &lt;strong&gt;1번 방법&lt;/strong&gt;을 사용하기로 했습니다. 또 SOTA로 분류되는 OpenHybrid, PROPOSER보다 더 간단하고 결과적으로도 더 좋은 “Simple and Effective”한 논문인 &lt;a href=&quot;https://arxiv.org/abs/2110.06207&quot;&gt;open set reocognition : a good closed-set classifier is all you need&lt;/a&gt;를 참고하여 저희 상황에 맞게 변형하여 실험 옵션을 구성해 보고자 했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;첫 번째 실험에서의 Calibration 문제를 해결하고자 Label Smoothing의 Parameter를 조정해가면서 실험했고&lt;/li&gt;
  &lt;li&gt;Softmax 전의 Logit 값을 사용하여 Thresholding 하는 실험을 진행했습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래의 그림을 확인해 보면, 이 실험을 통해 어느 정도 Bill, 즉 Out-of-distribution으로 가장 대표적인 영수증 데이터가 Thresholding으로 94% 정도 필터링 될 수 있음을 확인했습니다. 영수증 이외에 Out-of-distribution으로 분류되는 다른 두 클래스의 경우(=Human-hand-wash, Ego-hand-wash)는 제대로 Thresholding은 되지 않았지만, “사람이 직관적으로 판단하기에” 제대로 된 Class에 분류되는 것을 확인했기에 큰 문제가 되지 않고 Rejection 모델을 만들 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/8.png&quot; alt=&quot;8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래는 저희가 최종적으로 적용한 전체 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/9.png&quot; alt=&quot;9&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;33-실무-적용-위한-external-validation&quot;&gt;3.3. 실무 적용 위한 External Validation&lt;/h3&gt;

&lt;p&gt;모델의 최종 성능은 accuracy 기준 &lt;strong&gt;98%를 달성했습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;strong&gt;실제 운영팀에서 세차 인증을 진행하는 단위로 External Validation을 수행&lt;/strong&gt;해보았습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;한 고객당 평균 3장의 이미지를 업로드 (=하나의 예약 건)&lt;/li&gt;
  &lt;li&gt;예약 건별로 세차 인증일지, 보류 혹은 Rejection 예측&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;각 예약 건별로 한 장이라도 세차 인증 이미지가 들어있다면 해당 건은 세차 인증으로 분류했고, 전부 세차 보류라면 세차 보류로 분류했습니다. 또 Rejection으로 예측되는 이미지와 세차 보류의 이미지가 섞인 예약 건은 사람이 검수해 줄 수 있는 Rejection 클래스로 분류했습니다. 그리고 각 클래스마다의 Precision을 확인해 보았습니다.&lt;/p&gt;

&lt;p&gt;모델이 “인증 성공”으로 분류한 건들의 97%는 실제로 사람도 인증을 해주고 있었습니다. 나머지 3%의 경우, 직접 이미지들을 확인해 보니, 월 3회 이상 세차 인증을 수행하여 쏘카 정책상 크레딧 지급이 불가능한 건들이었습니다. 즉 모델은 정확하게 판단했으나, 운영 정책상 사람이 인증해 주지 않은 부분이었기 때문에, 모델은 충분히 제 역할을 해주고 있다고 생각했습니다.&lt;/p&gt;

&lt;p&gt;또 모델이 “인증 보류”로 분류한 건들의 72%는 실제로 사람도 인증 보류를 해주고 있었습니다. 나머지 28%의 경우는, 아래 사진처럼 운영 정책상으로는 맞지 않지만 인간의 시각으로 봤을 때 사람마다 다를 수 있었습니다. 즉, 매니저 재량에 따라 달라지는 부분이었습니다. 이외에 모델이 판단하기에 애매한 사진들 즉 영수증 사진, 세차장 직원과 함께 찍힌 차량 사진 등은 사람이 검토할 수 있도록 Rejection이 잘 되는 것까지 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/9.png&quot; alt=&quot;10&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-이-글을-마치며&quot;&gt;4. 이 글을 마치며&lt;/h2&gt;

&lt;p&gt;이렇게 회사 구성원분들의 단순 작업을 효율적으로 줄이면서도 고객에게 빠른 세차 피드백을 제공하기 위한 세차 모델의 첫 번째 모델을 마무리하게 되었습니다. 실제로 다양한 데이터를 보면서 Data-centric AI가 정말 현업에서는 중요하게 작용한다는 것을 다시 한번 느끼게 되었고 또 다양한 접근 방식을 통해 성능이 개선되는 과정을 통해 저 또한 많이 성장했던 것 같습니다.&lt;/p&gt;

&lt;p&gt;이제 실제로 도입하여 사용하는 분들의 피드백을 받아볼 예정이고, 이를 통해 모델 개선을 해볼 예정입니다. 이후 모델을 개선하는 과정은 추가 글로 공유드리겠습니다.&lt;/p&gt;

&lt;p&gt;만약 AI팀에 관심이 생기셨다면 &lt;a href=&quot;https://tech.socarcorp.kr/data/2022/04/05/role-of-applied-research-scientist-at-socar.html&quot;&gt;쏘카 AI팀의 Applied Research Scientist는 어떤 일을 하나요?&lt;/a&gt; 글을 참고해주세요 :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/develop-model-classifying-washed-car/11.png&quot; alt=&quot;11&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-참고&quot;&gt;5. 참고&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;NetVLAD : &lt;a href=&quot;https://arxiv.org/abs/1511.07247&quot;&gt;https://arxiv.org/abs/1511.07247&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Patch NetVLAD : &lt;a href=&quot;https://arxiv.org/abs/2103.01486&quot;&gt;https://arxiv.org/abs/2103.01486&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Open-Set Recognition : good closed set is all you need : &lt;a href=&quot;https://arxiv.org/abs/2110.06207&quot;&gt;https://arxiv.org/abs/2110.06207&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>에스더</name></author><category term="data" /><category term="applied-research-scientist" /><category term="deep-learning" /><category term="ai" /><summary type="html">안녕하세요 쏘카 데이터비즈니스 본부의 AI팀 에스더라고 합니다! 😎</summary></entry><entry><title type="html">쏘카 AI팀의 Applied Research Scientist는 어떤 일을 하나요?</title><link href="https://tech.socarcorp.kr/data/2022/04/05/role-of-applied-research-scientist-at-socar.html" rel="alternate" type="text/html" title="쏘카 AI팀의 Applied Research Scientist는 어떤 일을 하나요?" /><published>2022-04-05T00:00:00+00:00</published><updated>2022-04-05T00:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/04/05/role-of-applied-research-scientist-at-socar</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/04/05/role-of-applied-research-scientist-at-socar.html">&lt;p&gt;안녕하세요, 쏘카 AI 팀의 케이피라고 합니다.&lt;/p&gt;

&lt;p&gt;저희 AI 팀은 다양한 데이터를 이용해 카셰어링 서비스의 운영을 효율화하고, 고객에게 더 나은 이동 경험을 제공하는 AI Product를 연구개발하고 있습니다. Computer Vision, Natural Language Processing 등의 도메인에서 현실의 문제를 해결하는 AI를 연구하고, 실질적인 Business Impact를 낼 수 있는 Product로 만들어가는 일을 하고 있습니다.&lt;/p&gt;

&lt;p&gt;최근 여러 기업에서 AI를 도입하면서 AI, Deep Learning, Machine Learning에 관련된 포지션이 많이 늘어나고 있습니다. 그러나 기업마다 정의하는 직무명이 나 요구하는 Responsibility가 달라서 헷갈려 하시는 분들을 종종 질문하는 분들이 많이 계셨습니다.&lt;/p&gt;

&lt;p&gt;이번 글에서는 AI에 관련된 포지션에 대해서 간단하게 정리해보고, 쏘카 AI팀의 Applied Research Scientist가 어떤 일을 하는지 소개드리고자 합니다. 이 글을 다 읽으시면 AI에 관련된 포지션이 요구하는 직무 역량에 대해 어느정도 윤곽을 잡고, 쏘카의 Applied Research Scientist가 어떤 일을 하는지도 이해하실 수 있습니다 :)&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AI에 관련된 직무에는 무엇이 있을까?
    &lt;ul&gt;
      &lt;li&gt;Research Scientist&lt;/li&gt;
      &lt;li&gt;Applied Research Scientist&lt;/li&gt;
      &lt;li&gt;Machine Learning Engineer&lt;/li&gt;
      &lt;li&gt;Data Scientist&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;쏘카 AI팀은 어떤 일을 할까?
    &lt;ul&gt;
      &lt;li&gt;Vision Domain&lt;/li&gt;
      &lt;li&gt;NLP Domain&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;마무리&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;ai에-관련된-직무에는-무엇이-있을까&quot;&gt;AI에 관련된 직무에는 무엇이 있을까?&lt;/h2&gt;

&lt;p&gt;여러 회사의 Job Description들을 살펴보면, AI, Deep Learning, Machine Learning에 관련된 주요 포지션은 아래 4가지로 정리해 볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Research Scientist&lt;/li&gt;
  &lt;li&gt;Applied Research Scientist&lt;/li&gt;
  &lt;li&gt;Machine Learning Engineer&lt;/li&gt;
  &lt;li&gt;Data Scientist&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Research라는 단어가 포함된 포지션을 보면, Research와 Applied Research를 구분해두었습니다. 그뿐만 아니라, Scientist와 Engineer가 구분되어 있는 포지션도 있습니다. 직무 명만 봐서는 각 포지션이 어떤 일을 하는지 직관적으로 이해하기가 어렵습니다. 이번 글에서는 이 4가지 포지션이 주로 하는 일과 갖추어야 할 핵심 역량에 대해 간단하게 정리하고자 합니다. 이 글을 보시는 분들이 AI와 관련된 포지션을 찾는 데 도움이 되기를 바랍니다.&lt;/p&gt;

&lt;p&gt;단, 이 글에서 정리하는 정의가 정답은 아닙니다. 조직마다 추구하는 인재상이 다르고, 시간이 흐르면서 요구하는 역량이 달라질 수 있기 때문입니다. 어느 정도의 윤곽을 잡는 목적으로 참고해 주시면 감사드리겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;research-scientist-어떻게-sota를-뛰어넘을-수-있을까&quot;&gt;Research Scientist: 어떻게 SOTA를 뛰어넘을 수 있을까?&lt;/h3&gt;

&lt;h4 id=&quot;concept&quot;&gt;Concept&lt;/h4&gt;

&lt;p&gt;Research Scientist는 AI에 관련된 원천 기술을 연구하는 포지션입니다. 특정 비즈니스 도메인 (i.e., 자율주행, 카셰어링 등)에만 적용할 수 있는 주제가 아닌, 여러 비즈니스 도메인에 적용될 수 있는 근간이 되는 원천 기술을 연구하는 포지션이라고 생각합니다.&lt;/p&gt;

&lt;p&gt;다양한 도메인에 적용할 수 있는 원천 기술을 연구하기 때문에, Research Scientist들은 여러 도메인의 연구자들이 이해하고 실험 결과를 납득할 수 있는 Public Benchmark Dataset을 이용하는 경우가 많습니다. &lt;strong&gt;Research Scientist는 Public Benchmark Dataset에서 이전 연구가 달성한 최고 성능 (State-of-the-Art; SOTA)를 넘어서는 기법을 연구하고, 이전 SOTA의 한계점을 보완하는 기법을 연구하기도 합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;2010년 즈음부터 지금까지 이미지, 텍스트(자연어), 시계열, 생성(Generation) 등의 다양한 기법들의 SOTA가 개선되면서 AI 연구가 점점 성장하고 있습니다. 아직 해결되지 않은 문제들과 연구 주제들도 무수히 많이 남아있어, 더 많은 연구들이 등장할 것으로 기대하고 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;key-responsibility&quot;&gt;Key Responsibility&lt;/h4&gt;

&lt;p&gt;Research Scientist는 과거에 제안된 연구들을 파악하고 새로운 연구를 수행해야 하기 때문에, 주로 요구되는 Responsibility는 아래와 같습니다. (조직마다 다르고, 시간이 흐름에 따라 변경될 수 있습니다)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존에 제안된 연구들을 이해하고 구현할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;기존 연구의 한계점을 개선할 수 있는 기법에 대한 Ideation 능력&lt;/li&gt;
  &lt;li&gt;독립적으로 연구 목적을 수립하고 실험 계획, 평가, 공유(논문 작성 등)를 수행할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;다른 연구자들과 원활하게 소통하고 협력할 수 있는 능력&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;research-questions&quot;&gt;Research Questions&lt;/h4&gt;

&lt;p&gt;원활한 이해를 돕기 위해 Research Scientist가 고민할 만한 Research Question들을 한 번 정리해 보았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gradient Descent를 기반으로 Learning Objective를 달성하는 것이 아니라, 인간처럼 Reasoning을 하는 AI를 만들 수는 없을까?&lt;/li&gt;
  &lt;li&gt;이미지를 이해하는 여러 Neural Networks Architecture가 있는데, 특정한 패턴에 bias 되지 않고 더 인간처럼 이미지를 이해하는(혹은 인간보다 더 뛰어나게) 구조는 없을까?&lt;/li&gt;
  &lt;li&gt;최근에 제안된 Language Model (BERT, RoBERTa, S-BERT 등)보다 더 인간처럼 (혹은 인간보다 더 뛰어나게) 지식을 이해하는 모델은 없을까?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;applied-research-scientist-우리-비즈니스-도메인의-문제를-어떻게-풀-수-있을까&quot;&gt;Applied Research Scientist: 우리 비즈니스 도메인의 문제를 어떻게 풀 수 있을까?&lt;/h3&gt;

&lt;h4 id=&quot;concept-1&quot;&gt;Concept&lt;/h4&gt;

&lt;p&gt;Applied Research Scientist는 특정 비즈니스 도메인의 문제를 풀 수 있는 AI를 연구하고, 연구된 모델을 배포하는 일을 수행하는 포지션입니다. &lt;strong&gt;Research Scientist가 여러 도메인에 범용적으로 적용될 수 있는 AI를 연구한다면, Applied Research Scientist는 특정 도메인에 최적화된 AI를 연구하고 이를 Production 환경에 맞게 구현하는 역할도 수행합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Applied Research Scientist는 Public Benchmark Dataset을 이용하기도 하지만, 현실에서 발생한 Real-world Dataset을 주로 이용합니다. Real-world Dataset 이란 현실에서 발생하는 데이터 셋으로, 쏘카의 경우 카셰어링 서비스를 운영하면서 발생하는 여러 데이터 (i.e., 차량 이미지, 차량 센서, 채팅 텍스트 등)를 의미합니다. 조직이 직면하고 있는 문제를 해결하기 때문에, Public Benchmark Dataset은 논문에서 제안된 여러 기법의 성능을 확인하는 데 사용하고 주된 문제 해결에는 Real-world Dataset을 사용합니다. 하지만 세상만사 쉬운 일이 없듯이 public benchmark에서는 높은 성능을 달성한 기법(모델)이 Real-world Dataset에서는 잘 동작하지 않는 경우가 많습니다. &lt;strong&gt;Applied Research Scientist는 Public Benchmark와 Real-world의 차이를 고민하면서, SOTA 기법이 우리 도메인에서 왜 안되는지 (혹은 왜 잘 되는지)를 파악하고, 제안된 여러 기법들을 최적화하거나 새로운 기법을 디자인하기도 합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AI 연구에서 더 나아가, Applied Research Scientist는 Software Engineer, Data Engineer와 협업하여 AI 모델을 배포하는 과정에도 참여합니다. 리서치용으로 작성했던 코드를 배포 가능한 형태로 리팩토링하고, 다른 조직과 커뮤니케이션하며 배포에 필요한 요소들을 결정합니다. 그뿐만 아니라, AI 모델의 추론(Inference) 결과를 모니터링하여 Dataset Shift나 Feature Drift, Out-of-Distribution 샘플이 프로덕션에 들어오는지를 파악합니다.&lt;/p&gt;

&lt;h4 id=&quot;key-responsibility-1&quot;&gt;Key Responsibility&lt;/h4&gt;

&lt;p&gt;Applied Research Scientist는 도메인의 문제를 해결할 수 있는 AI를 연구하고, 다른 조직과 협업하여 배포하는 과정까지의 업무를 수행하므로, 주로 요구되는 Key Responsibility는 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;도메인에 대한 이해&lt;/li&gt;
  &lt;li&gt;기존에 제안된 연구들을 이해하고 구현할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;기존에 제안된 연구를 도메인에 최적화하거나 더 나은 방법을 Ideation 할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;독립적으로 연구 목적을 수립하고 실험 계획, 평가, 공유(논문 작성 등)를 수행할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;다른 연구자, 엔지니어, End-User 들과 커뮤니케이션하고 니즈를 파악할 수 있는 능력&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;research-questions-1&quot;&gt;Research Questions&lt;/h4&gt;

&lt;p&gt;원활한 이해를 돕기 위해 Applied Research Scientist가 고민할 만한 Research Question들을 한 번 정리해 보았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 A는 ImageNet, SUN, Place 365에서 높은 성능을 달성했는데, 우리 도메인에서는 성능이 높지 않은데, 그 이유가 뭘지? 우리 데이터와 Public Benchmark에는 어떤 차이가 있어서 그럴까?&lt;/li&gt;
  &lt;li&gt;우리 도메인에서 다루는 데이터는 Public Benchmark들과는 너무 다른데, 우리 도메인에서 잘 동작하는 새로운 Neural Architecture를 디자인해 볼까?&lt;/li&gt;
  &lt;li&gt;모델 B가 배포되었을 때 낮은 Overhead를 달성하려면 코드를 어떻게 리팩토링 해야 할까? 모델에 들어가는 Input은 어떻게 설계하고, Inference 결과는 어떤 테이블에 어떻게 적재하지?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;machine-learning-engineer-ai-모델을-어떻게-효과적으로-구현하고-서비스화-시킬까&quot;&gt;Machine Learning Engineer: AI 모델을 어떻게 효과적으로 구현하고 서비스화 시킬까?&lt;/h3&gt;

&lt;h4 id=&quot;concept-2&quot;&gt;Concept&lt;/h4&gt;

&lt;p&gt;Machine Learning Engineer는 AI 모델의 개발과 서비스에 더 무게를 두고 있는 포지션입니다. Scientist와 커뮤니케이션하며 모델을 구현하고, 효율적으로 모델을 학습시킬 수 있는 환경을 구축하기도 합니다. 모델이 학습이 완료된 이후에는, 효율적으로 Inference를 수행할 수 있는 아키텍처를 구성하거나 모델을 리팩토링합니다.&lt;/p&gt;

&lt;h4 id=&quot;key-responsibility-2&quot;&gt;Key Responsibility&lt;/h4&gt;

&lt;p&gt;전 문단에서 “효율”이라는 단어가 자주 등장했는데, Machine Learning Engineer는 특정 데이터 셋에서 성능을 향상시키기보다는 모델을 개발하고 운영하는 과정을 효율화하는 포지션입니다. AI 모델의 학습과 배포하는 단계에서 Machine Learning Engineer가 하는 일들을 나열해 보면, 아래 4가지 정도를 꼽아볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scientist가 사용하는 데이터 셋을 효율적으로 구축하고 관리하기&lt;/li&gt;
  &lt;li&gt;AI 모델을 리팩토링하여 Computation Overhead 줄이기&lt;/li&gt;
  &lt;li&gt;GPU 클러스터를 더 효과적으로 사용할 수 있도록 최적화하기&lt;/li&gt;
  &lt;li&gt;반복적인 작업을 자동화할 수 있는 ML Pipeline 구성하기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Machine Learning Engineer는 데이터 셋 구성에서부터 모델 구현, 학습 효율화, 그리고 배포에 이르는 AI 모델의 전 과정에 참여해 효율성을 높이고, 더 나아가 AI 조직의 생산성을 높이는 일을 수행합니다. Research Scientist, Applied Research Scientist뿐만 아니라 Software Engineer, Data Engineer와 자주 커뮤니케이션하면서 효율이 낮은 부분을 찾고, 기술적인 문제를 진단하며, 그 문제를 해결할 수 있는 방법을 찾는 것이 Machine Learning Engineer의 주요한 역할입니다.&lt;/p&gt;

&lt;h4 id=&quot;research-questions-2&quot;&gt;Research Questions&lt;/h4&gt;

&lt;p&gt;원활한 이해를 돕기 위해 Machine Learning Engineer가 고민할 질문들을 몇 개 가져왔습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;매 실험에 사용된 데이터 셋과 모델의 아키텍처, Weight 파일들이 관리가 어려운데, 이를 좀 효과적으로 관리할 수 있는 방법이 없을까?&lt;/li&gt;
  &lt;li&gt;Pytorch로 작성된 모델이 비효율적인 것 같아. 프로덕션에 들어가려면 더 Overhead를 낮춰야 할 것 같은데, Tensorflow로 이를 변환해 볼 수 있을까?&lt;/li&gt;
  &lt;li&gt;GPU의 개수는 많은데 그 성능을 100% 사용하지는 못하네. 최대한 효율적으로 GPU 자원을 사용할 수는 없을까?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-scientist-데이터를-기반으로-어떤-action을-할-수-있을까&quot;&gt;Data Scientist: 데이터를 기반으로 어떤 Action을 할 수 있을까?&lt;/h3&gt;

&lt;h4 id=&quot;concept-3&quot;&gt;Concept&lt;/h4&gt;

&lt;p&gt;마지막으로, Data Scientist는 비즈니스 도메인에서 발생한 다양한 데이터를 분석하는 포지션입니다. 비즈니스의 여러 실무자와 커뮤니케이션하며 문제를 해결하며, 그 과정에서 필요한 다양한 업무를 진행합니다. 이때, 가장 중요한 것은 단순 Report가 아닌 Action을 위한 데이터 분석을 수행한다는 점입니다. 목적 없이 데이터를 분석하는 것이 아니라, 분석 결과를 기반으로 실무자가 실제 Action을 수행하기 위한 데이터 분석을 진행합니다.&lt;/p&gt;

&lt;h4 id=&quot;key-responsibility-3&quot;&gt;Key Responsibility&lt;/h4&gt;

&lt;p&gt;Data Scientist는 주로 비즈니스 도메인에서 발생하는 다양한 종류의 데이터를 다룹니다. 쏘카의 Data Scientist 포지션으로 예를 들어보겠습니다. 쏘카의 Data Scientist는 서비스의 앱, 웹 로그, 유저의 서비스 이용 데이터, 차량의 센서 데이터 등 다양한 영역에서 비즈니스 문제 해결을 위한 문제 정의, 가설 설정, 실험 설계와 성과 측정을 진행합니다. 쏘카는 주로 분석하는 데이터를 기준으로 Business Data Scientist와 Product Data Scientist로 나누어 채용하고 있는데, 공통적으로 요구되는 역량은 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;비즈니스 임팩트를 위한 문제 정의, 가설 설정, 실험 설계와 성과 측정 능력&lt;/li&gt;
  &lt;li&gt;데이터 기반 비즈니스 알고리즘 개발 능력&lt;/li&gt;
  &lt;li&gt;효과적인 데이터 분석을 위한 핵심 지표의 도출과 관리, 예측 모델링 능력&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;research-questions-3&quot;&gt;Research Questions&lt;/h4&gt;

&lt;p&gt;원활한 이해를 돕기 위해 Data Scientist가 고민할 질문들을 정리해 보았습니다. 이번에는 원활한 이해를 위해 Business Data Scientist와 Product Data Scientist로 나누어서 가져와 보았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(Business) 이번 주말에 강남역 10번 출구 쏘카 존의 예약 건은 얼마나 될까?&lt;/li&gt;
  &lt;li&gt;(Business) 2022년에 서울시 은평구에 몇 대의 차량을 배차하면 대 당 매출이 얼마나 될 것으로 예측할 수 있을까?&lt;/li&gt;
  &lt;li&gt;(Business) 가장 적은 매출이 나올 지역을 데이터에 기반해 찾아주는 알고리즘을 어떻게 만들 수 있을까?&lt;/li&gt;
  &lt;li&gt;(Product) 쏘카의 Funnel 중 가장 전환율이 낮은 부분은 어디일까? 그 부분을 개선하기 위해서는 어떤 Action을 할 수 있을까? 어떤 실험을 진행하면 이에 대한 결론을 얻을 수 있을까?&lt;/li&gt;
  &lt;li&gt;(Product) 새로운 기능 개발을 시작하려고 하는데, 이 기능 개발이 성공했다고 보려면 어떤 Metric을 결정해야 할까? 그 Metric을 보기 위해 어떤 앱, 웹 데이터를 로깅해야할까 새로운 기능을 AB Test 하려고 할 경우, 어떤 방법으로 설계할 수 있을까?&lt;/li&gt;
  &lt;li&gt;(Product) 새로운 기능이 출시된 이후에 성공적인지 확인하기 위해 대시보드는 어떻게 구성해야 할까?&lt;/li&gt;
  &lt;li&gt;더 자세한 내용은 쏘카의 &lt;a href=&quot;https://www.notion.so/socarcorp/d458b6b77a2243fb873d1ac800c321f7&quot;&gt;채용 노션 페이지&lt;/a&gt;를 참고하시면 좋을 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 한번 강조 드리고 싶은 점은, Research Scientist / Applied Research Scientist / Machine Learning Engineer / Data Scientist로 나누어진 포지션이 절대적인 기준이 아니라는 것입니다. AI에 관련된 일들이 칼로 무 자르듯이 나누기 어렵기 때문에, 조직마다 수행하는 역할이 겹치기도 하고 한 포지션에서 여러 역할을 수행하는 경우도 많습니다. 각 조직이 풀고 있는 문제, 성향, 문화 등 여러 가지 요인이 작용하여 포지션을 구성하기 때문에, JD를 보실 때 참조하는 용도로 이 글을 읽으시는 것을 추천합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;쏘카-ai팀은-어떤-일을-할까&quot;&gt;쏘카 AI팀은 어떤 일을 할까?&lt;/h2&gt;

&lt;p&gt;쏘카의 AI 팀은 Human-Interactive AI System라는 목표 아래, Vision, NLP 기술을 활용해 카셰어링 서비스에서 발생하는 여러 문제를 해결하고 있습니다. “Human-Interactive”라는 말이 다소 생소하실 텐데요, 저희 팀이 정의한 &lt;strong&gt;Human-Interactive AI System 이란 End User(사람)의 피드백을 기반으로 지속적으로 성장하는 시스템으로, 현실에서 Robust 하게 동작하여 비즈니스 임팩트를 내는 AI 시스템을 뜻합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/role-of-applied-research-scientist-at-socar/human_in_the_loop.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://hai.stanford.edu/news/humans-loop-design-interactive-ai-systems&quot;&gt;Human-Interactive AI(Human-In-The-Loop)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;vision-domain&quot;&gt;Vision Domain&lt;/h3&gt;

&lt;p&gt;현실에서 Robust 한 AI 시스템을 디자인하기 위해서는 많은 고민들이 필요합니다. 먼저 Vision 도메인에서 간단한 예를 들어보겠습니다.&lt;/p&gt;

&lt;p&gt;경차와 중형차 이미지를 구분하는 이진 차종 분류 모델 (Binary Classifier)이 존재한다고 가정해 봅시다. 이진 분류기를 만드는 것은 간단합니다. DB에 쿼리를 날려 경차와 중형차 이미지를 수집하고, 레이블링을 수행하고, ResNet과 같은 Neural Network Architecture에 Cross Entropy Loss를 설정하여 분류 모델을 학습시키면 됩니다. 그러나 이 분류기가 현실에 배포되기 위해서는 더 깊고, 다양한 고민이 필요합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/role-of-applied-research-scientist-at-socar/car_images.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;경차(왼쪽)와 SUV(오른쪽) 이미지 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;만약 분류기에 들어가는 Inference 이미지가 경차도, 중형 차도 아닌 경우에는 어떻게 해야 할까요? 자동차 사진이 아니라 음식 사진, 차종을 판단할 수 없는 사진, 심한 blur 등으로 식별할 수 없는 사진이 들어오면 어떻게 처리해야 할까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/role-of-applied-research-scientist-at-socar/not_car_images.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;차종 분류와 관련없는 이미지들 예시&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;모델이 풀고자 하는 Task에 관련 없는 이미지 (Out-of-Distribution sample)가 분류기에 들어오면, Supervised Learing 패러다임으로 학습된 분류기는 무조건 경차나 중형차 중에 하나로 판단을 내리게 됩니다. 즉, 불필요하고 잘못된 정보가 Inference 되어 End-User에게 전달됩니다. (고기 사진을 경차라고 판단하여 실무자에게 전달하는 것입니다!) 이러한 문제는 어떻게 해결할 수 있을까요? &lt;strong&gt;사전에 경차나 중형 차에 속하지 않는다고 판단하면서 (Out-of-Distribution Detection), 기존 분류기의 성능을 유지할 수는 없을까요? (Open-Set Recognition)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;다른 관점에서 차종 분류 모델을 살펴보겠습니다. CNN 기반의 분류 모델은 이미지가 들어왔을 때, 해당 이미지에 대한 예측의 Confidence를 갖습니다. 이 Confidence가 특정 이미지가 Class에 속할 Probability(확률)이라고 말할 수 있을까요? 일반적인 CNN들은 대부분의 판단에 대해 overly-confident 한 예측을 수행하는 Overconfidence Problem을 가지고 있습니다. 이때, 과도하게 높은 Confidence를 갖는 문제를 어떻게 해결할 수 있을까요? &lt;strong&gt;잘못된 예측을 수행했을 때는 less-confident 하게 틀리고, 옳은 예측에 대해서는 more confident 하게 맞추도록 할 수는 없을까요? (Calibration) 실무에서는 모델의 예측 결과뿐만 아니라, 모델이 확실하게 예측한 건들을 먼저 검토하고자 하는데, 이 확신의 정도를 어떻게 잘 측정할 수 있을까요?&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;nlp-domain&quot;&gt;NLP Domain&lt;/h3&gt;

&lt;p&gt;AI 팀에서는 고객이 쏘카 이용 중 겪을 수 있는 문제를 빠르게 해결해 줄 수 있는 채팅 AI(Dialogue Sytem)를 연구하고 있습니다. 예를 들어, 고객이 겪는 문제 상황을 이해하기 위해 간단한 Intent Classifier가 있다고 가정해 보겠습니다. 고객의 여러 문제상황(Intent)들 중 “쏘카 존에 반납이 불가능해요”라는 Intent가 있을 때, 아래와 같은 고객의 문의들은 어떻게 처리할 수 있을까요?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1) “쏘카 존 입구가 어디에 있는지 몰라서 반납을 못하겠어요”&lt;/li&gt;
  &lt;li&gt;2) “쏘카 존이 침수되어 진입이 불가능해서 반납을 못할 것 같아요”&lt;/li&gt;
  &lt;li&gt;3) “차단기가 열리지 않아 쏘카 존에 들어갈 수 없어요. 어떻게 할까요?”&lt;/li&gt;
  &lt;li&gt;4) “기름이 떨어져서 차량이 중간에 멈췄어요. 쏘카 존에 반납을 못하겠어요.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 문의들은 공통적으로 “쏘카 존에 반납이 불가능하다”라고 생각할 수 있지만, 각 문의들은 모두 다른 Reply가 필요합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1의 경우 쏘카 존 입구를 안내드려야 하고&lt;/li&gt;
  &lt;li&gt;2의 경우 인근 쏘카 존을 안내드려야 하고&lt;/li&gt;
  &lt;li&gt;3의 경우 쏘카 존 관리자와의 커뮤니케이션이 필요하며&lt;/li&gt;
  &lt;li&gt;4의 경우 긴급출동이 필요합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;고객이 필요로 하는 솔루션이 각기 다른데, 이 문의들을 하나의 Intent로 묶을 수 있을까요? 혹은 한 문장에 여러 가지 문제가 섞여있을 때는 어떻게 처리할 수 있을까요? (Multi-Labeled Sample)&lt;/strong&gt; 뿐만 아니라, &lt;strong&gt;사전에 정의해둔 Intent에서 벗어난 문의는 어떻게 응답해야 할까요? (Unknown Intent Detection)&lt;/strong&gt; Vision 도메인에서와 마찬가지로, 고객의 문의에 대해 &lt;strong&gt;예측한 Intent에 대한 Confidence를 어떻게 측정할 수 있을까요?&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;마무리&quot;&gt;마무리&lt;/h2&gt;

&lt;p&gt;쏘카의 AI 팀은 이러한 고민들을 해결하기 위해 여러 분야의 논문을 스터디하고 구현하며, 실무에 적용하고 있습니다. 과거 논문에서 제안된 기법을 이용하는 것뿐만 아니라, 비즈니스 도메인에서 최적의 성능을 달성하는 새로운 기법을 디자인하기도 합니다. 기술적인 문제를 해결한 후에는 다른 팀과 협업하며 현실에서 Business Impact을 달성합니다. 데이터 사이언스 팀과 협업하여 개발한 AI가 가져올 임팩트를 산정하기도 하고, 엔지니어링 그룹과 협업하여 모델을 배포하고, 모델의 예측 결과를 모니터링합니다. 모델을 실무에 적용하고 프로젝트의 한 Cycle을 완수한 이후, End-User의 피드백을 기반으로 모델을 성장시키고 있습니다 (Human-in-the-Loop).&lt;/p&gt;

&lt;p&gt;앞으로 이어질 테크 블로그 글에서는 쏘카 AI 팀이 비즈니스의 문제를 해결한 Case들을 소개하고, Conference나 Journal에 Publish 한 저희 팀의 연구 실적에 대해서도 소개 드리고 자 합니다.&lt;/p&gt;

&lt;p&gt;다음 글에서 뵙겠습니다. 긴 글 읽어주셔서 감사합니다!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;AI 팀에서는 Human-Interactive AI System을 함께 만들어갈 Applied Research Scientist를 채용하고 있습니다. AI 팀 채용에 관심이 있으시다면, 저희 &lt;a href=&quot;https://www.notion.so/socarcorp/Applied-Research-Scientist-78d277441aba4b8e91b2f053abb8f2c0&quot;&gt;채용 페이지&lt;/a&gt;에 방문하셔서 공고를 확인해 주시면 감사드리겠습니다.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>케이피</name></author><category term="data" /><category term="applied-research-scientist" /><category term="deep-learning" /><category term="ai" /><summary type="html">안녕하세요, 쏘카 AI 팀의 케이피라고 합니다.</summary></entry><entry><title type="html">CES 2022 - 미래 모빌리티를 만나다</title><link href="https://tech.socarcorp.kr/product/2022/03/31/ces2022-review-by-gigi.html" rel="alternate" type="text/html" title="CES 2022 - 미래 모빌리티를 만나다" /><published>2022-03-31T01:00:00+00:00</published><updated>2022-03-31T01:00:00+00:00</updated><id>https://tech.socarcorp.kr/product/2022/03/31/ces2022-review-by-gigi</id><content type="html" xml:base="https://tech.socarcorp.kr/product/2022/03/31/ces2022-review-by-gigi.html">&lt;p&gt;안녕하세요. 저는 쏘카에서 신규 서비스를 기획하는 프로덕트 매니저 지지입니다. 올해 1월, 저는 프로덕트 본부를 대표하여 모빌리티 서비스의 동향을 파악하고자 CES(Consumer Electronics Show) 2022에 다녀왔습니다.&lt;/p&gt;

&lt;p&gt;CES는 익히 알려진 것처럼 세계 최대 IT 제품 박람회입니다. 몇 년 전부터 CES에서 모빌리티 분야는 모빌리티 전시관(Vehicle Technology, Self-Driving Cars)이 따로 운영될 만큼 핵심 분야로 자리매김하고 있습니다. 여러 매체의 기사를 통해서 CES 행사장에 전시된 제품에 대해서는 많이 접해보셨을 텐데요. 저는 CES 행사가 열린 라스베이거스에서 체험해 볼 수 있는 미래 모빌리티 제품 경험을 공유드리려 합니다.&lt;/p&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;리프트(Lyft)의 자율주행 택시
    &lt;ul&gt;
      &lt;li&gt;라스베이거스 특산품, 자율주행 택시&lt;/li&gt;
      &lt;li&gt;자율주행 택시 이용하기&lt;/li&gt;
      &lt;li&gt;비대면 커뮤니케이션은 어디까지 가능할까&lt;/li&gt;
      &lt;li&gt;고객 경험을 특별하게 하기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;베이거스 루프(Vegas Loop) 체험
    &lt;ul&gt;
      &lt;li&gt;이동 수단인가, 놀이 기구인가&lt;/li&gt;
      &lt;li&gt;막힘없는 이동 경험&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;시속 300Km의 자율주행 레이싱
    &lt;ul&gt;
      &lt;li&gt;스포츠도 로봇 간 경쟁 시대&lt;/li&gt;
      &lt;li&gt;사람의 마음이 담긴 레이싱카&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가까이 다가온 미래 모빌리티
    &lt;ul&gt;
      &lt;li&gt;트랙터부터 우주선까지 모빌리티화&lt;/li&gt;
      &lt;li&gt;쏘카가 그리는 미래 모빌리티&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;리프트lyft의-자율주행-택시&quot;&gt;리프트(Lyft)의 자율주행 택시&lt;/h2&gt;

&lt;h3 id=&quot;라스베이거스-특산품-자율주행-택시&quot;&gt;라스베이거스 특산품, 자율주행 택시&lt;/h3&gt;

&lt;p&gt;라스베이거스 시내에서 이동할 일이 있어서 리프트(Lyft) 앱을 실행해 보니 처음 보는 자율주행 표기가 된 리프트 차량이 눈에 띄었습니다. 자율 주행 택시는 간혹 뉴스에서나 보던 터라 정말로 자율주행 차량인가 궁금증이 들었고 일단 타보자 싶어서 차를 호출해 봤습니다. 탑승 이후에 찾아보니 이 택시는 라스베이거스에서는 이미 2021년 11월부터 현대와 앱티브(Aptiv)가 합작한 모셔널(Motional)이 리프트를 통해서 자율주행 택시를 시범 운행 중인 서비스였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/1.jpg&quot; alt=&quot;1&quot; /&gt;
&lt;em&gt;외관상 차량이 특별해 보이지 않지만, 하드웨어 담당 동료로부터 차량에 설치된 라이다나 카메라에 대한 설명을 듣고 나니 좀 더 특별해 보였습니다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;자율주행-택시-이용하기&quot;&gt;자율주행 택시 이용하기&lt;/h3&gt;

&lt;p&gt;리프트(Lyft) 앱에서 차량을 호출할 때에는 먼저 자율주행 차량이라는 점과 두 명의 안전 직원이 상주하고 있다는 내용이 안내되었습니다. 이어서 &lt;strong&gt;별도의 신규 서비스 메뉴로 진입할 필요 없이 기존 리프트 차량을 이용하는 방식과 동일하게 차량을 호출할 수 있었습니다&lt;/strong&gt;. 사용자에게 자연스럽게 시범 서비스를 이용하도록 유도하고 추가로 안내가 필요한 정보만 잘 제공하였다는 느낌을 받았습니다.&lt;/p&gt;

&lt;p&gt;세세한 부분에서는 다른 점도 있었습니다. 기존 리프트 차량은 운행이 종료되면 드라이버에게 팁을 권유하는 안내 문구가 보였는데 자율주행 이용 후에는 별도의 팁 권유가 없는 점은 재밌는 부분이었습니다. 아무래도 무인 차량이다 보니 팁을 줄 필요가 없다는 이치가 자연스럽게 느껴졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/2.png&quot; alt=&quot;2&quot; /&gt;
&lt;em&gt;기존 차량 호출 과정과 크게 다르지 않은 대신, Self-Driving 차량임을 인지하도록 안내합니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/3.jpg&quot; alt=&quot;3&quot; /&gt;
&lt;em&gt;10분 정도 탑승했는데 택시비는 만 오천 원이 나왔네요. 라스베이거스 물가는 상당히 비싼 편이에요.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;비대면-커뮤니케이션은-어디까지-가능할까&quot;&gt;비대면 커뮤니케이션은 어디까지 가능할까&lt;/h3&gt;

&lt;p&gt;차량에 탑승하니 보조석에 앉은 직원분께서 간단히 이용 안내를 해주셨습니다. 실제 자율주행 과정과 유사한 환경이 되도록 &lt;strong&gt;문의 사항이나 궁금한 점이 있을 경우에는 뒷좌석 앞에 부착된 태블릿을 통해서 문의할 수 있다고 합니다.&lt;/strong&gt; 주행 중에 태블릿에서는 제 이름과 간단한 안내 문구, 문의를 할 수 있는 버튼, 그리고 실시간으로 이동 경로가 안내되었습니다.&lt;/p&gt;

&lt;p&gt;그럼에도, 자율주행 택시에서는 오로지 태블릿을 통해 소통해야 하는 점에 대해 의문인 점도 있었습니다. 즉각적으로 하차할 장소를 변경하거나, 또 운행 중 발생할 수 있는 긴급한 연락할 일이 생겼을 때 과연 이 태블릿으로 빠른 소통이 가능할까 하는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;비대면 또는 무인 기반의 자율주행 서비스가 좀 더 보편화된다면 탑승 과정에서 발생하는 &lt;strong&gt;일상적인 문의에 대한 대응을 쉽게 할 수 있는 방식에 대한 고민이 필요해 보였고, 또 불편한 분들을 위해 화장실에 설치된 긴급 호출 버튼처럼 대응할 수 있는 장치가 필요하지 않을까&lt;/strong&gt;라는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/4.png&quot; alt=&quot;4&quot; /&gt;
&lt;em&gt;좌석 앞에 비치된 태블릿은 탑승 전 환영부터 이동 경로 안내, 문의 사항 전달, 탑승 완료를 안내하는 역할을 합니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/5.jpg&quot; alt=&quot;5&quot; /&gt;
&lt;em&gt;때로는 이런 긴급 버튼이 더 안전한 대안이 될 수도 있겠다는 생각을 했습니다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;고객-경험을-특별하게-하기&quot;&gt;고객 경험을 특별하게 하기&lt;/h3&gt;

&lt;p&gt;자율 주행이 탑재된 상용 택시를 타본다는 것이 제게는 새롭고 흥분되는 경험이었는데요. 그럼에도 불구하고, 앱에서 차량을 호출하거나 차량을 탑승할 때를 돌이켜보면 크게 이를 느끼게 한다는 인상은 없었습니다. 그래서 저도 운행이 끝나고 리프트의 블로그를 통해서나마 좀 더 자세한 내용을 알 수 있었는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/6.png&quot; alt=&quot;6&quot; /&gt;
&lt;em&gt;리프트 블로그에 자율주행 택시 도입에 관한 &lt;a href=&quot;https://www.lyft.com/blog/posts/motional-and-lyft-to-launch-fully-driverless-ride-hail-service-in-las-vegas&quot;&gt;글&lt;/a&gt;이 있었습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;저는 좀 더 &lt;strong&gt;탑승자가 자율주행 택시임을 느낄 수 있도록 장치를 만들어뒀으면 훨씬 더 특별한 이용 경험으로 만들 수 있지 않았을까&lt;/strong&gt;란 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;예를 들어 리프트 차량에서는 자율주행 기술이 적용되는 때에는 운전자가 보는 디스플레이에 작은 글씨로 “AUTO”라고 표시된 것으로 알 수 있었는데요. 탑승자도 이를 차량 내에서 식별할 수 있도록 표현해 줄 수 있지 않을까 상상했습니다. 또, 태블릿에 자율주행으로 주행되는 경로를 표현해 줘서 자율주행이 어떤 판단을 하였는지를 알 수 있으면 “자율주행 기술이 어떤 판단을 하여 차선을 바꿨구나”라는 구체적인 생각으로 연결되지 않을까 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;이처럼 새로운 서비스나 기술을 선보일 때에는 &lt;strong&gt;기술적인 완성 이외에도, 사람이 탑승하고 그 사람을 위한 서비스이니 자율주행을 체험하는 사용자를 고려할 필요가 있지 않을까&lt;/strong&gt;라는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;그럼에도, 실제로 신호등에 따라 차량이 서기도 하고, 목적지에 다다르니 인도 옆 차선까지 스스로 이동하는 것을 체험해 보니 자율주행 기술이 코앞까지 왔음을 실감할 수 있었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;베이거스-루프vegas-loop-체험&quot;&gt;베이거스 루프(Vegas Loop) 체험&lt;/h2&gt;

&lt;h3 id=&quot;이동-수단인가-놀이-기구인가&quot;&gt;이동 수단인가, 놀이 기구인가&lt;/h3&gt;

&lt;p&gt;라스베이거스에서만 체험할 수 있는 또 하나의 교통수단으로 라스베이거스 컨벤션 센터(LVCC)의 웨스트홀, 사우스홀, 노스홀 구간을 오가는 베이거스 루프가 있습니다. 베이거스 루프는 일론 머스크의 보링 컴퍼니에서 제작하였는데 라스베이거스 지하에 터널을 뚫어서 라스베이거스 주요 도심의 교통 체증을 해소하는 것을 목표로 현재 컨벤션 센터 주변에서 운행되고 있습니다.&lt;/p&gt;

&lt;p&gt;베이거스 루프를 탑승하기 위해 승차장인 지하로 내려가면 안내하는 직원이 있고, 직원으로부터 탑승할 구역을 안내받아서 대기하였습니다. 곧 테슬라 차량이 탑승 구역에 도착하였고, 차량에 탑승하여 운전자에게 가고자 하는 홀을 이야기하니 차량은 곧 출발하였습니다.&lt;/p&gt;

&lt;p&gt;재밌는 점은 구간이 그리 길지 않아서 차량 탑승 시간은 1, 2분 이내이지만 탑승장 내 울리는 음악과 터널 구간의 조명 때문인지 왠지 테마파크의 놀이 기구를 탑승하는 기분으로 자주 애용하게 되었습니다.&lt;/p&gt;

&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/693440889?h=a156dcdff5&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;PXL_20220105_233317086.mp4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;막힘없는-이동-경험&quot;&gt;막힘없는 이동 경험&lt;/h3&gt;

&lt;p&gt;베이거스 루프를 이용해 보면 탑승장에 들어서서 이동 후 차량에서 내리기까지 막힘이 없다는 인상을 줍니다. 차량을 선택하고 목적지로 호출하는 등 고민할 필요 없이 그저 원하는 장소를 떠올리고 이동해서 기다리면 되었는데요. 짧은 거리이지만 탑승 후 일 방향의 지하 터널로 이동하다 보니 트래픽을 마주할 일도 없고, 어떤 경로로 이동할지 고민할 필요 없이 목적지로 이어지는 구간을 따라가다 보면 목적지에 다다르는 경험은 새로웠습니다.&lt;/p&gt;

&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/693440867?h=e2d46a7392&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;PXL_20220105_233426550.mp4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;베이거스 루프를 직접 체험해 보니 일부 기술의 한계도 보입니다. 베이거스 루프의 구간은 2km 남짓으로 길지 않은 구간이며 법적인 제한으로 인하여 자율주행 운행이 제한되기도 하였습니다. 그래서 사람이 1-2분의 구간을 운전해 주는데 꼭 차로 이러한 터널을 이동해야 하나 싶은 생각도 들었습니다.&lt;/p&gt;

&lt;p&gt;그럼에도, 일론 머스크는 베이거스 루프는 본 사업의 시작에 그치지 않는다고 합니다. 베이거스 루프보다 더 첨단 기술로 구현한 하이퍼 루프를 통해서 라스베이거스 도심을 벗어나 샌프란시스코까지 30분 만에 이동할 수 있는 교통수단을 만드는 것을 장기적인 목표라고 하는데요. 아직은 사람이 자동차를 통해 라스베이거스 일부 지하에서만 이동하는 것에 그치는 수준이지만, 머지않아 하이퍼 루프가 실현된다면 한 번쯤 다시 이곳에 와서 이용해 보고 싶다는 생각이 들었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;시속-300km의-자율주행-레이싱&quot;&gt;시속 300Km의 자율주행 레이싱&lt;/h2&gt;

&lt;h3 id=&quot;스포츠도-로봇-간-경쟁-시대&quot;&gt;스포츠도 로봇 간 경쟁 시대&lt;/h3&gt;

&lt;p&gt;이번 CES2022 야외 행사로 라스베이거스 모터 스피드웨이(Las Vegas Motor Speedway, LVMS) 경기장에서 자율주행 레이싱 경기(Autonomous Challenge)가 열렸습니다. 이번 대회에 전 세계의 8개의 대학팀이 참여하였고 국내에서는 카이스트가 한국 대표로 출전하였습니다.&lt;/p&gt;

&lt;p&gt;자율주행 레이싱은 일반 자율주행 자동차와 몇 가지 차이가 있습니다. 먼저, 최대 300Km의 속도에서도 자율 주행 센서가 안정적으로 동작해야 하며 경쟁 경기에서는 공기저항을 줄이기 위해 앞차의 경로를 따라야 합니다.&lt;/p&gt;

&lt;p&gt;또, 주행 중 차량에 이상이 있을 시에는 벽이나 사물에 충돌하지 않도록 스스로 멈추는 상황도 고려되어야 하는데요. 이처럼 자율주행 레이싱은 가장 극한의 자율주행 기술로 현재 개발 중인 자율주행 기술이 어느 수준까지 다다른 상태인지 체감할 수 있는 스포츠입니다.&lt;/p&gt;

&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/693440806?h=5aeeb94baf&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;PXL_20220107_204436844.mp4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;사람의-마음이-담긴-레이싱카&quot;&gt;사람의 마음이 담긴 레이싱카&lt;/h3&gt;

&lt;p&gt;자율주행 레이싱은 이제 시범 도입되는 단계이기에 우리가 아는 레이싱 경기처럼 박진감 넘치는 경기는 아니었습니다. 두 가지 경기 방식으로 진행이 되었는데 하나는, 차량별로 한 대씩 누가 더 빠른 속도를 내는지를 겨루는 기록 경쟁 방식이 있었고, 또 하나는 두 대가 같이 주행하되 서로 공격 순서를 정해서 뒤 차가 앞차를 추월하는 턴 방식으로 진행되는 등 정해진 규칙 내에서만 이뤄지다 보니 보는 이로 하여금 경기 상황을 체감하기 어려운 점이 있었습니다.&lt;/p&gt;

&lt;p&gt;그럼에도 재밌었던 점은 자율주행으로 경기장을 돌고 무사히 첫출발 선으로 돌아온 차에게 각 팀의 팀원들은 물론이고, 경기를 지켜보는 이들 모두 경기 결과에 관계없이 차량이 사고 없이 &lt;strong&gt;무사히 완주하고 돌아왔을 때 손뼉을 치고 기뻐하는 모습은 영락없이 사람이 직접 참여하는 스포츠의 한 장면이라는 생각이 들었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;한편, 차량에 문제가 생겼을 때 차량이 도로에 정차하여 이를 수리하는 것 역시 사람의 몫이었는데요. 문제 상황을 인지하고 기술적인 문제를 해결해야 하는 정밀한 조치가 필요한 순간에는 아직 한계가 보였습니다. 그럼에도 빠르게 홀로 주행하는 자율주행 차량을 보니 놀랍다는 생각이 들었는데요. 2016년, 이세돌과 알파고와의 대결처럼 빅 테크 기업이 제작한 AI 레이싱카와 드라이버 간 대결을 하는 때도 곧 오지 않을까 상상이 됩니다.&lt;/p&gt;

&lt;div style=&quot;padding:56.25% 0 0 0;position:relative;&quot;&gt;&lt;iframe src=&quot;https://player.vimeo.com/video/693440840?h=63f2778c33&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; fullscreen; picture-in-picture&quot; allowfullscreen=&quot;&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; title=&quot;PXL_20220107_203939894.mp4&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;script src=&quot;https://player.vimeo.com/api/player.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;주행을 마치고 돌아온 차량에 팀원과 관객들 모두 박수를 보냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/7.jpg&quot; alt=&quot;7&quot; /&gt;
&lt;em&gt;경기장에 방문하니 맛있는 음식과 음료가 제공되네요.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;가까이-다가온-미래-모빌리티&quot;&gt;가까이 다가온 미래 모빌리티&lt;/h2&gt;

&lt;h3 id=&quot;트랙터부터-우주선까지-모빌리티화&quot;&gt;트랙터부터 우주선까지 모빌리티화&lt;/h3&gt;

&lt;p&gt;CES 야외 전시장에서는 대형 모빌리티 제품들을 경험해 볼 수 있었습니다. 오로라사(Aurora)는 자율주행 센서를 탑재한 트럭을 전시하였고, 우주항공 스타트업 시에라 스페이스(Sierra Space)는 우주선과 스테이션 모듈을 전시하였습니다. 시에라 스페이스의 전시 제품은 실물은 아닌 모형이었지만 거대한 크기가 인상적이었고, 스테이션은 마치 우리가 업무를 보는 공간인 것처럼 일상화된 공간으로 디자인된 것이 흥미로웠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/8.png&quot; alt=&quot;8&quot; /&gt;
&lt;em&gt;모빌리티 전시장에 트럭부터 우주선까지 볼 수 있어서 마치 놀이동산에 온 듯했습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;농업용 중장비를 제작하는 존 디어(John Deer)도 모빌리티 전시장에 참여하였는데요. 영화 인터스텔라에서나 봤던 무인 자율주행 트랙터를 선보였습니다. 트랙터에 인공지능(AI) 프로세서, 그래픽 처리 장치(GPU), 위성항법 시스템(GPS) 기술을 접목하여 작업자가 설정한 작업 구간을 따라 자율 주행하는 동시에 데이터를 기반으로 작물을 재배할 수 있다고 합니다.&lt;/p&gt;

&lt;p&gt;존 디어는 이러한 기술이 농부의 편의성만 개선하는 것이 아니라, 농작물의 생산성을 높여서 미래에 다가올 식량난까지 극복하는 것이 목표라고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;쏘카가-그리는-미래-모빌리티&quot;&gt;쏘카가 그리는 미래 모빌리티&lt;/h3&gt;

&lt;p&gt;라스베이거스에서 경험한 미래 모빌리티는 사람이나 물건을 수송하는 것에 그치지 않았습니다. 이동 과정의 편의는 물론이며 이동에 가장 적합한 이동 수단을 제공하기도 합니다. 그뿐만 아니라 고도로 발달된 자율주행으로 불의의 사고를 방지하는가 하면, 식량난과 같은 전 지구적인 어려움까지 해결할 수도 있다는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/ces2022-review-by-gigi/9.png&quot; alt=&quot;9&quot; /&gt;
&lt;em&gt;쏘카에서 함께 미래 모빌리티를 그려 나가는 동료들&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;쏘카 또한 스트리밍 모빌리티라는 비전을 갖고 미래 모빌리티를 그려 나가고 있습니다. 고객 개인에게는 다양한 이동 수단을 제공하여 최종 목적지까지 끊김 없이 이동하도록 하고, 이동 전후로 이동과 연계된 호스피탈리티 경험까지 제공하고자 합니다. 사회적으로는 자차 소유를 줄임으로써 전 지구적 관심사인 탄소 줄이기를 실현하는 것을 목표로 하고 있습니다.&lt;/p&gt;

&lt;p&gt;이처럼 멋진 미래 모빌리티를 만드는 일에 관심이 있다면 쏘카의 프로덕트 매니저(Product Manger)를 지원해 보세요. 미래 모빌리티를 직접 기획하고 제품화하여 우리 삶을 긍정적으로 변화시킬 수 있습니다.&lt;/p&gt;</content><author><name>지지</name></author><category term="product" /><category term="mobility" /><category term="ces" /><summary type="html">안녕하세요. 저는 쏘카에서 신규 서비스를 기획하는 프로덕트 매니저 지지입니다. 올해 1월, 저는 프로덕트 본부를 대표하여 모빌리티 서비스의 동향을 파악하고자 CES(Consumer Electronics Show) 2022에 다녀왔습니다.</summary></entry><entry><title type="html">쏘카 QA는 무슨 일을 하고 어떻게 일하나요?</title><link href="https://tech.socarcorp.kr/qa/2022/03/18/probationary-period_QA.html" rel="alternate" type="text/html" title="쏘카 QA는 무슨 일을 하고 어떻게 일하나요?" /><published>2022-03-18T02:00:00+00:00</published><updated>2022-03-18T02:00:00+00:00</updated><id>https://tech.socarcorp.kr/qa/2022/03/18/probationary-period_QA</id><content type="html" xml:base="https://tech.socarcorp.kr/qa/2022/03/18/probationary-period_QA.html">&lt;p&gt;안녕하세요.  작년 11월에 쏘카 QA 팀에 경력직으로 입사한 카밀라입니다.&lt;/p&gt;

&lt;p&gt;신입 때 QA 업무는 반복적인 업무를 하는 단순 직무라 생각했었습니다. 하지만 여러 프로젝트들을 진행하며 커버리지를 높이기 위한 활동들과 기본 기능을 지속적으로 검증하기 위한 자동화 테스트 구축 등 계속해서 배워야 할 것들이 많은 직무라는 것을 몸소 깨달았습니다.&lt;/p&gt;

&lt;p&gt;이번 글에서는 저의 입사 지원 과정부터 입사 후 3개월간의 수습 기간 동안 경험하며 느꼈던 내용을 공유해 보려고 합니다.&lt;/p&gt;

&lt;p&gt;다음과 같은 내용에 관심 있으신 분들이 보시면 도움 될 거라 생각합니다. 🙂&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카 QA 팀에 어떻게 입사했을까?&lt;/li&gt;
  &lt;li&gt;QA가 하는 일?&lt;/li&gt;
  &lt;li&gt;쏘카 QA 팀은 온보딩을 어떻게 진행할까?&lt;/li&gt;
  &lt;li&gt;쏘카의 QA 팀이 어떻게 업무할까?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;p&gt;저는 다음 순서에 따라 글을 적어보았습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QA 팀 입사 지원 과정
    &lt;ul&gt;
      &lt;li&gt;쏘카에 지원한 이유&lt;/li&gt;
      &lt;li&gt;입사 과정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QA란?
    &lt;ul&gt;
      &lt;li&gt;QA? 뭐 하는 팀이지?&lt;/li&gt;
      &lt;li&gt;일반적으로 QA는 무엇을 하나?&lt;/li&gt;
      &lt;li&gt;이슈를 찾기 위한 QA의 활동들&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;쏘카 QA의 업무
    &lt;ul&gt;
      &lt;li&gt;업무 방식&lt;/li&gt;
      &lt;li&gt;회의&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;쏘카 QA 팀의 온보딩 과정
    &lt;ul&gt;
      &lt;li&gt;쏘키에 익숙해지기&lt;/li&gt;
      &lt;li&gt;업무 경험하기&lt;/li&gt;
      &lt;li&gt;혼자 프로젝트 진행하기&lt;/li&gt;
      &lt;li&gt;온보딩을 하며 느낀 점&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;QA 직군 지원자를 위한 TIP&lt;/li&gt;
  &lt;li&gt;끝으로&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-qa-팀-입사-지원-과정&quot;&gt;1. QA 팀 입사 지원 과정&lt;/h2&gt;

&lt;p&gt;먼저 간략히 쏘카 QA 팀에 입사하게 된 과정에 대해 소개해 보려고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;11-쏘카에-지원한-이유&quot;&gt;1.1. 쏘카에 지원한 이유&lt;/h3&gt;

&lt;p&gt;제가 쏘카에 지원한 이유는 모빌리티 산업에 관심이 많았고 자동차를 좋아해서였습니다. 또한 다음의 기준들을 만족하는지도 중요했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;내가 좋아하는 일을 하면서 회사가 성장하고 나도 성장할 수 있는지&lt;/li&gt;
  &lt;li&gt;자체 서비스를 운영하는 회사인지&lt;/li&gt;
  &lt;li&gt;직원 규모가 어느 정도 이상인지(100명 이상)&lt;/li&gt;
  &lt;li&gt;지속적으로 성장하고 있는 회사인지&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;쏘카는 ‘쏘카 앱’을 기반으로 다양한 서비스를 운영하고 있고, 위와 같은 기준을 충분히 만족하는 회사였습니다. 그리고 채용공고와 커뮤니티 후기에서 QA 팀에서는 ‘주도적인 업무를 진행해 볼 수 있다’라는 부분과 ‘테스팅 자동화’를 진행한다는 부분이 가장 마음에 들었습니다. 쏘카에서는 저의 부족한 부분을 발전시키고 이전과는 다른 업무 경험을 쌓을 수 있을 것이라는 생각이 들었습니다.&lt;/p&gt;

&lt;h3 id=&quot;12-입사-과정&quot;&gt;1.2. 입사 과정&lt;/h3&gt;

&lt;p&gt;입사 과정은 다음 프로세스로 진행되었습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;서류 제출&lt;/li&gt;
  &lt;li&gt;1차 면접 (기술)&lt;/li&gt;
  &lt;li&gt;2차 면접 (본부장 면접)&lt;/li&gt;
  &lt;li&gt;3차 면접 (CTO 면접)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;서류를 제출하고 일주일이 지나기 전에 전화로 채용 과정에 대해 안내받았습니다. 1차 면접 전날까지 사전과제를 제출해야 한다는 안내를 받았고 1차 면접이 통과하면 2차, 3차 면접이 있다고 안내받았습니다. 사전과제는 채용공고에도 나와있듯 쏘카의 서비스 중 하나를 선택하여 테스트 케이스를 작성하는 것이었습니다.&lt;/p&gt;

&lt;p&gt;1차 면접(기술면접)에 실무진, 쏘카에 입사하면 같이 일하게 될 팀장님과 팀원 두 분이 면접관으로 참여하였습니다. 주로 이력서 위주의 질문이 나왔습니다. 참여했던 프로젝트에서 어떤 업무를 어떤 프로세스와 방식으로 진행했는지에 대한 질문과 업무를 하면서 마주할 수 있는 문제들에 대한 해결 방법을 묻는 질문이었습니다. 제출한 사전과제에 대해 해당 주제를 선택한 이유와 본인이 진행한 과제에 대해 설명하는 시간을 가졌고 마지막으로 입사해서 하고 싶은 업무와 본인의 성격에 대한 질문으로 마무리되었습니다.&lt;/p&gt;

&lt;p&gt;1차 면접 후 1주일 안에 합격 연락이 왔고 2차와 3차 면접이 같은 날에 진행될 것이라 안내받았습니다. 2차는 본부장 면접이고 3차는 인사팀과 CTO 면접이라 안내받았습니다.&lt;/p&gt;

&lt;p&gt;2차 면접에서는 본부장님과의 면접이 이루어졌고 1차와 비슷하게 이력서 위주의 질문들과 기술적인 부분, 인성 관련 질문도 있었습니다. 2차는 30분 정도 진행되었고 2차 면접 종료 후 바로 3차 면접을 진행했습니다. 3차 면접에서는 주로 CTO 님이 질문을 하셨고 딱딱한 질의응답보다는 저의 경험을 바탕으로 자유롭게 대화를 하듯 진행되었습니다. 그리고 조언도 해주셨는데 면접이 종료되고 난 뒤에도 해주신 말씀이 계속 생각나고 질문을 곱씹어 생각할 정도로 여운이 남는 면접이었습니다.&lt;/p&gt;

&lt;p&gt;3차 면접 이후로 최종적으로 합격하여 쏘카에서 일하게 되었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-qa란&quot;&gt;2. QA란?&lt;/h2&gt;

&lt;p&gt;먼저 쏘카 QA 업무 설명에 앞서, 일반적으로 QA는 어떤 일을 하는지 먼저 설명해 보고자 합니다.&lt;/p&gt;

&lt;h3 id=&quot;21-qa-뭐-하는-팀이지&quot;&gt;2.1. QA? 뭐 하는 팀이지?&lt;/h3&gt;

&lt;p&gt;QA에 대해 궁금하고 생소하신 분들도 계시고 왜 필요한지에 대한 의문을 가지는 분들도 많을 겁니다. 먼저 QA는 Quality Assurance의 약자로 ‘품질 보증’ 이란 뜻을 가지고 있습니다. QA 팀은 서비스의 ‘품질 보증’ 관련 업무를 하는 팀으로, &lt;strong&gt;서비스의 기능을 검증하고 관리하기 위한 일련의 활동을 합니다.&lt;/strong&gt; 프로젝트, 조직 규모가 작은 경우 개발자나 관리 조직에서 직접 기능 검증을 진행하기도 하지만, 프로젝트와 조직 규모가 커지는 경우 QA를 전담으로 하는 QA 팀을 구성해서 운영하기도 합니다.&lt;/p&gt;

&lt;p&gt;‘QA = Tester’라고 생각하시는 분들이 많습니다. 물론 QA의 활동 중 기능 테스트도 포함되어 있습니다. 하지만 QA는 단순히 서비스의 기능 테스트만 하지 않습니다. QA는 프로젝트의 시작부터 마무리까지 모든 과정에 참여하여 각 단계별로 품질을 저하시키거나 리소스가 낭비될 수 있는 요소를 발견한 뒤, 해당 프로젝트의 품질을 향상시키고 리소스 낭비를 방지하는 것을 목적으로 품질 보증 활동을 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_1.png&quot; alt=&quot;img&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-일반적으로-qa는-무엇을-하나&quot;&gt;2.2. 일반적으로 QA는 무엇을 하나?&lt;/h3&gt;

&lt;h4 id=&quot;킥오프-참여&quot;&gt;킥오프 참여&lt;/h4&gt;

&lt;p&gt;회사마다 QA가 투입되는 시기는 다를 수 있지만 주로 QA는 킥오프(Kick-off) 단계부터 참여하게 됩니다. &lt;strong&gt;단순히 버그를 찾기 위한 것이 아니라 프로젝트의 방향성, 목적, 요구사항 등 프로젝트의 근본적인 목적을 명확히 파악해야 올바른 검증을 진행할 수 있기 때문입니다.&lt;/strong&gt; 또한 초기 단계에서 발생할 수 있는 오류를 사전에 방지하여 낭비될 수 있는 리소스를 줄일 수 있습니다.&lt;/p&gt;

&lt;p&gt;프로젝트가 진행되는 중간에 QA가 참여할 경우 프로젝트의 구성과 기능 파악, 테스트 범위, 검증 항목 선정 등에 더 많은 시간을 소모하게 되고 초기에 발견할 수 있던 문제점이 발견된다면 불필요한 리소스가 낭비될 수 있습니다. 그리고 프로젝트의 기간과 목적에 적합한지, 프로젝트 기간을 고려하여 진행 가능한지, 기간이 얼마나 더 필요한지 등을 제대로 판단할 수 없게 됩니다.&lt;/p&gt;

&lt;p&gt;QA는 지속적으로 개발자, 기획자와 소통하며 기획의 목적대로 흘러가고 있는지 업무상 진행되는 과정에서 비효율적인 부분은 없는지 리뷰를 진행하고 개선안을 제시합니다. 또한 유저의 입장에서 발생할 수 있는 불편함은 없는지 확인합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_3.png&quot; alt=&quot;img&quot; /&gt;
&lt;em&gt;QA의 업무 단계별 활동과 산출물입니다.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;분석--qa-plan-작성&quot;&gt;분석 &amp;amp; QA Plan 작성&lt;/h4&gt;

&lt;p&gt;킥오프가 진행된 후에는 기획서를 분석하여 프로젝트 수행 시 발생할 수 있는 리스크를 예상해 보고 테스트 전략을 수립합니다.&lt;/p&gt;

&lt;p&gt;이후에는 QA Plan 또는 테스트 계획이라고도 하는 문서는 작성합니다. &lt;strong&gt;QA Plan은 테스트를 하기 위해 필요한 리소스들을 요약해놓은 문서로, QA가 정해진 프로세스대로 업무를 수행하기 위한 청사진 역할을 합니다.&lt;/strong&gt; 이 안에서 테스트의 명확한 기준을 세워 원하는 방향으로 테스트가 진행될 수 있도록 합니다. 이 문서는 테스트를 수행하는 도중 변경되기도 하는 등 지속적으로 관리됩니다.&lt;/p&gt;

&lt;p&gt;QA Plan에는 다음 내용들이 포함됩니다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[프로젝트 명] QA Plan

1. 요약
2. 프로젝트명
3. 프로젝트 자료
    3.1. 기획서
    3.2. 디자인
    3.3. Test case
    3.4. Test data
4. 참여자
5. 배포되는 버전
6. QA 기간
    6.1. Test case 작성 기간
    6.2. 테스트 기간
    6.3. Sign off 날짜
    6.4. 배포 요청일
    6.5. 모니터링
7. 테스트 범위
8. 테스트 제외 범위
9. 품질 목표 설정
10. 테스트 종료 조건 설정
11. 테스트 환경
    11.1. 디바이스
    11.2. OS 버전
12. 테스트 요청사항
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;테스트-케이스-작성&quot;&gt;테스트 케이스 작성&lt;/h4&gt;

&lt;p&gt;QA Plan이 작성되었다면 테스트 케이스(TC)를 작성합니다. &lt;strong&gt;테스트 케이스를 작성하는 과정에서 요구사항에 대한 오류를 찾을 수도 있고 고려되지 않았던 부분을 찾아낼 수 있습니다&lt;/strong&gt;. 또한 테스트 케이스 리뷰 과정을 통해 팀 구성원들에게 테스트 케이스 적정성을 점검하고 잘못된 이해로 인한 오류도 점검할 수 있습니다.&lt;/p&gt;

&lt;p&gt;다음은 테스트 케이스 예시입니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ID&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Category&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Preconditions&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Steps&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Expected result&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Galaxy21 (Android 11)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;iPhoneX (iOS 14.4.2)&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Comment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Socar_001&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;예약하기&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;쏘카 앱에 로그인되어 있는 상태&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;지도에서 쏘카 존을 선택&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;선택한 쏘카 존의 차량 목록이 노출됨&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PASS&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;N/A&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;다른 기종 확인 필요&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;테스트-데이터와-환경-준비&quot;&gt;테스트 데이터와 환경 준비&lt;/h4&gt;

&lt;p&gt;테스트 케이스 작성과 리뷰가 완료되었다면, 본격적으로 테스트에 들어가기에 앞서 테스트 데이터와 환경을 준비해야 합니다. &lt;strong&gt;테스트를 수행하기 위한 기본 데이터들을 정리하고 어떤 환경에서 테스트가 시작될지를 미리 준비하는 과정입니다.&lt;/strong&gt; 테스트를 위한 데이터의 예로는 테스트 계정, 계정에 따른 권한 등이 있을 수 있고 테스트 환경은 테스트 서버 설정이 포함됩니다.&lt;/p&gt;

&lt;h4 id=&quot;테스트-수행--버그-리포트-작성&quot;&gt;테스트 수행 &amp;amp; 버그 리포트 작성&lt;/h4&gt;

&lt;p&gt;본격적으로 테스트 수행을 시작하게 되면 준비된 테스트 케이스 외에도 &lt;strong&gt;탐색적 테스트, ad-hoc 테스트&lt;/strong&gt;를 통해 이슈를 발견할 수 있습니다. 이때 발견한 이슈들을 구두로만 개발자와 주고받게 된다면 히스토리 관리가 힘들어질 수 있습니다. 이런 불편함을 줄이고 프로젝트 이슈들을 관리하기 위해 버그 리포트를 작성합니다.&lt;/p&gt;

&lt;p&gt;버그 리포트에는 다음 내용들이 포함됩니다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[버그리포트 제목]

- 발생 환경(os 버전, 앱/웹 버전, 서버)
- 재현율
- 이슈 설명
- 사전 조건
- 재현 절차
- 예상 결과
- 실제 결과
- 담당 부서
- 우선순위/심각도
- 발생 버전/수정 버전
- 이슈 카테고리
- 첨부파일
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;테스트-결과-공유&quot;&gt;테스트 결과 공유&lt;/h4&gt;

&lt;p&gt;테스트가 완료되고 나면 결과를 정리하여 프로젝트 구성원들에게 공유합니다. QA Plan에서 정한 품질 목표를 달성하여 &lt;strong&gt;릴리즈가 가능한 상태인지를 알려주고 테스트 수행 결과와 품질 목표 달성에 따른 판단 결과, 테스트 진행하면서 발생한 이슈 현황들에 대한 정보&lt;/strong&gt;를 담고 있습니다.&lt;/p&gt;

&lt;p&gt;QA가 수행하는 단계마다 산출물이 발생하고 있고 이 산출물들은 QA가 프로젝트를 수행함에 있어 어떤 업무를 수행하고 어떻게 진행해야 하는지 방향을 알려주는 표지판 역할을 하게 됩니다. 수행하는 단계별로 QA 업무에 대해 정리하였지만 사실 ‘여기까지가 QA의 업무입니다’라고 할 수 없습니다.&lt;/p&gt;

&lt;p&gt;사실 유관부서와 협의하에 품질 향상에 도움이 되는 일 중 QA가 할 수 있는 모든 일들을 한다고 생각하시면 QA 업무가 더 쉽게 와닿을 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;23-이슈를-찾기-위한-qa의-활동들&quot;&gt;2.3. 이슈를 찾기 위한 QA의 활동들&lt;/h3&gt;

&lt;p&gt;“이슈 찾는 활동? 테스트 케이스로만 테스트하고 이슈 찾는 거 아니었나요?”라고 생각하실 수 있습니다.&lt;/p&gt;

&lt;p&gt;작성한 테스트 케이스로만 이슈를 찾을 수는 없습니다. 일반적으로 테스트 케이스는 기획서를 기반으로 재현 조건과 기대 결과를 도출하여 기대 결과가 정상적으로 출력되는지 확인하는 긍정 테스트에 가깝습니다. 그럼 부정 테스트 케이스를 추가하면 되지 않을까? 물론 부정 테스트의 방법도 있지만 비정상적인 상황은 너무 다양하기 때문에 모든 것을 다 케이스화하는 것은 현실적으로 불가능합니다. 또한 자주 업데이트가 되는 서비스라면 더욱 힘들 겁니다.&lt;/p&gt;

&lt;p&gt;그래서 QA는 각 단계에서 할 수 있는 최선의 활동들을 통해 이슈를 찾아가고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;개발 전에 프로젝트의 목적과 기획서를 바탕으로 다양한 시나리오를 생각하는 과정에서 명확히 정의되지 않은 것들을 확인한 뒤, 이해관계에 따라 발생할 수 있는 문제점들을 발견하여 개선될 수 있도록 합니다.&lt;/strong&gt; 또한 기획이나 컨텐츠 리뷰를 통해 논리적 오류나 유저에게 잘못 이해될 수 있는 부분, 오타 등을 확인하여 &lt;strong&gt;개발 이후 발생할 수 있는 이슈들을 파악&lt;/strong&gt;합니다.&lt;/p&gt;

&lt;p&gt;또한 단순히 버그를 찾는 것이 아닌 유저 입장에서 미래에 발생할 수 있는 문제들을 미리 예측해 보는 것도 이슈를 찾는 활동입니다. 실제 유저의 입장에서 서비스를 사용해 보며 유저가 사용할 수 있는 경로는 어떤 것이 있으며 어떤 부분에서 불편함을 느낄 수 있을지에 대한 고민을 합니다. 이때 유저 사용성에 따른 시나리오를 분리하여 테스트를 진행하기도 하는데 앞서 언급한 부정 테스트(Unhappy Path Test)와 긍정 테스트(Happy Path Test) 방법이 있습니다. 부정 테스트는 주어진 소프트웨어가 올바르게 작동하는지 확인하기 위해 잘못된 데이터를 입력하고 잘못된 작업을 수행하도록 구성합니다. 그러면 소프트웨어는 작동되지 않고 &lt;strong&gt;사용자가 이해할 수 있는 오류 메시지를 노출해 주는지 확인합니다.&lt;/strong&gt; 반대로 긍정 테스트는 오류가 생성되지 않는 데이터를 입력하여 &lt;strong&gt;소프트웨어가 의도대로 작동하는 것을 확인합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;기존 서비스에 기능이 추가되는 경우 기존 서비스가 정상 동작하는지 확인하기 위해 기본 기능에 대한 체크리스트를 수행하기도 하지만, 반복적으로 진행해야 되는 테스트의 경우 자동화 테스트를 진행하기도 합니다. 자동화 테스트란 자동화 도구로 테스트 스크립트를 개발하여 소프트웨어의 유효성을 검사하는 것입니다. &lt;strong&gt;자동화 테스트를 통해 기존에는 발생하지 않았던 문제점들을 찾아낼 수 있고 변경된 부분을 발견할 수 있습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이처럼 QA는 테스트 케이스를 활용하는 테스트 외에도 다양한 활동들 가운데 품질 향상을 위해 문제점을 찾고 개선하는 노력을 하고 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;3-쏘카-qa의-업무&quot;&gt;3. 쏘카 QA의 업무&lt;/h2&gt;

&lt;p&gt;이제 쏘카 QA 팀에서는 구체적으로 어떻게 업무하는지에 대해서 설명드리고자 합니다.&lt;/p&gt;

&lt;h3 id=&quot;31-업무-방식&quot;&gt;3.1. 업무 방식&lt;/h3&gt;

&lt;p&gt;QA라도 회사마다 각기 다른 프로세스에 따라 업무를 진행합니다. 기획과 개발이 다 완료된 상태에서 QA가 투입되는 경우도 있고, 주제만 정하고 기획을 만들어 가는 과정에서 QA가 투입되는 경우, 기획서가 만들어진 후 QA가 투입되는 경우 등 다양한 단계에서 참여가 이루어지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;쏘카에서는 QA 요청서가 발의되었을 때 QA가 프로젝트에 참여합니다.&lt;/strong&gt; QA 요청서는 JIRA를 통해 만들어지고 정해진 양식에 맞춰 프로젝트 리더가 작성하여 요청합니다. 이 요청서에는 기획서가 포함되어 있고 기획, 개발, 디자인 일정이 포함됩니다. 프로젝트별로 상이하긴 하지만 디자인이 완료된 경우 디자인 링크도 포함됩니다. 그리고 프로젝트에 관련된 자료들이 첨부됩니다.&lt;/p&gt;

&lt;p&gt;QA 요청서가 발의되면 팀 내에서 &lt;strong&gt;담당자를 정하게 되고 본격적으로 프로젝트에 투입&lt;/strong&gt;되어 업무를 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배정된 담당자는 킥오프(Kick-off) 회의에 참여&lt;/strong&gt;하여 프로젝트의 규모와 목적을 파악하고 다음 내용들을 &lt;strong&gt;QA Plan에 작성&lt;/strong&gt;합니다.
    - 프로젝트 정보
    - QA 일정
    - 테스트 데이터
    - 테스트 기준
    - 테스트 범위
    - 테스트 환경&lt;/p&gt;

&lt;p&gt;QA Plan을 작성할 때 보통 QA 요청서를 참고하고 QA를 진행하면서 필요할 사항들을 수집하고 정리합니다. 프로젝트를 진행하기에 앞서 필요한 전제 조건들과 검증 시 반드시 확인해야 될 사항들, 그리고 검증 진행 방법들도 회의를 통해 확인하고 적어놓습니다. 쏘카의 QA Plan은 초기에 한번 적고 끝나는 것이 아니라 테스트 케이스를 작성하거나 테스트를 수행하면서도 새로운 내용이나 참고할 사항들을 꾸준히 업데이트하게 됩니다.
프로젝트에 관해 설명이 더 필요한 부분이나 의견이 있다면 PM, 개발자, 디자이너 등 프로젝트에 참여하는 인원들과 소통하여 문제를 해결합니다.&lt;/p&gt;

&lt;p&gt;프로젝트에 대해 파악되었다면 &lt;strong&gt;테스트 케이스 혹은 체크리스트를 작성&lt;/strong&gt;합니다. 프로젝트의 규모가 크지 않거나 빠르게 확인해야 프로젝트의 경우, 확인해야 될 주요 항목을 간략하게 적어서 정상적으로 수행이 되는지를 판단하기 위해 체크리스트를 작성하게 됩니다. 체크리스트와 다르게 테스트 케이스의 경우에는 전제조건과 수행하는 절차 그리고 수행함으로 인해 기대되는 결과가 포함되기 때문에 체크리스트보다 더 세밀하게 기능을 테스트를 진행할 수 있습니다. 쏘카에서는 테스트 케이스 관리를 웹 기반 테스트 관리 시스템인 &lt;a href=&quot;https://testlink.org/&quot;&gt;Testlink&lt;/a&gt;로 하고 있습니다. 테스트 케이스는 작성하는 사람에 따라 다르겠지만 주어진 형식은 맞추되 작성하는 건 본인 성향에 따라 작성하고 있습니다. 어떤 사람은 오타와 띄어쓰기 하나까지 세세하게 테스트 케이스로 작성할 수 있고 어떤 사람은 기능만 위주로 작성하여 차이가 나지 않을까 우려할 수 있지만 테스트 케이스를 작성한 후 팀 내에서 리뷰를 진행하고 피드백을 통해 맞춰가고 있습니다. 또한 필요에 따라 &lt;strong&gt;팀 내에서 테스트 케이스 리뷰가 진행된 후 프로젝트 내에서도 테스트 케이스 리뷰&lt;/strong&gt;를 진행합니다.&lt;/p&gt;

&lt;p&gt;개발이 완료되고 나면 개발 환경에서 테스트를 수행할 수 있게 &lt;strong&gt;테스트 서버를 띄워 QA를 수행&lt;/strong&gt;합니다. 작성한 테스트 케이스를 바탕으로 테스트를 수행하는 데 프로젝트에 따라서 앱 또는 웹 테스트를 진행합니다. 데이터를 확인해야 되는 프로젝트라면 DB를 조회해서 테스트를 진행할 수도 있고 실제 차량에 들어가는 장비를 가지고 테스트를 진행하기도 합니다. 또한 일정에 따라 실제 차량으로 테스트를 진행하는 등 다양한 테스트 활동을 추가로 진행하기도 합니다. &lt;strong&gt;테스트를 수행하면서 발견되는 이슈들은 JIRA에 등록&lt;/strong&gt;합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;QA가 완료된 후에 팀과 프로젝트 내에 프로젝트가 종료되었다는 의미로 Sign off&lt;/strong&gt;를 합니다. 프로젝트 수행 내용을 간략하게 전달하고 서버나 웹 관련 프로젝트라 배포까지 완료되었다면 모니터링에 대한 내용도 포함하여 공유합니다.&lt;/p&gt;

&lt;p&gt;그 후에는 &lt;strong&gt;프로젝트를 수행하면서 알게 된 내용이나 공유할 내용, 남겨야 되는 주요 토픽 등에 대해 자유롭게 문서 정리&lt;/strong&gt;를 합니다. QA Plan에 내용을 추가하여 정리하는 경우도 있지만 규모와 히스토리가 긴 프로젝트의 경우엔 컨플루언스에 따로 페이지를 만들어서 작성합니다.&lt;/p&gt;

&lt;p&gt;프로젝트별로 QA가 완료되고 나면 &lt;strong&gt;하나의 버전으로 배포하기 위해 검토를 한 후 빌드 요청&lt;/strong&gt;을 합니다. 빌드 된 앱의 회귀 테스트(Regression test)를 진행한 후 앱 심사를 요청하고 &lt;strong&gt;심사가 완료되고 난 뒤 QA 팀에서 마켓 배포를 수행합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-회의&quot;&gt;3.2. 회의&lt;/h3&gt;

&lt;p&gt;쏘카 QA 팀에서 정기적으로 진행되는 회의는 2개가 있는데 그중 하나는 &lt;strong&gt;매일 아침 10시에 업무공유를 위한 회의&lt;/strong&gt;입니다. 각자 당일에 진행할 업무에 대해 공유하고 전달사항을 공유 받는 회의입니다. 또한 새로운 업무가 생겼을 때 담당자를 지정하는 것도 아침 회의 시간에 주로 진행됩니다.&lt;/p&gt;

&lt;p&gt;또 다른 하나는 &lt;strong&gt;매주 금요일 오후 2시에 진행하는 회고&lt;/strong&gt;입니다. 한주를 돌아보며 자유롭게 이야기하고 의견을 나누는 시간입니다. 한 주 동안 업무를 하면서 좋았던 점과 아쉬웠던 점에 대해 이야기를 하고 진행하고 있는 프로젝트나 팀 내에서 논의하고 싶은 내용에 대해 주제를 정해 이야기하기도 합니다. 처음에는 회고라 해서 무겁고 딱딱한 분위기일 수도 있겠다라 생각이 들었지만, 실제로 해보니  편하게 수다 떨듯 얘기하면서도 진지한 얘기를 할 때는 진지하게 의견을 나누기도 했습니다. 참여하기 전에는 무슨 얘기를 해야 될지, 이런 얘기도 해도 되나? 하는 걱정이 들었지만 생각보다 2시간이 훌쩍 지나가곤 합니다. 회고 시간을 통해 팀원들과 다 같이 얘기하는 시간을 갖고 업무에 대한 조언과 도움도 받을 수 있습니다. 또한 우리 팀이 나아 갈 방향과 진행할 업무들 그리고 개선할 점들을 토론하며 각자가 QA 팀의 일원으로서 함께 팀을 만들어나가는 의미 있는 시간이라 생각합니다.&lt;/p&gt;

&lt;p&gt;이 외에 한 달에 한 번 쏘카의 전 직원들이 참여하는 &lt;strong&gt;Town Hall 이라는 회의&lt;/strong&gt;가 있고 각자가 속한 &lt;strong&gt;프로젝트에서 진행하는 회의&lt;/strong&gt;가 있습니다. 팀 내에서 갑자기 생긴 논의사항이나 전달사항이 있을 경우에도 회의가 생기기도 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-쏘카-qa-팀의-온보딩-과정&quot;&gt;4. 쏘카 QA 팀의 온보딩 과정&lt;/h2&gt;

&lt;p&gt;다음으로 QA 팀에 합류한 이후 온보딩 과정의 경험을 공유해 보려 합니다.&lt;/p&gt;

&lt;h3 id=&quot;41-쏘카에-익숙해지기&quot;&gt;4.1. 쏘카에 익숙해지기&lt;/h3&gt;

&lt;p&gt;채용 프로세스가 끝나고 드디어 쏘카 입사 첫날! 로비로 가는 것부터 지하를 뱅글뱅글 돌고 물어보고서야 찾아갈 수 있었습니다. 팀 내에서 진행되는 온보딩은 4주 과정으로 계획되어 있었지만 일정상 5주간 진행되었습니다.&lt;/p&gt;

&lt;p&gt;재택근무를 시행하고 있어 동료들을 많이 만나진 못했지만 화상회의로 또는 종종 사무실에 출근하시는 분들과 인사하고 같이 점심을 먹거나 티타임을 가졌습니다. 팀 온보딩을 진행하는 동안 저는 사무실 출근을 했고 사무실 투어도 하고 근처 맛집들을 가보기도 하며 종종 서울숲 산책도 했습니다. 그동안 미로 같던 회사 길도 이리저리 잘 찾아다닐 수 있게 되었고 예약도 찾기도 어렵던 회의실도 척척 예약하고 찾아갈 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;PX(People Experience) 팀에서 진행하는 온보딩은 입사 후 일주일 뒤에 하루 일정으로 진행되었습니다. 같은 날 입사한 동료들과 온보딩에 참여하여 자연스럽게 서로에 대해 알아가며 점심도 같이 먹으며 친해질 수 있었습니다. 또한 쏘카의 히스토리와 문화를 액티비티를 통해 몸으로 익히며 알 수 있었습니다. 온보딩을 같이한 동료들과는 팀이 달라 자주는 아니지만 종종 만나서 점심을 먹기도 하고 티타임을 갖고 있습니다. 입사 후 몇 번의 만족도 조사를 시행했는데 신규 입사자가 쏘카에 적응할 수 있도록 지속적으로 돕는 프로세스가 정말 잘 되어있다고 느꼈습니다.&lt;/p&gt;

&lt;h3 id=&quot;42-업무-경험하기&quot;&gt;4.2. 업무 경험하기&lt;/h3&gt;

&lt;p&gt;QA 팀에서 진행된 온보딩은 5주 동안 진행되었고 입사 후 첫 주는 주로 PC 세팅과 업무에 필요한 장비와 계정을 등록했습니다. 업무 파악을 위해 각종 회의에 참여하기도 했습니다. 2주에 걸쳐 팀장님에게 업무 프로세스와 쏘카 앱에 대한 설명과 히스토리에 대해 교육을 받았습니다. 이후에는 쏘카 앱을 직접 사용해 보는 시간을 가졌고 각 서비스들을 담당했던 팀원들과 같이 해당 서비스를 살펴보는 시간을 가졌습니다. 3주 차엔 테스트 케이스 관리 툴의 사용법을 익혔습니다. 쏘카 앱 체크리스트를 직접 수행하면서 교육받은 내용을 리마인드하고 모르는 부분을 체크하는 시간도 가졌습니다. 마지막으로 4주 차, 5주 차에는 팀원과 같이 프로젝트에 참여하여 실제 업무를 익혔습니다. 해당 프로젝트 종료 후에는 혼자 프로젝트에 참여하여 직접 QA 업무를 수행할 수 있었습니다.&lt;/p&gt;

&lt;h4 id=&quot;문서-작성은-중요해&quot;&gt;문서 작성은 중요해&lt;/h4&gt;

&lt;p&gt;쏘카에 입사 후 놀랐던 것 중 하나가 신규 입사자가 볼 수 있는 자료가 많다는 것이었습니다.
회사와 업무에 대해 궁금한 것들을 찾아볼 수 있게 자료들이 잘 정리되어 있습니다.
신규 입사자들이 쏘카의 업무방식과 쏘카에 대한 이야기, 그리고 업무지원 및 요청 가이드 등을 알 수 있도록 ‘SOCAR 백과사전’ 이라는 것이 있고 노션(Notion)과 컨플루언스(Confluence)에 각 팀별, 프로젝트 별로 문서가 정리되어 있습니다.&lt;/p&gt;

&lt;p&gt;신규 입사자를 위한 것뿐 아니라 쏘카의 모든 직원들을 위한 자료도 많았습니다. 저도 입사 후 온보딩을 진행하면서 필요하다고 생각되는 문서들은 직접 만들거나 기존 문서를 업데이트하기도 했습니다.
이 문서들은 한 번에 일괄적으로 작성되는 것은 아니고, 업무를 하는 동안 본인이 수행한 업무를 리마인드하고 히스토리를 관리하기 위해 작성되곤 했습니다. 이렇게 모인 자료들은 쏘카의 모든 직원들에게 공유될 수 있었습니다. 저는 본인이 수행한 일을 문서로 남기는 것은 실제 업무를 수행하는 것만큼이나 중요하다고 생각하는데, 이게 생각과 다르게 지속되기란 쉽지 않은 문제입니다. 그런데 쏘카에서는 많은 동료들이 본인이 진행한 업무뿐만 아니라, 다른 동료들에게 필요할 수 있는 정보들을 문서화하고 지속적인 업데이트를 통해 관리해나가고 있는 것을 볼 수 있어서 인상 깊었습니다.&lt;/p&gt;

&lt;h4 id=&quot;같이하는-프로젝트-진행-경험하기&quot;&gt;같이하는 프로젝트 진행 경험하기&lt;/h4&gt;

&lt;p&gt;온보딩 4주 차에는 팀원과 같이 프로젝트에 참여하여 실제 업무를 하며 익히는 시간을 가졌습니다. 업무에 익숙한 팀원과 같이 프로젝트를 진행하면서 쏘카의 업무를 알아가도록 도와주는 과정이었습니다. 저는 팀 동료 에이미와 같이 프로젝트에 참여하였는데, 어느 단계에서 무엇을 해야 하는지, 다음 순서는 무엇인지, 제가 수행할 수 있는 업무는 직접 해볼 수 있게 에이미가 도와주셨고, 처음 해보는 것은 같이 수행하면서 업무에 적응할 수 있도록 도와주셨습니다. 물론 프로젝트마다 참여하는 인원은 다르지만 담당자와 어떻게 소통을 해야 하는지, 어떤 방식으로 업무가 진행되는지에 대해서도 배울 수 있는 시간이었습니다.&lt;/p&gt;

&lt;h4 id=&quot;모르는-건-질문하기&quot;&gt;모르는 건 질문하기&lt;/h4&gt;

&lt;p&gt;새로운 회사에 경력직으로 입사를 하더라도 새로운 회사의 업무방식에 맞춰 기존에 알던 것도 다시 보고 사용하던 도구들도 다시 배우게 됩니다. “경력직이니까 이런 건 당연히 알겠지?”라고 생각하시는 분들이 종종 있기 때문에, 입사 전에 심적으로 꽤 큰 부담이 있었습니다. 하지만 걱정했던 게 무색할 정도로 쏘카에 첫 출근하는 날 모두들 “모르는 거 궁금한 거 언제든 물어보세요”라고 먼저 말씀해 주셨습니다. 저도 마음을 조금 내려놓고 정말 많이 질문했는데 다들 하나같이 친절하게 알려주셨습니다. 시간 날 때마다 계속 신경 써주시고 알려주셔서 업무를 익히고 적응하는 데 도움이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_4.png&quot; alt=&quot;img&quot; width=&quot;50%&quot; /&gt;
&lt;img src=&quot;/img/QA/qa_5.png&quot; alt=&quot;img&quot; width=&quot;50%&quot; /&gt;
&lt;em&gt;팀 채널에 궁금한 사항을 남기면 팀원 분들이 친절하게 알려주십니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또한 회사 뒤편에 서울숲이 있어 점심을 먹고 가볍게 산책하면서 소소한 이야기를 나누기도 합니다. 회사가 카페가 많은 성수동에 있어, 커피를 종종 마시면서 수다를 떨면서 자연스럽게 모르는 걸 묻기도 하고, 팀원들 간에 서로 일상을 공유하는 시간을 가지기도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_6.jpeg&quot; alt=&quot;img&quot; width=&quot;50%&quot; /&gt;
&lt;em&gt;성수동엔 이쁜 카페가 많습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_7.jpeg&quot; alt=&quot;img&quot; width=&quot;50%&quot; /&gt;
&lt;em&gt;가을에 서울숲에 꼭 가보시길 추천드립니다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;43-혼자-프로젝트-진행하기&quot;&gt;4.3. 혼자 프로젝트 진행하기&lt;/h3&gt;

&lt;p&gt;같이 프로젝트를 진행해 본 후 혼자서 프로젝트에 참여하게 되었습니다. 쏘카의 QA로서 처음 맡은 업무였습니다.&lt;/p&gt;

&lt;p&gt;참여하게 된 프로젝트는 일부 기능 변경이 있는 작은 규모였습니다. 하지만 당혹스럽게도 팀 동료 에이미와 함께 진행했던 프로젝트와는 일하는 방식이 달랐습니다. 팀원들에게 조언을 구하며 업무를 진행했고, 예상치 못한 문제들이 발생하여 일정이 기존에 계획했던 것보다 딜레이가 되었지만 결과적으로는 잘 마무리를 할 수 있었습니다. 프로젝트가 종료된 후에는 진행한 업무에 대해 문서 정리를 하고, 이해관계자들과 논의하는 자리를 마련하여 개선되었으면 하는 부분과, 정리가 아직 안 된 부분에 대해 정리하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;44-온보딩을-하며-느낀-점&quot;&gt;4.4. 온보딩을 하며 느낀 점&lt;/h3&gt;

&lt;p&gt;쏘카에 와서 처음부터 지금까지 너무 좋다고 생각하는 부분은 모두가 자유롭게 의견을 내고 듣는 사람들도 편견 없이 들어주고 같이 고민하고 개선하려 한다는 것입니다. 적극적이고 쏘카의 서비스를 쏘카의 모든 직원들이 같이 만들어나가고 있다는 느낌을 받았고 제가 그런 쏘카의 직원이라는 게 자랑스럽게 느껴졌습니다. 개개인이 아닌 쏘카라는 하나의 목표를 향해 나아간다는 느낌을 들었고 그것이 누군가의 강요가 아닌 모두의 자발적인 모습에서 비롯된 것이라는 점이 인상 깊었습니다.&lt;/p&gt;

&lt;p&gt;한편 온보딩을 하는 동안 좀 더 개선하면 좋겠다고 생각한 것은 ‘회의가 너무 많아서 줄었으면 좋겠다’ 이었습니다. 어떤 날은 정말 하루 종일 점심시간을 제외하고 회의에 참석한 날도 있었으니까요. 하지만 조직 개편 후에는 정기적으로 진행되던 많은 회의들이 사라졌고 본인 업무에 직접적인 연관이 있는 회의로 많이 축소되었습니다. 맡은 업무에 따라 여전히 많은 회의를 참석하는 날도 있지만 그만큼 할 수 있는 일이 늘었다는 것에 한편으로는 정말 쏘팸이 된 걸 느끼게 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-qa-직군-지원자를-위한-tip&quot;&gt;5. QA 직군 지원자를 위한 TIP&lt;/h2&gt;

&lt;p&gt;이번에는 QA 직군 지원자분들을 위한 저만의 팁에 대해 공유드리려 합니다.&lt;/p&gt;

&lt;p&gt;QA는 꼼꼼해야 해, 커뮤니케이션이 능숙해야 해, 대처법을 잘 알아야 해, 문제를 많이 찾을 수 있어야 해, 남들과 다른 관점에서 볼 줄 알아야 해 등 QA 직무에 대해 물어보면 주로 들을 수 있는 말입니다. 하지만 모든 QA가 저런 특성을 가지진 않습니다. 커뮤니케이션이 약한 사람도 있고 꼼꼼하지 못한 사람도 많습니다. 저런 부분은 각 사람의 성향에 해당하는 것이기 때문에 저런 성향을 가지신 분들이라면 좋겠지만 저는 다음 세 가지가 더 중요하다 생각합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;먼저 QA를 하고 싶다면 QA가 사용하는 기본적인 용어를 알고 있어야 된다 생각합니다.&lt;/strong&gt; 왜 그런 걸 알아야 하지? 몰라도 할 수 있지 않나? 하고 생각하실 수 있습니다. 물론 모르셔도 업무는 할 수 있습니다. 하지만 해당 직군에서 사용하는 용어들을 알고 있으면 본인 업무에 대한 이해와 수행할 수 있는 업무의 폭을 넓혀줄 수 있다 생각합니다. QA 채용공고를 보면 우대 항목에 ‘ISTQB 자격증’ 이 있습니다. 자격증이 있어야 된다는 것은 아닙니다. 하지만 QA에 대한 지식이 없다면 ISTQB 책을 한 번 정도는 읽어보면 업무를 이해하는 데 도움이 될 것입니다.&lt;/p&gt;

&lt;p&gt;두 번째로는 &lt;strong&gt;QA 프로세스를 알고 있어야 된다 생각합니다.&lt;/strong&gt; 여기서 말하는 프로세스는 단순히 ‘QA Plan을 작성하고 테스트 케이스를 만들어 테스트를 한다’가 아닌 프로젝트에 투입된 순간부터 QA가 각 단계별로 수행할 수 있는 업무와 만들어지는 산출물들의 구성을 파악하고 있는지를 말합니다. 회사별로 프로젝트 진행은 다르더라도 QA가 수행하는 기본 프로세스는 동일합니다. 해당 직무의 업무 순서와 정의를 알고 있다면 어떤 프로젝트에 참여하던지 흔들리지 않고 본인의 역할을 해낼 수 있다 생각합니다.&lt;/p&gt;

&lt;p&gt;마지막으로는 &lt;strong&gt;스스로 나아가려는 의지가 필요합니다.&lt;/strong&gt; 알아서 모든 일을 하라는 의미가 아니고 품질 개선을 위해 스스로 다양한 방법을 시도하고 업무를 찾아서 할 수 있어야 한다는 의미입니다. 글을 시작할 때 언급했던 바와 같이 QA 업무는 결코 단순하지 않습니다. 소프트웨어 개발 환경이 발전함에 따라 품질을 높이기 위한 방법들도 다양해지고 빠르게 변화하고 있습니다. 틀에 박힌 테스트 방식이 아닌 새로운 방법들을 스스로 해보고 발전시키려는 의지가 필요하다 생각합니다.&lt;/p&gt;

&lt;p&gt;위의 세 가지는 제가 면접관일 때나 지원자일 때 중요하게 생각하는 부분입니다. 그래서 면접을 보기 전에 QA 용어와 프로세스에 대해 다시 리마인드를 하고, 항상 배우려는 마인드 셋을 갖추려고 합니다. 저의 팁이라고 하기엔 거창한 거 같지만 이 팁이 QA 지원을 준비하거나 생각하시는 분들에게 작은 도움이라도 되길 바랍니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;6-끝으로&quot;&gt;6. 끝으로&lt;/h2&gt;

&lt;p&gt;처음 쏘카 블로그 글 제안을 받았을 때 걱정이 앞섰습니다. 글을 잘 쓰는 것도 아니고 누군가에게 설명하는 것도 잘하지 못해 글 순서를 정하는 것부터 쉽지 않았습니다. 글도 생각한 것처럼 써지지 않아 다시 읽을 때마다 수정하고 있고, 글의 마지막을 작성하는 지금도 다시 읽을 때마다 계속 지웠다 썼다를 반복하고 있습니다. 여전히 이 글이 어떻게 보일까 걱정은 되지만 최선을 다해 글을 썼고,  제가 쏘카에서 보낸 3개월을 돌아보며 QA로서 스스로도 다시 한번 돌아 볼 수 있는 뜻깊은 시간이었습니다. 쏘카 블로그 글 작성을 제안해 주신 팀장님 감사드리고 응원해 주신 팀원들도 감사합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/QA/qa_8.png&quot; alt=&quot;img&quot; width=&quot;30%&quot; /&gt;
&lt;em&gt;쏘카 10주년 기념으로 키카쿠브를 받았어요&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;-참고&quot;&gt;* 참고&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://brunch.co.kr/@210a51cb29764cb/2&quot;&gt;https://brunch.co.kr/@210a51cb29764cb/2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.illunex.com/202010105-2/&quot;&gt;http://blog.illunex.com/202010105-2/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://neklo.com/what-is-quality-assurance-testing/&quot;&gt;https://neklo.com/what-is-quality-assurance-testing/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://tech.devsisters.com/posts/not-enough-testcase/&quot;&gt;https://tech.devsisters.com/posts/not-enough-testcase/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brunch.co.kr/@hsoochun/13&quot;&gt;https://brunch.co.kr/@hsoochun/13&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>카밀라</name></author><category term="QA" /><category term="qa" /><category term="onboarding" /><summary type="html">안녕하세요. 작년 11월에 쏘카 QA 팀에 경력직으로 입사한 카밀라입니다. 신입 때 QA 업무는 반복적인 업무를 하는 단순 직무라 생각했었습니다. 하지만 여러 프로젝트들을 진행하며 커버리지를 높이기 위한 활동들과 기본 기능을 지속적으로 검증하기 위한 자동화 테스트 구축 등 계속해서 배워야 할 것들이 많은 직무라는 것을 몸소 깨달았습니다.</summary></entry><entry><title type="html">데이터 디스커버리 플랫폼 도입기 - 2편. GKE에 Datahub 구축하기</title><link href="https://tech.socarcorp.kr/data/2022/03/16/metdata-platform-02.html" rel="alternate" type="text/html" title="데이터 디스커버리 플랫폼 도입기 - 2편. GKE에 Datahub 구축하기" /><published>2022-03-16T07:00:00+00:00</published><updated>2022-03-16T07:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/03/16/metdata-platform-02</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/03/16/metdata-platform-02.html">&lt;p&gt;안녕하세요, 데이터 플랫폼 팀의 디니입니다.&lt;/p&gt;

&lt;p&gt;이번 글은 ‘쏘카의 데이터 디스커버리 플랫폼 도입기’ 3부작 중 2편입니다. &lt;a href=&quot;https://tech.socarcorp.kr/data/2022/02/25/data-discovery-platform-01.html&quot;&gt;1편 : 데이터 디스커버리 플랫폼 도입기 - 데이터 디스커버리란?&lt;/a&gt;에서는 데이터 디스커버리의 개념과 쏘카가 데이터 디스커버리 플랫폼으로 Datahub를 선택하게 된 의사결정 과정을 소개했습니다.&lt;/p&gt;

&lt;p&gt;2편에서는 Datahub를 GKE 환경에 배포한 과정과 데이터 디스커버리 플랫폼에 필요한 추가 기능들을 어떻게 구현하였는지에 대해 소개하려고 합니다. 다음과 같은 분들이 읽으시면 도움이 되리라고 생각합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;메타데이터 플랫폼 도입에 관심이 있는 개발자&lt;/li&gt;
  &lt;li&gt;클라우드 환경에 메타데이터 플랫폼 배포하는 과정에 관심이 있는 사람&lt;/li&gt;
  &lt;li&gt;쏘카 데이터 플랫폼팀 업무에 관심이 있는 사람&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;목차는 다음과 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#problem-definition&quot;&gt;문제 정의&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;1.1. 무엇을 해야 하나요?&lt;/p&gt;

    &lt;p&gt;1.2. 고려해야 할 부분&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#datahub-on-gke&quot;&gt;Datahub on GKE 배포 과정&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;2.1. GKE 배포&lt;/p&gt;

    &lt;p&gt;2.2. CloudSQL DB migration&lt;/p&gt;

    &lt;p&gt;2.3. Keycloak 인증&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#metadata-ingestion&quot;&gt;메타데이터 주입 과정&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;3.1. 메타데이터 주입 방법&lt;/p&gt;

    &lt;p&gt;3.2. 메타데이터 주입 정책 결정&lt;/p&gt;

    &lt;p&gt;3.3. 메타데이터 주입 과정 자동화 (with Airflow)&lt;/p&gt;

    &lt;p&gt;3.4. 메타데이터 추출 과정의 권한 축소&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;#wrap-up&quot;&gt;마무리&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-문제-정의&quot;&gt;1. 문제 정의&lt;a name=&quot;problem-definition&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;11-무엇을-해야-하나요&quot;&gt;1.1. 무엇을 해야 하나요?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub를 사내 클라우드 환경에 안정적으로 배포합니다.&lt;/li&gt;
  &lt;li&gt;Datahub에 메타데이터 주입 파이프라인을 자동화합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-고려해야-할-부분&quot;&gt;1.2. 고려해야 할 부분&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카는 데이터 소스로 MySQL(운영)과 BigQuery(분석)를 사용하고 있습니다. 두 데이터 소스의 특성을 고려한 메타데이터 주입 파이프라인이 필요합니다.&lt;/li&gt;
  &lt;li&gt;플랫폼 상의 데이터가 유실 위험 없이 안전하게 저장되어야 합니다.&lt;/li&gt;
  &lt;li&gt;인증된 사용자만 플랫폼에 접속할 수 있어야 합니다.&lt;/li&gt;
  &lt;li&gt;CI/CD 파이프라인을 이용한 배포 자동화가 되어야 합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-datahub-on-gke-배포-과정-&quot;&gt;2. Datahub on GKE 배포 과정 &lt;a name=&quot;datahub-on-gke&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;먼저 Datahub를 어떻게 사내 클라우드 환경에 안정적으로 배포했는지 알아보겠습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Datahub는 오픈소스 기반으로 매우 빠르게 업데이트되고 있습니다. 해당 배포는 6개월 전에 이루어진 것으로, 현재 Datahub 배포 및 metadata ingestion 과정과는 다소 차이가 있을 수 있습니다. 이 점 양해 부탁드립니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;21-gke-배포&quot;&gt;2.1. GKE 배포&lt;/h3&gt;

&lt;p&gt;쏘카 데이터 엔지니어링 그룹의 쿠버네티스 환경은 GCP(Google Cloud Platform)의 GKE(Google Kuberentes Engine)를 사용하고 있습니다. 또한 대부분의 애플리케이션을 &lt;a href=&quot;https://helm.sh/&quot;&gt;Helm&lt;/a&gt; Chart를 이용하여 클러스터에 배포하고 있습니다. Datahub 역시 공식 &lt;a href=&quot;https://github.com/acryldata/datahub-helm&quot;&gt;Helm Chart&lt;/a&gt;를 제공하고 있으며, 총 2벌의 차트로 구성되어 있습니다. Datahub Helm Chart 구성은 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;datahub&lt;/code&gt; : Datahub 애플리케이션에 필요한 요소들 설치 (Frontend, GMS 등)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;prerequisites&lt;/code&gt; : Datahub에 필요한 사전 요소들을 설치 (MySQL, Kafka, ElasticSearch, Neo4j 등)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-helm-chart-tree.png&quot; alt=&quot;datahub-helm-chart-tree&quot; /&gt; &lt;em&gt;Datahub 차트 구조&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;공식 Datahub Helm Chart의 Ingress를 쏘카의 환경에 맞게 수정한 뒤 배포하였습니다. 또한 원활한 테스트를 위해 개발 클러스터, 운영 클러스터에 각각 배포하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-pods.png&quot; alt=&quot;datahub-pods&quot; /&gt; &lt;em&gt;Datahub 최초 배포 시 Pod 상태&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-cloudsql-db-migration&quot;&gt;2.2. CloudSQL DB migration&lt;/h3&gt;

&lt;p&gt;Datahub는 자체 DB(storage)로 MySQL Pod을 사용합니다. 물론 PVC(PersistentVolumeClaim)이 붙어있긴 했지만, 앞으로 Datahub 애플리케이션 상에서 쌓일 데이터가 점점 늘어날 것이며 데이터의 내용 또한 중요하기 때문에, 앞으로의 확장성과 만에 하나라도 있을 유실 가능성을 방지하는 방향으로 아키텍처를 고민했습니다. 결국에는 MySQL Pod 대신 외부 데이터베이스로 GCP CloudSQL Instance를 연결하기로 결정했습니다.&lt;/p&gt;

&lt;p&gt;구체적으로는 다음 과정으로 진행했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CloudSQL DB (혹은 새로운 Instance) 생성&lt;/li&gt;
  &lt;li&gt;(Optional) 사용자 생성&lt;/li&gt;
  &lt;li&gt;Datahub가 CloudSQL 가리키게 하기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;기존 Datahub의 Helm Chart는 SQL Host로 MySQL Pod을 가리키고 있습니다. 위에서 만든 CloudSQL DB를 가리키게 하기 위해서 Helm Chart를 다음과 같이 수정합니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# charts/datahub/values.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;datasource&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;cloudsql_host_address&amp;gt;:&amp;lt;port&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;jdbc:mysql://&amp;lt;cloudsql_host_address&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;cloudsql_db_name&amp;gt;?verifyServerCertificate=false&amp;amp;useSSL=true&amp;amp;useUnicode=yes&amp;amp;characterEncoding=UTF-8&amp;amp;enabledTLSProtocols=TLSv1.2&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostForMysqlClient&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;cloudsql_host_address&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;port&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;com.mysql.jdbc.Driver&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;username&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;별도로 생성한 secret 이름&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;별도로 생성한 secret 키&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# charts/datahub/subcharts/datahub-gms/values.yaml&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;datasource&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;cloudsql_host_address&amp;gt;:&amp;lt;port&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;jdbc:mysql://&amp;lt;cloudsql_host_address&amp;gt;:&amp;lt;port&amp;gt;/datahub?verifyServerCertificate=false&amp;amp;useSSL=true&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;com.mysql.jdbc.Driver&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;username&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;별도로 생성한 secret 이름&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;별도로 생성한 secret 키&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실제로는 민감한 정보들을 Helm Chart에 직접 명시하지 않고 다음처럼 별도의 Secret으로 생성하여 참조하였습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;na&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;datasource&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-host&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-url&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostForMysqlClient&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-hostForMysqlClient&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3306&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;com.mysql.jdbc.Driver&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-username&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;secretKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-password&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Secret yaml 파일은 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Secret&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mysql-secrets&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;datahub&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Opaque&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-hostForMysqlClient&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-root-password&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;mysql-username&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;data&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;최종적으로 정상 작동을 테스트합니다. 예를 들어, Datahub UI 상에서 데이터를 수정한 뒤 해당 CloudSQL DB에서 동일한 데이터가 업데이트되는지 확인합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-create-policy.png&quot; alt=&quot;create-test-policy&quot; /&gt;&lt;em&gt;test_policy라는 정책을 생성해 보았습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-check-data.png&quot; alt=&quot;check-data&quot; /&gt;&lt;em&gt;CloudSQL DB에서 동일한 데이터가 확인됩니다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-keycloak-인증&quot;&gt;2.3. Keycloak 인증&lt;/h3&gt;

&lt;p&gt;Datahub이 배포되고 나면, 인증된 사용자만 애플리케이션에 접속되어야 합니다. Datahub는 Okta, Keycloak 등 여러 SSO를 지원합니다. 쏘카에서 이미 Keycloak을 이용하고 있기 때문에 Keycloak을 사용하기로 결정했습니다.&lt;/p&gt;

&lt;p&gt;Datahub에 Keycloak 로그인을 적용하는 방법은 간단했습니다. Helm Chart의 &lt;code class=&quot;highlighter-rouge&quot;&gt;values.yaml&lt;/code&gt;파일에서 frontend 부분에 몇 줄의 설정만 넣어주면 가능했습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;datahub-frontend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;extraEnvs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AUTH_OIDC_ENABLED&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;true&quot;&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AUTH_OIDC_CLIENT_ID&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;your-client-id&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AUTH_OIDC_CLIENT_SECRET&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;your-client-secret&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AUTH_OIDC_DISCOVERY_URI&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;your-provider-discovery-url&lt;/span&gt;  
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;AUTH_OIDC_BASE_URL&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;your-datahub-url&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;또한 다음과 같은 다양한 설정을 쉽게 정의할 수 있었습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# User and groups provisioning&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# OIDC 로그인 시, Datahub 상에 유저 없으면 자동 생성 여부&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;AUTH_OIDC_JIT_PROVISIONING_ENABLED=true&lt;/span&gt; 

&lt;span class=&quot;c1&quot;&gt;# OIDC 로그인 시, Datahub 상에 유저가 이미 존재해야 로그인 성공하는지 여부&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;AUTH_OIDC_PRE_PROVISIONING_REQUIRED=false&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# OIDC의 그룹 정보를 Datahub에 연동&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;AUTH_OIDC_EXTRACT_GROUPS_ENABLED=true&lt;/span&gt; 
&lt;span class=&quot;s&quot;&gt;AUTH_OIDC_GROUPS_CLAIM=&amp;lt;your-groups-claim-name&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-메타데이터-주입-과정--&quot;&gt;3. 메타데이터 주입 과정  &lt;a name=&quot;metadata-ingestion&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이렇게 Datahub를 클라우드 상에 안정적으로 구축했습니다. 하지만 아직 데이터 디스커버리 플랫폼으로서의 기능을 하지는 못합니다. 데이터 소스에서 실제로 메타데이터를 가져와서 플랫폼에서 보여줘야 하고, 이렇게 메타데이터를 가져오는 과정(=metadata ingestion)이 자동화되어야 합니다.&lt;/p&gt;

&lt;h3 id=&quot;31-메타데이터-주입-방법&quot;&gt;3.1. 메타데이터 주입 방법&lt;/h3&gt;

&lt;p&gt;Datahub에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;recipe&lt;/code&gt;라고 불리는 yaml 파일을 &lt;code class=&quot;highlighter-rouge&quot;&gt;datahub CLI&lt;/code&gt;로 실행하여 메타데이터를 주입합니다. 다음은 BigQuery에서 메타데이터를 가져오는 recipe 파일의 기본 예시입니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bigquery&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;project_id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;lt;project_id&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;credentials_path &lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;${GOOGLE_APPLICATION_CREDENTIALS}&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;sink&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;datahub-rest&quot;&lt;/span&gt;
 &lt;span class=&quot;na&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;na&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;${DATAHUB_GMS_ADDRESS}&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# datahub 애플리케이션의 backend 서버&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;source&lt;/code&gt; : 데이터 소스, 즉 “데이터를 어디서 가져오는지” 정의합니다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sink&lt;/code&gt; : “데이터를 어디에 저장하는지” 를 정의합니다. Datahub 어플리케이션에 올릴 수도 있고, 콘솔에 출력할 수도 있고, 파일로 저장할 수도 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 형식을 바탕으로 데이터 소스를 바꿀 수도 있고 여러 설정을 적용할 수도 있습니다. 파일을 실행할 때는 &lt;code class=&quot;highlighter-rouge&quot;&gt;datahub ingestion -c &quot;&amp;lt;파일_이름&amp;gt;&quot;&lt;/code&gt;으로 실행합니다.&lt;/p&gt;

&lt;h3 id=&quot;32-메타데이터-주입-정책-결정&quot;&gt;3.2. 메타데이터 주입 정책 결정&lt;/h3&gt;

&lt;p&gt;먼저 “어떤 데이터 소스”에서 메타데이터를 “얼마나 자주” 가져올 건지 결정해야 합니다.&lt;/p&gt;

&lt;p&gt;쏘카에서는 주요 데이터 소스로 MySQL Aurora(운영)와 BigQuery(분석)를 사용하고 있습니다. 하지만 이 데이터 소스의 모든 테이블을 가져올 필요는 없었습니다. 예를 들면 DB에 따라서 개인 정보 관련 민감한 데이터들도 있고, 분석 DB 쪽에는 굳이 전사에 공유될 필요는 없는 임시 테이블들도 다수 존재했습니다. 따라서 각 데이터 소스 별 DB의 목록을 사전에 정하고, 해당 DB의 메타데이터를 주입하기로 했습니다.&lt;/p&gt;

&lt;p&gt;참고로, 다음과 같이 Table 혹은 DB의 이름을 &lt;code class=&quot;highlighter-rouge&quot;&gt;regex pattern&lt;/code&gt;으로 감지하여 선택적 메타데이터 주입이 가능합니다. (물론 데이터 소스마다 방법이 약간 다를 수 있습니다 - 예시는 BigQuery입니다.)&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bigquery&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;schema_pattern&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;allow&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 허용하는 dataset 패턴&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my_dataset&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;table_pattern&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;allow&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 허용하는 table 패턴&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;^my_project.my_dataset.my_good_table&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;deny&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 허용하지 않는 table 패턴&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;^my_project.my_dataset.my_bad_table&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;include_views&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그러면 얼마나 자주 가져와야 할까요? 매일매일 필요에 따라 테이블이 생겨났다가 사라지기도 하고, 테이블의 칼럼이 추가되거나 변경되는 일도 있을 것입니다. 하지만 이런 변화들을 꼭 실시간으로 봐야 할 필요는 없다고 생각했습니다. 하루에 한 번 정도 업데이트한다면, 리소스도 효율화하고 사내 데이터 현황을 파악하는 데 충분하다고 결정을 내렸습니다.&lt;/p&gt;

&lt;h3 id=&quot;33-메타데이터-주입-과정-자동화-with-airflow&quot;&gt;3.3. 메타데이터 주입 과정 자동화 (with Airflow)&lt;/h3&gt;

&lt;p&gt;이렇게 하루에 한 번 메타데이터 주입을 결정하고 난 뒤, 메타데이터 주입을 어떻게 자동화했는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;하루에 한 번 Batch 단위의 주입이라면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Airflow DAG&lt;/code&gt;로 간단하게 구현할 수 있었습니다. 데이터 소스에서 메타데이터를 주입하는 Task를 만들고, 하루 한 번만 돌려주면 됐습니다. 그런데 이 작업에는 &lt;code class=&quot;highlighter-rouge&quot;&gt;datahub&lt;/code&gt; 패키지를 설치해야 하는 의존성이 필요합니다.&lt;/p&gt;

&lt;p&gt;데이터 플랫폼 팀에서는 이런 경우 Airflow에 직접 의존성을 설치하지 않고, 필요한 의존성을 담은 Docker Image를 실행하는 &lt;code class=&quot;highlighter-rouge&quot;&gt;KubernetesPodOperator&lt;/code&gt;를 만들어 해결하고 있습니다. 이렇게 하면 DAG 가 아무리 많아도 DAG 간 사용하는 라이브러리나 환경의 의존성 충돌을 방지할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/metadata-ingestion-flow.png&quot; alt=&quot;metadata-ingestion-flow&quot; /&gt; &lt;em&gt;메타데이터 주입 흐름&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;작성한 Dockerfile은 다음처럼 간단합니다.&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# datahub-ingestion 이미지를 이용합니다. &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; linkedin/datahub-ingestion:v0.8.20 &lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 미리 정의한 recipe 파일을 복사해 가져옵니다. &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; datahub-ingestion-bigquery /datahub-ingestion-bigquery &lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# datahub CLI를 이용하여 recipe를 실행합니다. &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; [&quot;ingest&quot;, &quot;-c&quot;, &quot;/datahub-ingestion-bigquery/recipe_bigquery.yaml&quot;] &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;datahub-ingestion-bigquery 안에는 recipe 파일이 들어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-ingestion-bigquery.png&quot; alt=&quot;datahub-ingestion-bigquery&quot; /&gt; &lt;em&gt;datahub-ingestion-bigquery 디렉터리 구조&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;34-메타데이터-추출-과정의-권한-축소&quot;&gt;3.4. 메타데이터 추출 과정의 권한 축소&lt;/h3&gt;

&lt;h4 id=&quot;어떻게-하면-최소한의-권한으로-메타데이터를-추출할-수-있을까&quot;&gt;어떻게 하면 최소한의 권한으로 메타데이터를 추출할 수 있을까?&lt;/h4&gt;

&lt;p&gt;Datahub는 메타데이터를 끌어오는 모든 대상 DB에 SELECT 권한을 허용해야 메타데이터 추출이 가능하도록 만들어져 있습니다. 예를 들면 MySQL DB에 3000개의 DB가 있다고 가정할 때, Datahub의 서비스 계정은 3000개의 DB에 대해 모두 권한이 있어야 하는 것입니다.&lt;/p&gt;

&lt;p&gt;하지만 이 권한을 단독 솔루션에 부여하기에는 보안상 너무 무겁다고 판단했습니다. 그래서 어떻게 하면 최소한의 DB에 접근하면서 같은 기능을 구현할 수 있을지가 큰 고민거리였습니다.&lt;/p&gt;

&lt;h4 id=&quot;information-schema에서-직접-뽑아내-보자&quot;&gt;Information Schema에서 직접 뽑아내 보자&lt;/h4&gt;

&lt;p&gt;당시 데이터 엔지니어링 팀 팀장(이시고 지금은 그룹장이신) 토마스가 아이디어를 주셨습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/file-based-ingestion-flow.png&quot; alt=&quot;file-based-ingestion-flow&quot; /&gt; &lt;em&gt;file을 이용하여 메타데이터 상태를 저장하는 흐름&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Datahub에는 데이터 소스의 메타데이터를 특정 형태의 &lt;code class=&quot;highlighter-rouge&quot;&gt;json file&lt;/code&gt;로 변환하여 저장하는 기능이 있습니다. 또한 같은 형식의 json file을 기반으로 메타데이터를 Datahub 플랫폼에 주입하는 것도 가능했습니다. 그리고 해당 파일 형식을 확인해 본 결과 &lt;code class=&quot;highlighter-rouge&quot;&gt;information_schema&lt;/code&gt;에서 대부분(사실 모두) 가져올 수 있는 정보였습니다.&lt;/p&gt;

&lt;p&gt;그러면 &lt;code class=&quot;highlighter-rouge&quot;&gt;information_schema&lt;/code&gt;에서 정보를 가져와서 file 형식을 맞춰 만들어주는 기능을 개발하고, 그 file을 기반으로 Datahub에 메타데이터를 주입하면 되지 않을까? 하는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/metadata-ingestion-as-is.png&quot; alt=&quot;metadata-ingestion-as-is&quot; /&gt;&lt;em&gt;metadata ingestion AS-IS 흐름&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/metadata-ingestion-to-be.png&quot; alt=&quot;metadata-ingestion-to-be&quot; /&gt;&lt;em&gt;metadata ingestion TO-BE 흐름&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그래서 앞부분은 python script로 개발하고, file을 기반으로 메타데이터를 주입하는 부분은 기존 Datahub 프레임워크를 그대로 이용했습니다.&lt;/p&gt;

&lt;div class=&quot;language-dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 파이썬 이미지를 이용합니다. &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.8 &lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; datahub-ingestion-mysql /datahub-ingestion-mysql &lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /datahub-ingestion-mysql/src&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; root&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# datahub를 포함한 필요한 의존성을 설치합니다. &lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-cache-dir&lt;/span&gt; mysql-connector-python&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;8.0.27 &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-cache-dir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--upgrade&lt;/span&gt; acryl-datahub&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.8.20 

&lt;span class=&quot;c&quot;&gt;# information_schema에서 메타데이터를 추출하는 python script를 실행하고, datahub CLI로 Datahub 플랫폼에 주입합니다.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;CMD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python main.py ; datahub ingest -c recipe_mysql_prod.yaml &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;main.py&lt;/code&gt;의 내용은 다음과 같습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# main.py
&lt;/span&gt;	
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;get_info_from_query&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_json_result&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;make_and_execute_query&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;execute_query&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mysql.connector&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errorcode&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;templates&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json_table_template&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;USER&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PASSWORD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;HOST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pattern_conditions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;schema_pattern_allowed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PATTERN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;schema_pattern_denied&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;table_pattern_allowed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;table_pattern_denied&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_metadata_from_info_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pattern_conditions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cnx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MySQLConnection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;information_schema&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;3306&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;table_and_column_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constraint_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;execute_query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pattern_conditions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern_conditions&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;json_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_json_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;table_and_column_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;constraint_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;json_table_template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json_table_template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;metadata.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w+&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json_result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cnx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errno&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errorcode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ER_ACCESS_DENIED_ERROR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Something is wrong with your user name or password&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errno&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errorcode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ER_BAD_DB_ERROR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Database does not exist&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;get_metadata_from_info_schema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pattern_conditions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;information_schema에서는 Table 정보, Column 정보, Constraint (Primary Key 등) 정보 등 여러 가지 정보를 추출합니다. 예를 들어 Column 정보는 다음과 같은 쿼리로 추출합니다. 이렇게 실행한 쿼리 결과를 Datahub에서 이용하는 json 형식에 맞게 바꿔줍니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# python script 중 information_schema에서 column info를 뽑아내는 부분
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_column_info_query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pattern_clause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;column_info_query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        SELECT Concat(table_schema, '.', table_name) AS schemaName,
               column_name,
               column_comment,
               data_type,
               column_type,
               is_nullable,
               column_key
        FROM   information_schema.columns 
        {pattern_clause}
        ORDER BY schemaname,
                 ordinal_position; 
        &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;column_info_query&lt;/span&gt;
 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;최종-테스트&quot;&gt;최종 테스트&lt;/h4&gt;

&lt;p&gt;이렇게 기능을 구현한 뒤 프로젝트에 같이 참여하시고 계시는 DBA 제이든과 직접 테스트를 해보았습니다. 마지막으로 MySQL 계정 권한에 변경이 필요했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;AS-IS : 모든 DB에 대해 SELECT 권한&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TO-BE : 모든 DB에 대해 REFERENCE 권한&lt;/p&gt;

    &lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&amp;lt;username&amp;gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'XX.XX.%'&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IDENTIFIED&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&amp;lt;password&amp;gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GRANT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;References&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TO&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&amp;lt;username&amp;gt;'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'XX.XX.%'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결과적으로 원하는 DB의 모든 메타데이터를 가져와서 Datahub에 주입하는 데에 성공했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-test-success.png&quot; alt=&quot;datahub-test-success&quot; /&gt; &lt;em&gt;기능 구현 내용&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-test-success-3.png&quot; alt=&quot;datahub-test-success&quot; /&gt; &lt;em&gt;축소된 권한으로 모든 정보를 가져올 수 있습니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;이 기능 개발로 쏘카의 DB에 접근하는 Datahub 계정의 권한이 크게 축소되어 내부 정보보호 규칙에 맞게 보안을 개선할 수 있습니다. 그리고 Datahub 최종 도입 결정에 긍정적인 영향을 미쳤습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-마무리--&quot;&gt;4. 마무리  &lt;a name=&quot;wrap-up&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이러한 여러 과정 끝에, Datahub가 쏘카에 도입될 준비를 마쳤습니다. 현재는 이렇게 테스트 배포를 마치고 사내 공개를 위한 준비 작업을 하고 있습니다. 이 자리를 빌려 Datahub 도입에 힘써주신 모든 분들에게 감사드립니다.&lt;/p&gt;

&lt;p&gt;입사하고 맡은 첫 프로젝트였는데 쏘카 데이터 플랫폼팀의 전반적인 인프라와 배포 흐름에 대해 알 수 있는 좋은 기회였습니다. 여담으로, 구현하면서 슬랙에서 질답을 너무 많이 한 나머지 커뮤니티 기여자로 Datahub 팀과 원격 인터뷰를 하는 기회도 얻었습니다 (!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-swag-all.jpeg&quot; alt=&quot;datahub-swag-all&quot; /&gt; &lt;em&gt;인터뷰 기념품으로 준 Datahub 기념품 세트&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-02/datahub-swag-thanks.jpeg&quot; alt=&quot;datahub-swag-thanks&quot; /&gt;&lt;em&gt;기념품과 함께 온 감사 메시지&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 Datahub를 도입하려는 분들에게 팁을 드리자면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;데이터 소스 특성에 따라서 메타데이터 파이프라인 구현 방법 정하기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 소스와 업데이트 주기를 결정한 뒤 구현 방법을 결정하기를 추천합니다.&lt;/li&gt;
  &lt;li&gt;하루 한 번 정도의 Batch 작업이라면 &lt;code class=&quot;highlighter-rouge&quot;&gt;Airflow DAG&lt;/code&gt; 로도 충분합니다.&lt;/li&gt;
  &lt;li&gt;최근에는 Datahub UI 상에서 Ingestion을 설정할 수 있는 기능도 나왔습니다. 장단점을 비교해 보고 결정하시면 좋을 것 같습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;DB 특성에 따라서 메타데이터 추출 로직 조정하기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;권한에 민감한 DB라면 information_schema에서 바로 메타데이터를 뽑는 로직을 구현하는 방법이 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Datahub 공식 Slack Workspace에 참여하기&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ingestion, Deployment 등 다양한 주제별로 질답을 나눌 수 있는 채널이 있습니다.&lt;/li&gt;
  &lt;li&gt;거의 모든 질문에 빠르게 답이 달릴 정도로 커뮤니티가 활성화되어 있습니다. 적극적으로 참여하시면서 도움을 얻기를 추천드립니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 편에서는 실제로 데이터 디스커버리 플랫폼이 도입된 후의 운영 방식과 효과에 대해서 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;긴 글 읽어주셔서 감사합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;데이터 플랫폼팀이 하는 업무가 궁금하시다면 &lt;a href=&quot;https://tech.socarcorp.kr/data/2021/03/24/what-socar-data-engineering-team-does.html&quot;&gt;데이터 엔지니어링 팀이 하는 일&lt;/a&gt;과 &lt;a href=&quot;https://www.notion.so/socarcorp/d458b6b77a2243fb873d1ac800c321f7?p=7c55b58735794368876dfb58acae96c5&quot;&gt;쏘카 데이터 플랫폼 엔지니어 채용공고&lt;/a&gt;를, 데이터 플랫폼 팀의 신입 온보딩 과정이 궁금하시다면 &lt;a href=&quot;https://tech.socarcorp.kr/data/2021/12/28/data-engineering-team-onboarding.html&quot;&gt;쏘카 신입 데이터 엔지니어 디니의 4개월 회고&lt;/a&gt;를 보시기를 추천드립니다.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>디니</name></author><category term="data" /><category term="data" /><category term="metadata-platform" /><category term="data-engineering" /><summary type="html">안녕하세요, 데이터 플랫폼 팀의 디니입니다.</summary></entry><entry><title type="html">데이터 디스커버리 플랫폼 도입기 - 1편. 데이터 디스커버리란?(feat. Datahub VS Amundsen 비교 분석)</title><link href="https://tech.socarcorp.kr/data/2022/02/25/data-discovery-platform-01.html" rel="alternate" type="text/html" title="데이터 디스커버리 플랫폼 도입기 - 1편. 데이터 디스커버리란?(feat. Datahub VS Amundsen 비교 분석)" /><published>2022-02-25T02:00:00+00:00</published><updated>2022-02-25T02:00:00+00:00</updated><id>https://tech.socarcorp.kr/data/2022/02/25/data-discovery-platform-01</id><content type="html" xml:base="https://tech.socarcorp.kr/data/2022/02/25/data-discovery-platform-01.html">&lt;p&gt;안녕하세요. 데이터 플랫폼 팀의 디니입니다.&lt;/p&gt;

&lt;p&gt;여러분 혹시 도서관에서 가서 책을 찾아보신 경험이 있으신가요? 방대한 도서관에서 원하는 책을 찾으려면 책의 제목, 저자, 분류 기호 같은 정보가 매우 중요합니다. 이런 정보가 없다면 책을 찾기가 많이 힘들어질 겁니다.&lt;/p&gt;

&lt;p&gt;데이터 분석, 머신러닝을 위해 회사의 데이터베이스에서 원하는 데이터를 찾으려고 할때도 비슷한 일이 발생합니다. 어느 데이터가 어디에 있는지, 이 데이터는 무슨 의미인지에 대한 안내가 없으면 데이터를 이용하기가 불편할 것입니다. 이런 문제를 해결하기 위해 데이터의 위치와 의미를 한눈에 보게 돕는 플랫폼이 &lt;strong&gt;“데이터 디스커버리 플랫폼”(DDP, Data Discovery Platform)&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;앞으로 3부에 걸쳐 쏘카의 데이터 디스커버리 플랫폼 도입기를 소개하려고 합니다. 그중 1부인 이 글에서는 데이터 디스커버리의 개념과 데이터 디스커버리 플랫폼은 왜 필요한지, 쏘카는 어떤 기준으로 데이터 디스커버리 플랫폼을 선택했는지를 담으려고 합니다. 다음과 같은 분들이 읽으시면 도움이 되리라고 생각합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사내 데이터 디스커버리 플랫폼 도입에 관심이 있는 분&lt;/li&gt;
  &lt;li&gt;Datahub, Amundsen 등의 데이터 디스커버리 플랫폼을 PoC 하고 있는 개발자&lt;/li&gt;
  &lt;li&gt;쏘카가 데이터 디스커버리 및 메타데이터 관리를 어떻게 하고 있는지 궁금하신 분&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;목차는 이렇습니다. 각 제목을 클릭하시면 해당 부분으로 이동합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-metadata&quot;&gt;데이터 디스커버리란 무엇인가요?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#why-we-need-metadata-platform&quot;&gt;데이터 디스커버리 플랫폼이 왜 필요한가요?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#metadata-platform-comparison&quot;&gt;데이터 디스커버리 플랫폼 비교 분석 : Datahub VS Amundsen&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#final-decision&quot;&gt;최종 결정 : Datahub 결정 이유&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wrap-up&quot;&gt;마무리 &amp;amp; 다음 편 예고&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;1-데이터-디스커버리란-무엇인가요--&quot;&gt;1. 데이터 디스커버리란 무엇인가요?  &lt;a name=&quot;what-is-metadata&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;데이터-디스커버리의-정의&quot;&gt;데이터 디스커버리의 정의&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;데이터 디스커버리(Data Discovery)&lt;/strong&gt;란, “원하는 데이터를 쉽고 빠르게 찾을 수 있다” 의 개념입니다. 빅데이터 시대라는 흐름에 맞게, 회사에도 많은 양의 데이터가 여러 형태로 존재하게 되었습니다. 그리고 많은 사람이 데이터를 생산 및 소비하고 시간이 지나게 되면서 히스토리 파악도 점점 복잡해지기 시작합니다. 이런 상황에서 데이터 디스커버리는 데이터 이용자에게 &lt;strong&gt;“어떤 데이터”가 “어디에” “어떻게 존재”하는지에 대한 정보를 편리하게 제공&lt;/strong&gt;합니다.&lt;/p&gt;

&lt;p&gt;이런 데이터 디스커버리에는 &lt;strong&gt;“메타데이터”&lt;/strong&gt;라는 개념이 중요합니다. 메타데이터는 간단히 말해서 테이블 정보, 컬럼 정보, 코멘트, 테이블을 만든 사람(오너), 테이블 사이의 관계(데이터 리니지) 등을 말합니다. 이러한 메타데이터를 잘 관리하는 것이 데이터 디스커버리의 핵심 역할입니다.&lt;/p&gt;

&lt;h3 id=&quot;데이터-디스커버리의-중요성&quot;&gt;데이터 디스커버리의 중요성&lt;/h3&gt;

&lt;p&gt;데이터를 적극적으로 활용하는 기업은 데이터 디스커버리의 존재에 따라 업무 효율성이 크게 달라집니다. 사내에 많은 직원들이 데이터를 활용해 데이터 분석, 머신러닝, 데이터 기반 기획 등을 하고 있습니다. 각자의 목적에 맞는 데이터가 “어디에 있는지”, “이 데이터가 어떤 의미인지”를 파악하는 데에 대부분의 시간을 소요하게 된다면, 업무 효율성이 매우 떨어지게 될 겁니다.&lt;br /&gt;
데이터 디스커버리를 도입하여 잘 관리한다면 이런 비효율성을 피할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;데이터-디스커버리-플랫폼의-정의&quot;&gt;데이터 디스커버리 플랫폼의 정의&lt;/h3&gt;

&lt;p&gt;데이터 디스커버리를 가능하게 하는 플랫폼입니다. 웹 UI 환경을 제공하며, 데이터의 구조와 관계 등 메타데이터를 한 곳에서 쉽게 보고 검색할 수 있습니다(같은 이유로 데이터 디스커버리는 메타데이터 플랫폼이라는 용어로 쓰이기도 합니다) 대표적인 데이터 디스커버리 프레임워크는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/linkedin/datahub&quot;&gt;Datahub&lt;/a&gt; : LinkedIn에서 만든 플랫폼입니다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/amundsen-io/amundsen&quot;&gt;Amundsen&lt;/a&gt; : Lyft에서 만든 플랫폼입니다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eng.uber.com/databook/&quot;&gt;Databook&lt;/a&gt; : Uber에서 만든 인하우스 플랫폼 입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;2-데이터-디스커버리-플랫폼이-왜-필요한가요--&quot;&gt;2. 데이터 디스커버리 플랫폼이 왜 필요한가요?  &lt;a name=&quot;why-we-need-metadata-platform&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;도입-효과---data-discovery의-관점&quot;&gt;도입 효과 - Data Discovery의 관점&lt;/h3&gt;

&lt;p&gt;쏘카에서는 데이터 분석가, PM, 마케터 등 여러 직군이 업무에 데이터를 활용합니다. 하지만 데이터가 다양한 형식으로 존재하고 비개발 직군 입장에서는 DB에 직접 접근하는 것도 어려움이 있습니다. 기존에 스키마에 대한 정보를 알려주는 어드민이 있었지만, 유지보수가 되고 있지 않았습니다. 이런 이유로 “어떤 데이터를 어디서 찾아야 하는지” 혹은 “이 테이블의 데이터가 어떤 의미인지” 에 대한 질문을 기존에는 슬랙 채널을 이용해 받곤 했습니다. 이런 방식은 히스토리 파악도 쉽지 않고, 답변하는 사람의 시간을 많이 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/data-ask-brown.png&quot; alt=&quot;data-ask-brown&quot; /&gt;&lt;em&gt;데이터의 의미를 찾아 헤매는 브라운&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/data-ask-jung.png&quot; alt=&quot;data-ask-jung&quot; /&gt;&lt;em&gt;데이터의 의미를 찾아 헤매는 정&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;데이터 디스커버리 플랫폼을 도입하면 개발에 대한 도메인이 없더라도 간편한 UI를 통해서 메타데이터를 확인할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;도입-효과---data-governance의-관점&quot;&gt;도입 효과 - Data Governance의 관점&lt;/h3&gt;

&lt;p&gt;데이터 거버넌스란 데이터를 효과적으로 관리하기 위한 일련의 보안, 품질, 규정 등과 관련된 체계를 말합니다. 여러 사람이 데이터를 생산하고 소비할 수록 데이터 거버넌스의 관점은 중요해집니다. 데이터 디스커버리 플랫폼을 도입하면 기존에 흩어져서 관리되던 테이블 스키마, 코멘트가 중앙 관리될 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 여러 이유들 때문에 쏘카는 데이터 디스커버리 플랫폼을 도입하기로 결정했습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;3-데이터-디스커버리-플랫폼-비교-분석--datahub-vs-amundsen-&quot;&gt;3. 데이터 디스커버리 플랫폼 비교 분석 : Datahub vs Amundsen &lt;a name=&quot;metadata-platform-comparison&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;poc-과정-소개&quot;&gt;PoC 과정 소개&lt;/h3&gt;

&lt;p&gt;디스커버리 플랫폼을 선정하기 전, 공식 문서 등의 지원이 풍부하고 일반적으로 많이 쓰이는 Datahub와 Amundsen을 직접 배포하고 테스트하며 비교하는 PoC 과정을 거쳐보기로 했습니다.&lt;/p&gt;

&lt;p&gt;사용성, UI, 문서화, 권한, 인증 등 다양한 측면에서 비교해 보았는데, 이 글에서는 그중 중요한 콘셉트를 간추려서 소개해 보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;datahub-vs-amundsen-비교-분석&quot;&gt;Datahub VS Amundsen 비교 분석&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-vs-amundsen.png&quot; alt=&quot;datathub-vs-amundsen&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-구조&quot;&gt;1) 구조&lt;/h4&gt;

&lt;p&gt;두 플랫폼의 구조에는 다음과 같은 공통점이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;React 기반 Frontend&lt;/li&gt;
  &lt;li&gt;Elasticsearch 기반 Search Engine&lt;/li&gt;
  &lt;li&gt;Neo4j 기반 Graph DB&lt;/li&gt;
  &lt;li&gt;MySql 기반 Storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;차이점은 Datahub는 데이터를 주입할때 Kafka를 사용하고 Amundsen은 ETL 라이브러리를 통한 크롤링 방식을 사용하는 점입니다. 메타데이터 플랫폼 프레임워크의 히스토리를 살펴봤을 때, Amundsen 은 Monolith 방식인 반면 Datahub는 Event-based 방식입니다. 메타데이터 플랫폼의 히스토리에 대해서 좀더 궁금하신 분들은, LinkedIn의 엔지니어링 블로그에 작성된 &lt;a href=&quot;https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained&quot;&gt;DataHub: Popular metadata architectures explained&lt;/a&gt; 글을 보시는 것을 추천합니다.&lt;/p&gt;

&lt;h4 id=&quot;2-메타데이터-주입-방식&quot;&gt;2) 메타데이터 주입 방식&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub는 &lt;code class=&quot;highlighter-rouge&quot;&gt;yaml&lt;/code&gt; 파일을 실행하여 메타데이터를 주입합니다.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;datahub CLI&lt;/code&gt;를 이용하여 yaml 파일을 실행합니다.&lt;/li&gt;
      &lt;li&gt;연결하는 데이터 소스에 따라 Datahub 라이브러리에 따라오는 연결 플러그인 설치가 필요합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amundsen은 &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; 파일을 실행하여 메타데이터를 주입합니다.
    &lt;ul&gt;
      &lt;li&gt;기본적으로 &lt;code class=&quot;highlighter-rouge&quot;&gt;amundsen databuilder&lt;/code&gt; 라는 Python 라이브러리 (ETL framework) 를 사용하며, Extract, Transform, Load 각 과정에 여러 자체 모듈을 끌어와서 사용하는 방식입니다.&lt;/li&gt;
      &lt;li&gt;이 외에도 종종 여러 디펜던시(Dependancy)가 필요했습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-ingestion-script.png&quot; alt=&quot;datahub-ingestion-script&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/img/data-discovery-platform-01/amundsen-ingestion-script.png&quot; alt=&quot;amundsen-ingestion-script&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Datahub -  BigQuery 데이터 주입 script&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Amundsen - BigQuery 데이터 주입 script&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;흥미로운 점은 같은 기능을 수행할 때 Datahub와 Amundsen의 &lt;strong&gt;script 길이 차이&lt;/strong&gt;였습니다. Datahub는 10줄 내외의 직관적인 yaml 코드로 가능한 반면, Amundsen의 script는 기본적으로 50 줄 이상이었습니다. 개인적으로 스크립트가 긴 만큼 섬세한 커스텀이 가능하거나 필요하다는 생각은 들지 않았고 오히려 읽기 무겁다는 생각이 들었습니다(공식 깃헙에 있는 샘플이 400줄이었습니다)&lt;/p&gt;

&lt;p&gt;각 ingestion 소스코드는 &lt;a href=&quot;https://github.com/linkedin/datahub/blob/master/metadata-ingestion/examples/recipes/bigquery_to_datahub.yml&quot;&gt;Datahub 공식 Github Repository&lt;/a&gt; 와 &lt;a href=&quot;https://github.com/amundsen-io/amundsen/blob/main/databuilder/example/scripts/sample_bigquery_metadata.py&quot;&gt;Amundsen 공식 Repository&lt;/a&gt; 에서 좀더 자세히 확인할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;3-ui&quot;&gt;3) UI&lt;/h4&gt;

&lt;p&gt;개인 차가 있을 수 있으나, 팀원들의 의견으로는 &lt;strong&gt;Datahub가 훨씬 깔끔하고 보기 편하다&lt;/strong&gt;는 의견이 많았습니다. Datahub UI는 &lt;a href=&quot;https://demo.datahubproject.io/&quot;&gt;공식 데모 사이트&lt;/a&gt;에서 더 확인하실 수 있습니다. (Amundsen은 따로 데모 사이트를 제공하지 않습니다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-main.png&quot; alt=&quot;datahub-main&quot; /&gt;&lt;em&gt;Datahub - 메인 UI&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-dataset.png&quot; alt=&quot;datahub-dataset&quot; /&gt; &lt;em&gt;Datahub - 상세 UI&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/amundsen-main.png&quot; alt=&quot;amundsen-main&quot; /&gt;&lt;em&gt;Amundsen - 메인 UI&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/amundsen-main.png&quot; alt=&quot;amundsen-main&quot; /&gt;&lt;em&gt;Amundsen - 상세 UI&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;4-문서-기능&quot;&gt;4) 문서 기능&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub
    &lt;ul&gt;
      &lt;li&gt;테이블 별 / 컬럼 별 태그 부여가 가능합니다.&lt;/li&gt;
      &lt;li&gt;테이블 별 / 컬럼 별 풍부한 마크다운 문서 작성이 가능하고, 원본 소스의 Description을 보존합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amundsen
    &lt;ul&gt;
      &lt;li&gt;테이블 별 태그 부여가 가능합니다.&lt;/li&gt;
      &lt;li&gt;역시 테이블 별 / 컬럼 별 마크다운 제한적인 문서 작성이 가능하고, 원본 소스의 Description을 보존하지 않습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;중요한 점은 플랫폼 UI 상에서 테이블 혹은 컬럼의 설명을 수정했을 때 &lt;strong&gt;원본 소스의 Description을 따로 확인할 수 있는지&lt;/strong&gt;의 여부였습니다. Datahub는 다음과 같은 방식으로 Original Description을 동시에 보여주지만, Amundsen은 이런 기능이 없습니다. 또한 Datahub는 원본 Description과 UI 상 Description이 별개로 버전 관리가 되고 있어서, 한쪽의 수정이 다른 쪽에 영향을 끼치지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-description.png&quot; alt=&quot;datahub-dataset&quot; /&gt;&lt;em&gt;Datahub - UI 상에서 수정하더라도 “Original”(원본 코멘트)이 함께 표기됩니다.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;5-오너십&quot;&gt;5) 오너십&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub는 테이블에 유저 / 그룹 단위로 오너십을 지정할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;Amundsen은 테이블에 유저 단위로만 오너십을 지정할 수 있습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6-데이터-계보data-lineage&quot;&gt;6) 데이터 계보(Data Lineage)&lt;/h4&gt;

&lt;p&gt;데이터 계보(Data Lineage)란 데이터의 흐름을 시각화한 개념으로 특정 테이블이 어떤 테이블들을 참조하는지, 데이터가 어디에서 와서 어디로 흘러가는지를 편리하게 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;Datahub와 Amundsen 모두 dbt* 등을 연동하여 데이터 계보를 시각화 할 수 있습니다. (최근에는 Datahub에 dbt 없이 BigQuery 자체에서도 데이터 계보를 가져오는 기능이 추가되었습니다)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;*&lt;a href=&quot;https://github.com/dbt-labs/dbt-core&quot;&gt;dbt&lt;/a&gt; : 데이터 ETL 과정에서 T(Transform) 과정을 효율화하는 도구입니다. dbt 를 이용하면 SQL 쿼리 모듈화, 테스트, 계보 확인을 편하게 할 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-lineage.png&quot; alt=&quot;datahub-lineage&quot; /&gt;&lt;em&gt;Datahub - 데이터 계보&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/amundsen-lineage.png&quot; alt=&quot;amundsen-lineage&quot; /&gt;&lt;em&gt;Amudsen - 데이터 계보 (출처 : https://medium.com/alvin-ai/data-lineage-in-amundsen-powered-by-alvin-df50cd40944c)&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;7-인증-및-권한&quot;&gt;7) 인증 및 권한&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub는 SSO 지원 및 세부적인 권한 설정이 가능합니다.
    &lt;ul&gt;
      &lt;li&gt;SSO(Single Sing-On)로 Keycloak, Okta, Google Auth를 지원합니다.&lt;/li&gt;
      &lt;li&gt;사용자 / 그룹 단위로 정책 부여가 가능합니다. 현재는 View 관련 권한은 설정할 수 없고, 테이블이나 컬럼에 대한 설명, 오너, 태그 등을 수정할 수 있는 Edit 권한을 세부적으로 조정 가능합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amundsen은 SSO을 지원하나 세부적인 권한 설정은 지원하지 않습니다.
    &lt;ul&gt;
      &lt;li&gt;SSO 로 Keycloack, Okta, Flask_oidc를 지원합니다.&lt;/li&gt;
      &lt;li&gt;Amundsen은 자체적인 권한 설정을 지원하지 않습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-policies.png&quot; alt=&quot;datahub-policies&quot; /&gt;&lt;em&gt;Datahub - 권한 및 정책 페이지&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;8-데이터-소스-지원&quot;&gt;8) 데이터 소스 지원&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;두 플랫폼 모두 BigQuery, Mysql, dbt, AWS S3 등 대중적으로 쓰이는 데이터 소스를 지원합니다.&lt;/li&gt;
  &lt;li&gt;Amundsen은 pandas, neo4j 등의 더 다양한 형태를 지원합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;9-사용자-이용-통계&quot;&gt;9) 사용자 이용 통계&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Datahub는 시각화된 이용 분석 페이지가 따로 존재합니다.
    &lt;ul&gt;
      &lt;li&gt;자주 검색된 데이터 셋  / 자주 수행된 액션 등을 그래프로 확인할 수 있습니다. (데모 사이트에서 &lt;a href=&quot;https://demo.datahubproject.io/analytics&quot;&gt;해당 페이지&lt;/a&gt;를 직접 확인하실 수 있습니다.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amundsen은 단편적인 이용 통계를 제공합니다.
    &lt;ul&gt;
      &lt;li&gt;메인 화면에서 “인기 있는 데이터셋”을, 각 테이블마다 “해당 테이블을 자주 이용한 사용자”을 확인할 수 있습습니다.&lt;/li&gt;
      &lt;li&gt;따로 분석 페이지는 없습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-analytics.png&quot; alt=&quot;datahub-analytics&quot; /&gt;&lt;em&gt;Datahub - 사용자 이용 통계 페이지&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/amundsen-analytics.png&quot; alt=&quot;amundsen-analytics&quot; /&gt;&lt;em&gt;Amundsen - 테이블을 자주 이용한 사용자&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;10-서포트&quot;&gt;10) 서포트&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;공식 Github Repository 의 Star 수를 비교했을 때 Datahub가 4.5K, Amundsen이 3K 로 Datahub 가 더 많은 Star를 보유하고 있었습니다.&lt;/li&gt;
  &lt;li&gt;두 플랫폼 모두 공식 슬랙, 웹사이트, Github repository 등의 다양한 채널을 지원했으나, 슬랙의 활성화(질문, 답변의 활발함)나 공식 문서의 체계성 측면에서 Datahub가 좀더 우세했습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-최종-결정--datahub--&quot;&gt;4. 최종 결정 : Datahub!  &lt;a name=&quot;final-decision&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;사용성의-편리함&quot;&gt;사용성의 편리함&lt;/h3&gt;

&lt;p&gt;가장 결정적인 이유는 사용성 차이였습니다. 사용성은 데이터 이용자와 플랫폼 개발자, 두 측면에서 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;데이터 이용자 측면에서는 위에서도 비교했듯이, Datahub가 문서화, 오너십, 권한, 통계, 데이터 계보 관점에서 &lt;strong&gt;더 다양하고 풍부한 기능들을 지원&lt;/strong&gt;합니다. 이런 기능들이 실제로 도입됐을 때, 이용자가 원하는 데이터를 빠르게 찾고 쏘카의 데이터 디스커버리를 발전시키는 데에 더 많은 도움을 얻을 수 있을거라 판단했습니다.&lt;/p&gt;

&lt;p&gt;플랫폼 개발자 측면에서도 &lt;strong&gt;메타데이터 주입 시 Datahub가 더 편리&lt;/strong&gt;했습니다. 동일한 메타데이터를 주입한다고 가정했을 때 Datahub는 10 줄 내외의 yaml 파일로 가능한 반면, Amundsen은 100줄 이상의 Python Script가 필요했습니다. Amundsen Script가 긴 만큼 세세한 설정이 가능한지, 또 그런 세세한 설정이 가능하다고 해도 현재 상황에 필요한지를 고민해봤을 때는 의문점이 있었습니다. 따라서 메타데이터를 주입할 데이터 소스가 한정된 쏘카의 상황에는 Datahub 가 더 적절하다고 판단했습니다.&lt;/p&gt;

&lt;h3 id=&quot;ui의-깔끔함&quot;&gt;UI의 깔끔함&lt;/h3&gt;

&lt;p&gt;많은 사람이 이용하는 솔루션이나 플랫폼을 도입할때는 UI도 무시할 수 없다고 생각합니다. PoC시 Datahub UI가 훨씬 깔끔하다는 반응이 많았고, 매 버전마다 UI가 개선되고 있는 점도 Datahub으로 결정힌 이유 중 하나였습니다.&lt;/p&gt;

&lt;h3 id=&quot;빠르고-풍부한-서포트&quot;&gt;빠르고 풍부한 서포트&lt;/h3&gt;

&lt;p&gt;Datahub의 공식 슬랙 채널에는 현재 2,000명이 넘는 사람이 활동하고 있고, 주제별로 분리된 다양한 채널에서 질답과 오류 대응이 활발하게 이루어지는 편입니다. 또한 새로운 기능을 제안(Feature Request)하는 채널도 따로 있어서, 사용자의 피드백을 풍부하게 반영하려는 노력이 느껴졌습니다. 이 뿐만 아니라 최근 발생한 Log4j 취약점 사태에도 빠르게 해당 취약점을 보완한 패치가 반영되고, 모든 진행 상황이 슬랙을 통해 공유되었습니다.&lt;/p&gt;

&lt;p&gt;개인적인 경험으로는 공식 채널에 질문을 올리면 답이 안달리는 경우가 거의 없었던 것 같습니다. 국내에 데이터 디스커버리 플랫폼 관련 자료가 많지 않고, 팀에 합류하지 얼마 되지 않은 신입 엔지니어의 입장에서는 이런 활발한 서포트가 있다는 것이 매우 중요했습니다. 여담으로 PoC 과정에서 Datahub 공식 슬랙에 질문을 100개정도 한 것 같은데, 이제는 사람들이 질문이 있으면 저를 호출합니다(!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-slack-01.png&quot; alt=&quot;datahub-slack-01&quot; /&gt;&lt;em&gt;디니 콜 1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-slack-02.png&quot; alt=&quot;datahub-slack-02&quot; /&gt;&lt;em&gt;디니 콜 2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/data-discovery-platform-01/datahub-slack-03.png&quot; alt=&quot;datahub-slack-03&quot; /&gt;&lt;em&gt;디니 콜 3&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-마무리--다음-편-예고-&quot;&gt;5. 마무리 &amp;amp; 다음 편 예고 &lt;a name=&quot;wrap-up&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이렇게 쏘카는 데이터 디스커버리 플랫폼으로 Datahub를 선정하게 되었습니다. 다음 편에는 구체적으로 Datahub를 어떻게 사내 인프라 환경에 구축했는지, 메타데이터 주입 방식을 어떻게 자동화하고 효율화 했는지 설명하려고 합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;쏘카에서 신입 데이터 엔지니어가 어떤 일을 하는지 궁금하시다면, &lt;a href=&quot;https://tech.socarcorp.kr/data/2021/12/28/data-engineering-team-onboarding.html&quot;&gt;쏘카 신입 데이터 엔지니어 디니의 4개월 회고&lt;/a&gt;에서 확인하실 수 있습니다(데이터 엔지니어링 팀이 데이터 엔지니어링 그룹으로 바뀌고 데이터 웨어하우스 팀, 데이터 플랫폼 팀, 모비딕 팀으로 세분화되었어요)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;긴 글 읽어주셔서 감사합니다. 그러면 다음 편에서 만나요!&lt;/p&gt;</content><author><name>디니</name></author><category term="data" /><category term="data" /><category term="data-engineering" /><summary type="html">안녕하세요. 데이터 플랫폼 팀의 디니입니다.</summary></entry><entry><title type="html">쏘카 PM(Product Manager)은 어떻게 성장하나요?</title><link href="https://tech.socarcorp.kr/product/2022/02/23/growing-up-together-with-the-pm-team.html" rel="alternate" type="text/html" title="쏘카 PM(Product Manager)은 어떻게 성장하나요?" /><published>2022-02-23T07:00:00+00:00</published><updated>2022-02-23T07:00:00+00:00</updated><id>https://tech.socarcorp.kr/product/2022/02/23/growing-up-together-with-the-pm-team</id><content type="html" xml:base="https://tech.socarcorp.kr/product/2022/02/23/growing-up-together-with-the-pm-team.html">&lt;p&gt;안녕하세요. 쏘카의 PM1 팀 프로덕트 매니저 마리입니다.&lt;/p&gt;

&lt;p&gt;“PM은 어떻게 성장하나요? 역량을 키워 나가기 위해 무엇을 하나요?” 주변 동료들과 종종 이런 이야기를 나누곤 합니다. 저 역시 커리어를 시작하면서, 이런 고민들을 많이 했던 것 같습니다. 
이번 글에서는 PM 개인 관점과 동료, 조직과 함께 성장했던 경험을 공유하고자 합니다. 특히 빠르게 성장하는 조직에서 성장하는 방법을 고민하는 PM, PM 팀에게 도움이 되었던 방법을 소개합니다.&lt;/p&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;p&gt;글의 목차는 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;쏘카의 프로덕트 매니저는&lt;/li&gt;
  &lt;li&gt;빠르게 성장하는 조직, 그 성장 속도만큼 생겨나는 어려움과 고민 지점들&lt;/li&gt;
  &lt;li&gt;돌파구 1: 어려운 점이 있으면, 함께 풀어보자 ‘위클리 미팅’&lt;/li&gt;
  &lt;li&gt;돌파구 2: 업무 관련 공부 갈증은 스터디로 채워보자 ‘빅쿼리 스터디’&lt;/li&gt;
  &lt;li&gt;정리&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;쏘카의-프로덕트-매니저는&quot;&gt;쏘카의 프로덕트 매니저는&lt;/h2&gt;

&lt;p&gt;본격적으로 글을 시작하기에 앞서, 쏘카의 PM이 어떻게 일하는지를 간략하게 소개하고자 합니다.&lt;/p&gt;

&lt;p&gt;IT 스타트업 업계에서도, 회사마다 PM의 역할이 조금씩 다릅니다. 크게 Product Manager, Project Manager, Product Owner로 이야기해 볼 수 있을 것 같습니다. 주요 관리 대상이 Product라면 Product Manager, Project라면 Project Manager로 칭합니다. Product를 관장하더라도 관리의 범위 및 역할을 넓혀 Product Owner라고 칭하기도 합니다.&lt;/p&gt;

&lt;p&gt;쏘카의 PM은 Product Manager로 쏘카의 여러 Product를 관리합니다. PM 그룹은 PM1팀과 PM2팀으로 구성되어 있습니다. PM 1팀은 고객분들이 사용하는 앱&amp;amp;웹 제품을 담당하고 PM 2팀은 B2B, 쏘카 내부 구성원들을 위한 인터널 프로덕트를 담당합니다(보다 상세한 설명은 &lt;a href=&quot;https://bit.ly/SOCAR-RECRUIT&quot;&gt;채용 문서&lt;/a&gt;의 프로덕트 매니저 부분에 나와있습니다 😉)&lt;/p&gt;

&lt;p&gt;그리고 팀 명칭에서도 알 수 있듯이, 쏘카의 PM 조직은 기능 조직의 형태로 구성되어 있습니다. 기능 조직이란 조직 안에서 같은 전문 기능 영역을 수행하는 구성원들 간 같은 팀으로 구성되어 있는 형태를 말합니다. 기능 조직의 가장 큰 장점은 동일한 업무를 진행하는 PM들과 고민을 나누고, 주변 동료들이 진행하는 프로젝트를 가까이서 보면서 정말 많이 배울 수 있다는 점입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;빠르게-성장하는-조직-그-성장-속도만큼-생겨나는-어려움과-고민-지점들&quot;&gt;빠르게 성장하는 조직, 그 성장 속도만큼 생겨나는 어려움과 고민 지점들&lt;/h2&gt;

&lt;p&gt;팀 동료들과 서로의 프로젝트를 공유하고, 함께 성장하기까지는 부단한 노력이 필요했습니다. 조직이 빠르게 성장하는 만큼 프로젝트의 진행 속도가 빠르고, 일하는 동료들 대부분 바빠 보였습니다. 특히 지난 하반기는 IPO를 앞두고 회사가 빠르게 성장하면서 더 효율적으로 일할 수 있는 방법에 대해 고민하기 시작했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;제가 좋아하는 유튜브 채널 ‘존잡생각’에서도 비슷한 얘기를 합니다. 그렇기 때문에 회사에서 본인을 빠르게 성장하는 방법 - People Scaling이 필요하다고 강조합니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;쏘카의 PM들은 PM 직업 특성상 문제를 가만두지 않습니다. 이 문제를 해결할 방법을 찾아보기로 했습니다. 방법을 찾고, 작년에 실행했던 프로젝트 2가지를 소개합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;돌파구-1-어려운-점이-있으면-함께-풀어보자-위클리-미팅&quot;&gt;돌파구 1: 어려운 점이 있으면, 함께 풀어보자 ‘위클리 미팅’&lt;/h2&gt;

&lt;p&gt;우선 우리의 고민거리를 함께 이야기하는 시간을 마련했습니다. 매주 수요일 오전 11시로 미팅을 잡아두고, 안건이 있으면 이 시간에 함께 모여 이야기를 나눴습니다. 헤아려보니, 작년 6월 초에 시작해서 12월까지 7개월간 총 24번의 위클리 미팅이 진행되었습니다! 
&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/3.png&quot; alt=&quot;&quot; /&gt;
위클리 미팅은 아래의 방식으로 진행되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위클리 미팅 하루 전까지, 동료들과 이야기하고 싶은 안건이 있으면 ‘PM1 팀 위클리’ 노션 페이지에 해당 내용을 등록합니다.&lt;/li&gt;
  &lt;li&gt;안건이 등록되면, 매주 수요일 오전 11시에 회의실 또는 행아웃으로 만납니다. (가끔 날이 좋으면, 서울숲으로 나가기도 합니다 🌳 🎵)&lt;/li&gt;
  &lt;li&gt;발제자가 안건을 소개하고, 동료들과 함께 이야기를 나눕니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;처음 위클리 미팅을 만들자는 제안이 나왔을 때, 동료들의 의견은 반반이었습니다. “오, 너무 좋겠다. 나 이야기해보고 싶은 것 있어.” 라고 긍정적인 견해를 가진 경우도 있었지만, “매주 이야기할 만큼, 고민이 많을까.” 회의적인 동료들도 있었습니다. 그래서 안건이 있을 때마다 만나자고 한 것인데, 되돌아보니 한 달 평균 3번 이상, 1주 정도를 제외하면 늘 만났습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/4.jpg&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;가을 날의 서울숲 미팅은 정말 환상입니다! 😍&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;위클리 미팅은 ‘함께’ ‘빠르게’ 문제를 풀어나가는 창구가 되었습니다. 개개인은 프로젝트를 진행하면서 겪는 다음과 같은 문제를 해결할 수 있는 아이디어를 얻어 갔습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“배포하고 작고 큰 문제들이 발생하는데, 배포 시나리오는 어떻게 작성하고 계시나요?”&lt;/li&gt;
  &lt;li&gt;“업무 대체자는 어떻게 선정하고, 공유하는 게 좋을까요?”&lt;/li&gt;
  &lt;li&gt;“미팅이 너무 많아지고 있는데, 미팅 시간은 어떻게 조정하고 계신가요?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PM들마다 경험한 프로젝트도 다르고, 강점도 다릅니다. 누군가 이런 고민을 발제하면, 각자 도움 되었던 문서와 방법을 공유했고, 꿀팁 가득한 가이드 문서가 하나 뚝딱 만들어졌습니다. 위클리 회의 이후 팀 내 모든 PM이 가이드 문서를 보고 고민 포인트가 줄었습니다.&lt;/p&gt;

&lt;p&gt;또한 PM 위클리 미팅은 팀의 문제를 하나씩 푸는 계기가 되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“새로운 구성원이 오면, 매번 온보딩 준비를 해야해요. 시간도 줄이고 일관성도 갖추기 위해 가이드문서를 만드는 게 어떨까요?”&lt;/li&gt;
  &lt;li&gt;“다른 팀에서 제품에 관한 공통 질문이 자주 들어오는데요. 앱 사용설명서를 만들어서 공유하면 좋겠어요.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이런 부분은 혼자 해결해 나가기 어려운 문제입니다. 문서화 작업을 하더라도 끊임없는 업데이트가 필요하고, 이를 위해 우리 팀에 필요한 일임에 공감대가 형성되어야 하기 때문입니다. 실제 이 시간을 통해 그간의 숙원 사업이었던 ‘쏘카 앱 사용설명서’도 만들어졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 위클리 미팅의 효과는 기대했던 만큼 좋았습니다. 24번의 미팅을 거치면서 총 17개의 의제가 논의되었고, 그중 9개는 안건을 제안한 동료의 문제에 공감해 새로운 방법으로 시도해 보고 있습니다. 이를테면, 새로운 구성원이 우리 팀에 왔을 때, 보다 잘 온보딩할 수 있는 방법이 필요하다는 이야기에 PM1 팀 뉴비를 위한 온보딩 프로세스가 만들어졌습니다. 프로젝트 하고 나서 백로그를 체계적으로 관리하자는 안건이 제안되어 백로그 프로세스 및 문서를 만들어 운영하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;돌파구-2-업무-관련-공부-갈증은-스터디로-채워보자-빅쿼리-스터디&quot;&gt;돌파구 2: 업무 관련 공부 갈증은 스터디로 채워보자 ‘빅쿼리 스터디’&lt;/h2&gt;

&lt;p&gt;일을 하다 보면 더 공부해 보고 싶은 갈증이 생깁니다. 저의 경우에는 ‘프로덕트 데이터’에 대한 공부를 좀 더 하고 싶다는 생각이 있었습니다. 기능을 배포하고, 유저들의 행동 패턴을 더 들여다보고 싶거나 백로그에 등록해 둔 일감을 좀 더 디벨롭해보고 싶을 때 데이터를 직접 조회하여 문제를 해결하고 싶다는 생각이 들었습니다.&lt;/p&gt;

&lt;p&gt;그래서 빅쿼리 조회 역량을 키워나가는데 관심이 있는 동료들을 모아, 빅쿼리 스터디를 진행했습니다. 처음에는 PM팀으로 시작해, 지금은 사업팀, CRM팀의 동료도 모여 함께 진행하고 있습니다. 헤아려보니, 작년 7월부터 시작해서 총 20회 스터디가 진행되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;빅쿼리 스터디는 아래의 방식으로 진행되었습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4명의 동료들이 돌아가면서 매주 문제를 출제합니다.&lt;/li&gt;
  &lt;li&gt;직접 쿼리를 작성해 빅쿼리로 사내 데이터를 직접 조회합니다.&lt;/li&gt;
  &lt;li&gt;매주 금요일 오전 9시에 만나 서로 쿼리를 공유하며 피드백합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;조금 더 구체적인 설명을 하기 위해 과거에 진행했던 스터디 경험을 공유합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/8.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;1) 문제를 출제하고, 슬랙 스터디 채널에서 공유합니다.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이 케이스는 카리나가 발제자였습니다. 화요일 중에 발제자가 문제를 제출하고, 슬랙 스터디 채널에 문제 링크를 공유합니다. 유저의 서비스 사용 자료를 기반으로, &lt;성장 지수=&quot;&quot;&gt; 지표를 집계하고 도출하는 방법을 알아보았습니다.&lt;/성장&gt;&lt;/li&gt;
  &lt;li&gt;성장지수란 사용자의 서비스 사용과 관련한 상태 변화를 수치화해서 서비스가 성장하는지를 알려주는 지표입니다. 유저의 서비스 사용 자료를 기반으로 Signup(신규 등록 후 사용을 시작함), Deactivation(활성화 유저에서 비활성화 유저로 전환), Reactivation(비활성화 유저에서 활성화 유저로 전환), Exit(서비스를 탈퇴함)을 정의하고 이를 기반으로 성장지수(signup user + reactivation user - deactivation user - exit) 지표를 도출합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/9.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;2) 퇴근 후 목요일 밤은 빅쿼리 문제 푸는 시간입니다. 😂 질문과 답변이 오고 갑니다.&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;발제자가 문제를 출제하고 나면 스터디가 있는 금요일 오전 전까지 각자 문제를 풀어옵니다. 보통 퇴근 후 목요일 밤 다시 출근했다고 표현하곤 합니다 😂. 문제를 풀면서, 모르는 부분에 대해서는 질문과 답변이 오고 갑니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;금요일 오전 9시에 만나서, 작성한 쿼리를 공유하고, 피드백을 주고 받습니다. 피드백은 ‘출제자의 의도에 부합하는 방향으로 함수를 적재적소에 사용했는지’를 중점적으로 나눕니다. 이번 문제는 12월 1일부터 12월 10일까지 해당 기간 동안 첫 사용을 한 회원의 날짜별 회원 상태를 구하고, 그 집계를 통해 성장성을 지수화하는 것이 목표였습니다. 이를 위해
    &lt;ul&gt;
      &lt;li&gt;SELECT, UNION ALL 구문을 활용해 12월 1일부터, 12월 10일까지의 날짜 컬럼을 가진 테이블 생성&lt;/li&gt;
      &lt;li&gt;CASE WHEN 함수를 활용하여, 유저의 ‘신규 등록일’과 ‘탈퇴일’, ‘사용일’ 구하기&lt;/li&gt;
      &lt;li&gt;CROSS JOIN 함수를 활용하여, 두 개의 테이블을 상호 조인하기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위와 같은 내용이 잘 진행되었는지 이야기합니다. “우선 유저별 회원 상태를 구할 수 있도록, 각각의 활동 로그를 모은 테이블을 만들었어. 여기서 예약 테이블과 탈퇴 테이블을 left join 했고…” 와 같이 쿼리를 왜 이렇게 작성했는지를 동료들에게 설명합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;피드백을 주고 받는 과정을 통해 좀 더 효율적으로 쿼리를 작성할 수 있는 방법, 보다 적은 용량으로 조회할 수 있는 방법, 다른 사람들이 더 알아보기 쉽게 쿼리를 작성하는 방법을 익히게 되었습니다. 스터디 이후 서브 쿼리보다 WITH 구문을 더 애용하게 되었습니다!&lt;/li&gt;
  &lt;li&gt;특히 이번 시간에는 ‘확장성’ 에 대한 이야기를 나누었습니다. 성장 지수라는 것은 카리나가 제시한 것과 같은 특정한 날짜 구간이 아니더라도 더 넓히거나 좁혀가면서 다양하게 조회해 볼 수 있을텐데, 이를 위해서는 쿼리를 어떻게 작성해보면 좋을까는 궁금함이 생겨서 이야기를 꺼냈습니다. 이때 에이든이 이전에 실무에서 사용했던 DECLARE 구문을 소개해 주셨습니다!&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;declare&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DEFAULT&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;TIMESTAMP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;CURRENT_DATE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;Asia/Seoul&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;Asia/Seoul&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;-- WHERE 조건에서 아래와 같이 표현하면, '어제까지' 로 조회 가능.&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;DATE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이처럼 사내 스터디는 데이터 역량을 키워나가는데 매우 효과적이었습니다. 이를 통해 SELECT / FROM / WHERE 정도의 간단한 조회만 할 수 있었는데, 윈도우 함수가 손에 익는 수준으로 쿼리를 조회할 수 있게 되었습니다. 실무 진행에도 큰 도움이 되었습니다. 직접 제품 성과를 조회해서 업무 성과를 어필할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/12.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;최근 1년 빅쿼리 스터디 회고하면서 나눴던 이야기들&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/growing-up-together-with-the-pm-team/13.jpg&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;어느덧 8개월 가까이, 매주 금요일 온라인으로 함께 빅쿼리를 해나가고 있는 마리, 카리나, 에이든, 브라운&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;정리&quot;&gt;정리&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;무엇이건 실제 임팩트를 남기려면 혼자 힘으로만 되는 게 없습니다. 함께 해야 합니다. 
그리고 무엇보다 이런 ‘함께’, 그리고 ‘자라기’를 매일매일 해야 합니다. 
(…) 대화는 우리가 혼자서는 생각하지 못했던 것들을 만들게 해 줄 것입니다.&lt;/p&gt;

  &lt;p&gt;- 함께 자라기, 김창준&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;최근 재밌게 읽었던 &amp;lt;함께 자라기: 애자일로 가는 길&amp;gt; 책에 나오는 이야기입니다. 쏘카는 빠르게 성장하는 조직인 만큼, 어떻게 함께 성장할 것인가에 대한 고민을 끊임없이 하고 있습니다.&lt;/p&gt;

&lt;p&gt;이 글에서는 PM1팀 위클리와 사내 스터디를 소개해드렸습니다. PM1팀 위클리, 회고, 1on1을 통해 주 단위로 빠르게 피드백을 주고받고, 필요시 사내 스터디를 만들어 실무에 필요한 역량을 함께 키워나가고 있습니다. 만약 쏘카 PM 직군에 관심이 있다면 채용 공고를 확인 부탁드립니다!&lt;/p&gt;</content><author><name>마리</name></author><category term="product" /><category term="product manager" /><summary type="html">안녕하세요. 쏘카의 PM1 팀 프로덕트 매니저 마리입니다.</summary></entry></feed>